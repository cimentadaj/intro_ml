<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Machine Learning for Social Scientists</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jorge Cimentada" />
    <meta name="date" content="2020-06-25" />
    <script src="libs/header-attrs-2.2/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Machine Learning for Social Scientists
## Regularization
### Jorge Cimentada
### 2020-06-25

---


layout: true

&lt;!-- background-image: url(./figs/upf.png) --&gt;
background-position: 100% 0%, 100% 0%, 50% 100%
background-size: 10%, 10%, 10%



---

# What is regularization?

Regularization is when you force your estimates towards specific values:

* Bayesian: restrict coefficients based on prior distributions
&lt;br&gt;

* Machine Learning: restrict coefficents to zero

&lt;br&gt;

--

### What is this good for? It depends on your context

* Increasing predictive power
* Including important confounders in large models
* Understanding the strength of variables
* Testing the generalization of your model

---

# What is regularization?

* Machine Learning is almost always about prediction

* **It is important to make sure that out-of-sample accuracy is high**

* Overfitting is our weak spot by including redundant or unimportant variables

* Correct theoretical model is not always the aim

--

&lt;br&gt;
&lt;br&gt;

&gt; How do we make sure our model does good predictions on unseen data? We regularize how much it overfits the data. How do we do that? Forcing unimportant coefficients towards zero.

&lt;br&gt;

* ML parlance: reduce variance in favor of increasing bias
* SocSci parlance: make sure your model fits an unseen data as fairly well as this data

---

# A first example: ridge regression

* OLS minimizes the Residual Sum of Squares (RSS)
* Fit N lines that minimize the RSS and keep the one with the best fit

`\begin{equation}
RSS = \sum_{k = 1}^n(actual_i - predicted_i)^2
\end{equation}`

.center[
&lt;img src="02_regularization_afternoon_files/figure-html/unnamed-chunk-2-1.png" width="80%" /&gt;
]

---

# A first example: ridge regression

Ridge regression only adds one term:

`\begin{equation}
RSS + \lambda \sum_{k = 1}^n \beta^2_j
\end{equation}`

**The regularization term** or **penalty term**

* `\(RSS\)` estimates how the model fits the data
* `\(\sum_{k = 1}^n \beta^2_j\)` limits how much you overfit the data. 
* `\(\lambda\)` is the weight given to the penalty term (called **lambda**): the higher the weight the bigger the shrinkage term of the equation.

In layman words:

&gt; In other words, the whole gist behind ridge regression is penalizing very large coefficients for better generalization

---

# A first example: ridge regression

Some caveats:

* Since we're penalizing coefficients, their scale *matter*.

&gt; Suppose that you have the income of a particular person (measured in thousands per months) and time spent with their families (measured in seconds) and you're trying to predict happiness. A one unit increase in salary could be penalized much more than a one unit increase in time spent with their families **just** because a one unit increase in salary can be much bigger due to it's metric.

&lt;br&gt;
&lt;br&gt;

.center[
### **Always standardize coefficients before running a regularized regression**
]
---

# A first example: ridge regression

Download the data:


```r
library(tidymodels)
library(tidyflow)

# Read the PISA data
data_link &lt;- "https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv"
pisa &lt;- read.csv(data_link)
head(pisa, n = c(5, 8))
```

```
  CNTSCHID CNTSTUID BOOKID ST001D01T ST003D02T ST003D03T ST004D01T ST005Q01TA
1 84000001 84000250      3        11         7      2002         2          1
2 84000001 84000304     14        11         9      2002         1          1
3 84000001 84000353     15         9         6      2003         2          1
4 84000001 84000536     24        10         5      2003         2          3
5 84000001 84001240      6        10         5      2003         2          1
```

---

# A first example: ridge regression

Remember, we do everything with the **training** dataset and ignore the **testing** dataset for our best model. Adding a split:


```r
tflow &lt;-
  pisa %&gt;%
  tidyflow(seed = 23151) %&gt;%
  plug_split(initial_split)

tflow
```

```
══ Tidyflow ════════════════════════════════════════════════════════════════════
Data: 4.84K rows x 501 columns
Split: initial_split w/ default args
Recipe/Formula: None
Resample: None
Grid: None
Model: None
```

---

# A first example: ridge regression

Adding the preprocessing:


```r
rcp &lt;-
  ~ recipe(math_score ~ MISCED + FISCED + HISEI + REPEAT + IMMIG + DURECEC + BSMJ, data = .x) %&gt;%
  step_center(all_predictors()) %&gt;%
  step_scale(all_predictors())

tflow &lt;-
  tflow %&gt;%
  plug_recipe(rcp)

tflow
```

```
══ Tidyflow ════════════════════════════════════════════════════════════════════
Data: 4.84K rows x 501 columns
Split: initial_split w/ default args
Recipe: available
Resample: None
Grid: None
Model: None
```

---

# A first example: ridge regression

Adding the resample. `vfold_cv` is the function applying the cross-validated set:


```r
tflow &lt;-
  tflow %&gt;%
  plug_resample(vfold_cv)

tflow
```

```
══ Tidyflow ════════════════════════════════════════════════════════════════════
Data: 4.84K rows x 501 columns
Split: initial_split w/ default args
Recipe: available
Resample: vfold_cv w/ default args
Grid: None
Model: None
```


---

# A first example: ridge regression

Adding the model:


```r
# mixture 0 is the same as ridge regression
regularized_reg &lt;- linear_reg(penalty = tune(), mixture = 0) %&gt;% set_engine("glmnet")
tflow &lt;- tflow %&gt;% plug_model(regularized_reg)
tflow
```

```
══ Tidyflow ════════════════════════════════════════════════════════════════════
Data: 4.84K rows x 501 columns
Split: initial_split w/ default args
Recipe: available
Resample: vfold_cv w/ default args
Grid: None
Model:
Linear Regression Model Specification (regression)

Main Arguments:
  penalty = tune()
  mixture = 0

Computational engine: glmnet 
```

---

# A first example: ridge regression

Adding the grid:


```r
tflow &lt;-
  tflow %&gt;%
  plug_grid(expand.grid, penalty = seq(0, 3, length.out = 300))

tflow
```

```
══ Tidyflow ════════════════════════════════════════════════════════════════════
Data: 4.84K rows x 501 columns
Split: initial_split w/ default args
Recipe: available
Resample: vfold_cv w/ default args
Grid: expand.grid w/ penalty = ~seq(0, 3, length.out = 300)
Model:
Linear Regression Model Specification (regression)

Main Arguments:
  penalty = tune()
  mixture = 0

Computational engine: glmnet 
```


# A first example: ridge regression


```r
tflow
```

```
══ Tidyflow ════════════════════════════════════════════════════════════════════
Data: 4.84K rows x 501 columns
Split: initial_split w/ default args
Recipe: available
Resample: vfold_cv w/ default args
Grid: expand.grid w/ penalty = ~seq(0, 3, length.out = 300)
Model:
Linear Regression Model Specification (regression)

Main Arguments:
  penalty = tune()
  mixture = 0

Computational engine: glmnet 
```



```r
res &lt;- fit(tflow)
```

---

# A first example: ridge regression


```r
final_ridge &lt;- complete_tflow(res, metric = "rmse")

final_ridge %&gt;% 
  pull_tflow_fit() %&gt;%
  .[['fit']] %&gt;%
  plot(xvar = "lambda", label = TRUE)
```

&lt;img src="02_regularization_afternoon_files/figure-html/unnamed-chunk-11-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# A first example: ridge regression


```r
train_rmse_ridge &lt;-
  final_ridge %&gt;%
  predict_training() %&gt;%
  rmse(math_score, .pred)

test_ridge &lt;-
  final_ridge %&gt;%
  predict_testing() %&gt;%
  rmse(math_score, .pred)

train_rmse_ridge$type &lt;- "training"
test_ridge$type &lt;- "testing"

ridge &lt;- as.data.frame(rbind(train_rmse_ridge, test_ridge))
ridge$model &lt;- "ridge"
ridge
```

```
  .metric .estimator .estimate     type model
1    rmse   standard  76.87668 training ridge
2    rmse   standard  77.88607  testing ridge
```

---

# A first example: lasso regression

Lasso regression is very similar to ridge but the penalty term is different:

`\begin{equation}
RSS + \lambda \sum_{k = 1}^n |\beta_j|
\end{equation}`

The same notes for ridge applies with one caveat:

- The penalty term for lasso can **completely shrink to 0** meaning that it excludes variables.

&gt; Lasso excludes variables which are not adding anything useful to the model whereas ridge keeps them close to 0.

---

# A first example: lasso regression

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

.center[
## **Always standardize coefficients before running a regularized regression**
]

---

# A first example: lasso regression

`tflow` has all our steps, just replace the model:


```r
# mixture = 1 is lasso
lasso_mod &lt;- update(regularized_reg, mixture = 1)

tflow &lt;-
  tflow %&gt;%
  replace_model(lasso_mod)

res_lasso &lt;- fit(tflow)
```

---

# A first example: lasso regression


```r
final_lasso &lt;- complete_tflow(res_lasso, metric = "rmse")

final_lasso %&gt;%
  pull_tflow_fit() %&gt;%
  .[['fit']] %&gt;%
  plot(xvar = "lambda", label = TRUE)
```

&lt;img src="02_regularization_afternoon_files/figure-html/unnamed-chunk-14-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# A first example: lasso regression


```r
train_rmse_lasso &lt;-
  final_lasso %&gt;%
  predict_training() %&gt;%
  rmse(math_score, .pred)

holdout_lasso &lt;-
  final_lasso %&gt;%
  predict_testing() %&gt;%
  rmse(math_score, .pred)

train_rmse_lasso$type &lt;- "training"
holdout_lasso$type &lt;- "testing"

lasso &lt;- as.data.frame(rbind(train_rmse_lasso, holdout_lasso))
lasso$model &lt;- "lasso"
lasso
```

```
  .metric .estimator .estimate     type model
1    rmse   standard  76.87264 training lasso
2    rmse   standard  77.86454  testing lasso
```

---

# A first example: elastic net regression

Elastic Net regression is very similar to ridge but the penalty term is different:

`\begin{equation}
RSS + \lambda \sum_{k = 1}^n |\beta_j|
\end{equation}`

The same notes for ridge applies with one caveat:

- The penalty term for elastic net can **completely shrink to 0** meaning that it excludes variables.

&gt; Elastic Net excludes variables which are not adding anything useful to the model whereas ridge keeps them close to 0.

---

# A first example: elastic net regression

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

.center[
## **Always standardize coefficients before running a regularized regression**
]

---

# A first example: elastic net regression


```r
elnet_mod &lt;- update(lasso_mod, mixture = tune())

tflow &lt;-
  tflow %&gt;%
  replace_model(elnet_mod) %&gt;%
  replace_grid(grid_regular)

res_elnet &lt;- fit(tflow)
```

---

# A first example: elastic net regression


```r
final_elnet &lt;- complete_tflow(res_elnet, metric = "rmse")

final_elnet %&gt;%
  pull_tflow_fit() %&gt;%
  .[['fit']] %&gt;%
  plot(xvar = "lambda", label = TRUE)
```

&lt;img src="02_regularization_afternoon_files/figure-html/unnamed-chunk-17-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# A first example: elastic net regression


```r
train_rmse_elnet &lt;-
  final_elnet %&gt;%
  predict_training() %&gt;%
  rmse(math_score, .pred)

holdout_elnet &lt;-
  final_elnet %&gt;%
  predict_testing() %&gt;%
  rmse(math_score, .pred)

train_rmse_elnet$type &lt;- "training"
holdout_elnet$type &lt;- "testing"

elnet &lt;- as.data.frame(rbind(train_rmse_elnet, holdout_elnet))
elnet$model &lt;- "elnet"
elnet
```

```
  .metric .estimator .estimate     type model
1    rmse   standard  76.87256 training elnet
2    rmse   standard  77.87192  testing elnet
```

---

# Exercise
.center[
https://cimentadaj.github.io/ml_socsci/regularization.html#exercises
]

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
