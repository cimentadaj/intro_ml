<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Machine Learning for Social Scientists</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jorge Cimentada" />
    <meta name="date" content="2020-07-07" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Machine Learning for Social Scientists
## K-Means clustering and competition
### Jorge Cimentada
### 2020-07-07

---


layout: true

&lt;!-- background-image: url(./figs/upf.png) --&gt;
background-position: 100% 0%, 100% 0%, 50% 100%
background-size: 10%, 10%, 10%



---

# Load the data


```r
library(dplyr)
library(ggplot2)

data_link &lt;- "https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv"
pisa &lt;- read.csv(data_link)
```

---

## K-Means Clustering

* K-Means is a method for finding clusters in a dataset of `\(P\)` variables

* K-Means clustering is particularly useful for exploration in the social sciences

Suppose we have a scatterplot of two variables:

&lt;img src="../../img/km1.png" width="25%" style="display: block; margin: auto;" /&gt;

---
## K-Means Clustering

* How does K-Means identify clusters? 

* **Randomly** assigning each point a cluster

&lt;img src="../../img/km2.png" width="25%" style="display: block; margin: auto;" /&gt;

* Each point has now an associated color. However, these colors were randomly assigned. 

---
## K-Means Clustering

* K-Means clustering works by creating something called 'centroids'

* These represent the center of the different clusters

* The centroid is the **mean of the `\(P\)` variables**

&lt;img src="../../img/km3.png" width="25%" style="display: block; margin: auto;" /&gt;

* So far, everything is random!

---

## K-Means Clustering

* Let's work this out manually:


```r
centroids_df &lt;- data.frame(type = factor(c("orange", "purple", "green"), levels = c("orange", "purple", "green")), x = c(.54, .56, .52), y = c(.553, .55, .56))

ggplot(centroids_df, aes(x, y, color = type)) +
  geom_point(size = 4) +
  scale_color_manual(values = c("orange", "purple", "green")) +
  lims(x = c(0, 1), y = c(0, 1)) +
  theme_minimal()
```

&lt;img src="kmeans_competition_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

---

## K-Means Clustering

* Suppose we add a random point


```r
centroids_df %&gt;%
  ggplot(aes(x, y)) +
  geom_point(aes(color = type), size = 4) +
  geom_point(data = data.frame(x = 0.25, y = 0.75)) +
  scale_color_manual(values = c("orange", "purple", "green")) +
  lims(x = c(0, 1), y = c(0, 1)) +
  theme_minimal()
```

&lt;img src="kmeans_competition_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;

* How do we assign that point a cluster?

---

## K-Means Clustering

* We calculate the Euclidean distance:

`\(\sqrt{(x_2 - x_1) + (y_2 - y_1)}\)`


* Applied to our problem:

  + Orange: `\(\sqrt{(0.54 - 0.25) + (0.553 - 0.75)} = 0.304959\)`
  
  + Purple: `\(\sqrt{(0.56 - 0.25) + (0.550 - 0.75)} = 0.3316625\)`
  
  + Green: `\(\sqrt{(0.52 - 0.25) + (0.560 - 0.75)} = 0.2828427\)`

---

## K-Means Clustering

The random point is closest to the green centroid, as the distance is the smallest (0.28). Let's assign it to that cluster:


```r
centroids_df %&gt;%
  ggplot(aes(x, y, color = type)) +
  geom_point(size = 4) +
  geom_point(data = data.frame(type = factor("green"), x = 0.25, y = 0.75)) +
  scale_color_manual(values = c("orange", "purple", "green")) +
  lims(x = c(0, 1), y = c(0, 1)) +
  theme_minimal()
```

&lt;img src="kmeans_competition_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;

---
## K-Means Clustering

The K-Means clustering algorithm applies this calculation for **each point**:

&lt;img src="../../img/km4.png" width="25%" style="display: block; margin: auto;" /&gt;

where each point is assigned the color of the closest centroid. 

* The centroids are still positioned in the center, reflecting the random allocation of the initial points

---
## K-Means Clustering

* Calculates new centroids based on the average of the X and Y of the newly new assigned points:

&lt;img src="../../img/km5.png" width="25%" style="display: block; margin: auto;" /&gt;

* Repeat exactly the same strategy again:

  + Calculate the distance between each point and all corresponding clusters
  + Reassign all points to the cluster of the closest centroid
  + Recalculate the centroid

---
## K-Means Clustering

* After `\(N\)` iterations, each point will be allocated to a particular centroid and it **will stop being reassigned**:

&lt;img src="../../img/km6.png" width="25%" style="display: block; margin: auto;" /&gt;

* Minimize within-cluster variance
* Maximize between-cluster variance

&gt; Respondents are very similar within each cluster with respect to the `\(P\)` variables and very different between clusters

---
## Disadvantages K-Means Clustering

* You need to provide the number of cluster that you want

* K-Means will **always** calculate the number of supplied clusters

* The clusters need to make substantive sense rather than statistical sense.

* K-Means also has a stability problem

&lt;img src="../../img/km7.png" width="35%" style="display: block; margin: auto;" /&gt;

---
## Caveats K-Means Clustering

* Exploratory

* Should make substantive sense

* Robustness

* Replicability

* Centering and scaling might be appropriate

* Outliers

---
## K-Means Clustering

* How can we fit this in `R`? 

* Suppose that there are different clusters between the socio-economic status of a family and a student's expected socio-economic status:

  + Low socio-economic status might not have great aspirations
  
  + Students from middle socio-economic status have average aspirations 
  
  + Students from high socio-economic status might have great aspirations.

* We fit this using `kmeans` and passing a data frame with the columns

---
## K-Means Clustering

* K-Means can find clusters **even** when there aren't any clusters. 


```r
res &lt;- pisa %&gt;% select(ESCS, BSMJ) %&gt;% kmeans(centers = 3)
pisa$clust &lt;- factor(res$cluster, levels = 1:3, ordered = TRUE)
ggplot(pisa, aes(ESCS, BSMJ, color = clust)) +
  geom_point(alpha = 1/3) +
  scale_x_continuous("Index of economic, social and cultural status of family") +
  scale_y_continuous("Students expected occupational status") +
  theme_minimal()
```

&lt;img src="kmeans_competition_files/figure-html/unnamed-chunk-13-1.png" style="display: block; margin: auto;" /&gt;

---

## No free lunch

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

.center[
.middle[
&gt; The 'No free lunch' theorem is a simple axiom that states that since every predictive algorithm has different assumptions, no single model is known to perform better than all others *a priori*
]
]
&lt;br&gt;
&lt;br&gt;

**Lucky for us: social scientists are not only interested in predictive accuracy**

---

## Causal Inference

* Growing interest from the social science literature on achieving causal inference using tree-based methods:

  + Athey, Susan, and Guido Imbens. "Recursive partitioning for heterogeneous causal effects." Proceedings of the National Academy of Sciences 113.27 (2016): 7353-7360
  
  + Brand, Jennie E., et al. "Uncovering Sociological Effect Heterogeneity using Machine Learning." arXiv preprint arXiv:1909.09138 (2019)


* Tease out heterogeneity in variation to achieve causal inference

* Explore interactions in a causal fashion

---

## Inference

* We can use machine learning methods for exploring new hypothesis in the data

* Avoid overfitting by train/testing and resampling

* Tree-based methods and regularized regressions can help us understand variables which are very good for prediction but that we weren't aware of:

  + Arpino, B., Le Moglie, M., and Mencarini, L. (2018). Machine-Learning techniques for family demography: An application of random forests to the analysis of divorce determinants in Germany

* Understand the role of interactions from a more intuitive point of view through exploration

* This includes unsupervised methods such as `\(PCA\)` and K-Means clustering.

---

## Prediction

If prediction is the aim, then there's evidence that some models consistently achieve greater accuracy is different settings:

* Tree based methods
  + Random Forests 
  + Gradient Boosting
  
* Neural Networks
* Support Vector Machines

&gt; Don't forget our training: we need to explore our data and understand it. This can help a lot in figuring out why some models work more than others.

---

## Prediction challenge

* 2019 Summer Institute In Computational Social Science (SICSS)
  + Mark Verhagen
  
  + Christopher Barrie
  
  + Arun Frey
  
  + Pablo Beyt√≠a
  
  + Arran Davis
  
  + Jorge Cimentada

* All metadata on counties in the United States

* Counties with different poverty levels have varying edits and pageviews

---

## Prediction challenge

* Your task: **build a predictive model of the number of edits**

* Can help identify which sites are not being capture by poverty/metadata indicators

* 150 columns, including Wiki data on the website and characteristics of the county

* Ideas
  + Does it make sense to reduce the number of correlated variables into a few principal components?
  + Do some counties cluster on very correlated variables? Is it fesiable to summarize some of these variables through predicting the cluster membership?
  + Do we really need to use all variables?
  + Does regularized regression or tree-based methods do better?

Read: https://cimentadaj.github.io/ml_socsci/no-free-lunch.html#prediction-challenge

You have 45 minutes, start!
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
