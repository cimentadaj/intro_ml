<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Tree-based methods | Machine Learning for Social Scientists</title>
  <meta name="description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Tree-based methods | Machine Learning for Social Scientists" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://cimentadaj.github.io/ml_socsci/" />
  
  <meta property="og:description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  <meta name="github-repo" content="cimentadaj/ml_socsci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Tree-based methods | Machine Learning for Social Scientists" />
  
  <meta name="twitter:description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  

<meta name="author" content="Jorge Cimentada" />


<meta name="date" content="2020-06-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regularization.html"/>
<link rel="next" href="syllabus.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.13/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning for Social Scientists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html"><i class="fa fa-check"></i><b>1</b> Machine Learning for Social Scientists</a><ul>
<li class="chapter" data-level="1.1" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#a-different-way-of-thinking"><i class="fa fa-check"></i><b>1.1</b> A different way of thinking</a></li>
<li class="chapter" data-level="1.2" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#split-your-data-into-trainingtesting"><i class="fa fa-check"></i><b>1.2</b> Split your data into training/testing</a></li>
<li class="chapter" data-level="1.3" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#cross-validation"><i class="fa fa-check"></i><b>1.3</b> Cross-validation</a></li>
<li class="chapter" data-level="1.4" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>1.4</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="1.5" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#an-example"><i class="fa fa-check"></i><b>1.5</b> An example</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>2</b> Regularization</a><ul>
<li class="chapter" data-level="2.1" data-path="regularization.html"><a href="regularization.html#ridge-regularization"><i class="fa fa-check"></i><b>2.1</b> Ridge regularization</a></li>
<li class="chapter" data-level="2.2" data-path="regularization.html"><a href="regularization.html#lasso-regularization"><i class="fa fa-check"></i><b>2.2</b> Lasso regularization</a></li>
<li class="chapter" data-level="2.3" data-path="regularization.html"><a href="regularization.html#elastic-net-regularization"><i class="fa fa-check"></i><b>2.3</b> Elastic Net regularization</a></li>
<li class="chapter" data-level="2.4" data-path="regularization.html"><a href="regularization.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>3</b> Tree-based methods</a><ul>
<li class="chapter" data-level="3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.1</b> Decision trees</a><ul>
<li class="chapter" data-level="3.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#advanced-how-do-trees-choose-where-to-split"><i class="fa fa-check"></i><b>3.1.1</b> Advanced: how do trees choose where to split?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging"><i class="fa fa-check"></i><b>3.2</b> Bagging</a></li>
<li class="chapter" data-level="3.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercises-1"><i class="fa fa-check"></i><b>3.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="syllabus.html"><a href="syllabus.html"><i class="fa fa-check"></i><b>4</b> Syllabus</a><ul>
<li class="chapter" data-level="4.1" data-path="syllabus.html"><a href="syllabus.html#course-description"><i class="fa fa-check"></i><b>4.1</b> Course description</a></li>
<li class="chapter" data-level="4.2" data-path="syllabus.html"><a href="syllabus.html#schedule"><i class="fa fa-check"></i><b>4.2</b> Schedule</a></li>
<li class="chapter" data-level="4.3" data-path="syllabus.html"><a href="syllabus.html#software"><i class="fa fa-check"></i><b>4.3</b> Software</a></li>
<li class="chapter" data-level="4.4" data-path="syllabus.html"><a href="syllabus.html#prerequisites"><i class="fa fa-check"></i><b>4.4</b> Prerequisites</a></li>
<li class="chapter" data-level="4.5" data-path="syllabus.html"><a href="syllabus.html#about-the-author"><i class="fa fa-check"></i><b>4.5</b> About the author</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Social Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tree-based-methods" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Tree-based methods</h1>
<p>In this chapter we will touch upon the most popular tree-based methods used in machine learning. Haven’t heard of the term “tree-based methods”? Do not panic. The idea behind tree-based methods is very simple and we’ll explain how they work step by step through the basics. Most of the material on this chapter was built upon <span class="citation">Boehmke and Greenwell (<a href="#ref-boehmke2019" role="doc-biblioref">2019</a>)</span> and <span class="citation">James et al. (<a href="#ref-james2013" role="doc-biblioref">2013</a>)</span>.</p>
<p>Before we begin, let’s load <code>tidyflow</code> and <code>tidymodels</code> and read the data that we’ll be using.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="tree-based-methods.html#cb43-1"></a><span class="kw">library</span>(tidymodels)</span>
<span id="cb43-2"><a href="tree-based-methods.html#cb43-2"></a><span class="kw">library</span>(tidyflow)</span>
<span id="cb43-3"><a href="tree-based-methods.html#cb43-3"></a><span class="kw">library</span>(rpart.plot)</span>
<span id="cb43-4"><a href="tree-based-methods.html#cb43-4"></a><span class="kw">library</span>(baguette)</span>
<span id="cb43-5"><a href="tree-based-methods.html#cb43-5"></a></span>
<span id="cb43-6"><a href="tree-based-methods.html#cb43-6"></a>data_link &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot;</span></span>
<span id="cb43-7"><a href="tree-based-methods.html#cb43-7"></a>pisa &lt;-<span class="st"> </span><span class="kw">read.csv</span>(data_link)</span></code></pre></div>
<div id="decision-trees" class="section level2">
<h2><span class="header-section-number">3.1</span> Decision trees</h2>
<p>Decision trees are simple models. In fact, they are even simpler than linear models. They require little statistical background and are in fact among the simplest models to communicate to a general audience. In particular, the visualizations used for decision trees are very powerful in conveying information and can even serve as an exploratory avenue for social research.</p>
<p>Throughout this chapter, we’ll be using the PISA data set from the regularization chapter. On this example we’ll be focusing on predicting the <code>math_score</code> of students in the United States, based on the socio economic status of the parents (named <code>HISEI</code> in the data; the higher the <code>HISEI</code> variable, the higher the socio economic status), the father’s education (named <code>FISCED</code> in the data; coded as several categories from 0 to 6 where 6 is high education) and whether the child repeated a grade (named <code>REPEAT</code> in the data). <code>REPEAT</code> is a dummy variable where <code>1</code> means the child repeated a grade and <code>0</code> no repetition.</p>
<p>Decision trees, as their name conveys, are tree-like diagrams. They work by defining <code>yes-or-no</code> rules based on the data and assign the most common value for each respondent within their final branch. The best way to learn about decision trees is by looking at one. Let’s do that:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>In this example the top-most box which says <code>HISEI &lt; 56</code> is the <strong>root node</strong>. This is the most important variable that predicts <code>math_score</code>. Inside the blue box you can see two numbers: <span class="math inline">\(100\%\)</span> which means that the entire sample is present in this <strong>node</strong> and the number <code>474</code>, the average test score for mathematics for the entire sample:</p>
<p>On both sides of the root node (<code>HISEI &lt; 56</code>) there is a <code>yes</code> and a <code>no</code>. Decision trees work by <strong>partitioning</strong> variables into <code>yes-or-no</code> branches. The <code>yes</code> branch satisfies the name of <strong>root</strong> (<code>HISEI &lt; 56</code>) and always branches out to the left:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>In contrast, the <code>no</code> branch always branches out to the right:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>The criteria for separating into <code>yes-or-no</code> branches is that respondents must be very similar within branches and very different between branches (later in this chapter I will explain in detail which criteria is used and how). The decision tree figures out that respondents that have an <code>HISEI</code> below <span class="math inline">\(56\)</span> and above <span class="math inline">\(56\)</span> are the most different with respect to the mathematics score. The left branch (where there is a <code>yes</code> in the <strong>root node</strong>) are those which have a <code>HISEI</code> below 56 and the right branch (where there is a <code>no</code>) are those which have a <code>HISEI</code> above <span class="math inline">\(56\)</span>. Let’s call these two groups the low and high SES respectively. If we look at the two boxes that come down from these branches, the low SES branch has an average math score of <span class="math inline">\(446\)</span> while the high SES branch has an average test score of <span class="math inline">\(501\)</span>:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>For the sake of simplicity, let’s focus now on the branch of the low SES group (the left branch). The second node coming out of the low SES branch contains 50% of the sample and an average math score of <span class="math inline">\(446\)</span>. This is the node with the rule <code>REPEAT &gt;= 0.5</code>:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>This ‘intermediate’ node is called <strong>internal node</strong>. For calculating this <strong>internal node</strong>, the decision tree algorithm limits the entire data set to only those which have low SES (literally, the decision tree does something like <code>pisa[pisa$HISEI &lt; 56, ]</code>) and asks the same question that it did in the <strong>root node</strong>: of all the variables in the model which one separates two branches such that respondents are very similar within the branch but very different between the branches with respect to <code>math_score</code>?</p>
<p>For those with low SES background, this variable is whether the child repeated a grade or not. In particular, those coming from low SES background which repeated a grade, had an average math score of <span class="math inline">\(387\)</span> whereas those who didn’t have an average math score of <span class="math inline">\(456\)</span>:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>These two nodes at the bottom are called <strong>leaf nodes</strong> because they are like the ‘leafs of the tree’. <strong>Leaf nodes</strong> are of particular importance because they are the ones that dictate what the final value of <code>math_score</code> will be. Any new data that is predicted with this model will always give an average <code>math_score</code> of <span class="math inline">\(456\)</span> for those of low SES background who didn’t repeat a grade:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Similarly, any respondent from high SES background, with a highly educated father who didn’t repeat a grade, will get assigned a <code>math_score</code> of <span class="math inline">\(527\)</span>:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>That is it. That is a decision tree in it’s simplest form. It contains a <strong>root node</strong> and several <strong>internal</strong> and <strong>leaf nodes</strong> and it can be interpreted just as we just did. The right branch of the tree can be summarized with the same interpretation. For example, for high SES respondents, father’s education (<code>FISCED</code>) is more important than <code>REPEAT</code> to separate between math scores:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>This is the case because it comes first in the tree. Substantially, this might be due to the fact that there is higher variation in education credentials for parents of high SES background than for those of low SES background. We can see that those with the highest father’s education (<code>FISCED</code> above <span class="math inline">\(5.5\)</span>), the average math score is <span class="math inline">\(524\)</span> whereas those with father’s education below <span class="math inline">\(5.5\)</span> have a math score of <span class="math inline">\(478\)</span>.</p>
<p>Did you notice that we haven’t interpreted any coefficients? That’s right. Decision trees have no coefficients and many other machine learning algorithms also don’t produce coefficients. Although for the case of decision trees this is because the model produces information in another way (through the visualization of trees), lack of coefficients is common in machine learning models because they are too complex to generate coefficients for single predictors. These models are non-linear, non-parametric in nature, producing very complex relationships that are difficult to summarize as coefficients. Instead, they produce predictions. We’ll be delving into this topic in future sections in detail.</p>
<p>These examples show that decision trees are a great tool for exploratory analysis and I strongly believe they have an inmense potential for exploring interactions in social science research. In case you didn’t notice it, we literally just interpreted an interaction term that social scientists would routinely use in linear models. Without having to worry about statistical significance or plotting marginal effects, social scientists can use decision trees as an exploratory medium to understand interactions in an intuitive way.</p>
<p>You might be asking yourself, how do we fit these models and visualize them? <code>tidyflow</code> and <code>tidymodels</code> have got you covered. For example, for fitting the model from above, we can begin our <code>tidyflow</code>, add a split, a formula and define the decision tree:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="tree-based-methods.html#cb44-1"></a><span class="co"># Define the decision tree and tell it the the dependent</span></span>
<span id="cb44-2"><a href="tree-based-methods.html#cb44-2"></a><span class="co"># variable is continuous (&#39;mode&#39; = &#39;regression&#39;)</span></span>
<span id="cb44-3"><a href="tree-based-methods.html#cb44-3"></a>mod1 &lt;-<span class="st"> </span><span class="kw">set_engine</span>(<span class="kw">decision_tree</span>(<span class="dt">mode =</span> <span class="st">&quot;regression&quot;</span>), <span class="st">&quot;rpart&quot;</span>)</span>
<span id="cb44-4"><a href="tree-based-methods.html#cb44-4"></a></span>
<span id="cb44-5"><a href="tree-based-methods.html#cb44-5"></a>tflow &lt;-</span>
<span id="cb44-6"><a href="tree-based-methods.html#cb44-6"></a><span class="st">  </span><span class="co"># Plug the data</span></span>
<span id="cb44-7"><a href="tree-based-methods.html#cb44-7"></a><span class="st">  </span>pisa <span class="op">%&gt;%</span></span>
<span id="cb44-8"><a href="tree-based-methods.html#cb44-8"></a><span class="st">  </span><span class="co"># Begin the tidyflow</span></span>
<span id="cb44-9"><a href="tree-based-methods.html#cb44-9"></a><span class="st">  </span><span class="kw">tidyflow</span>(<span class="dt">seed =</span> <span class="dv">23151</span>) <span class="op">%&gt;%</span></span>
<span id="cb44-10"><a href="tree-based-methods.html#cb44-10"></a><span class="st">  </span><span class="co"># Separate the data into training/testing</span></span>
<span id="cb44-11"><a href="tree-based-methods.html#cb44-11"></a><span class="st">  </span><span class="kw">plug_split</span>(initial_split) <span class="op">%&gt;%</span></span>
<span id="cb44-12"><a href="tree-based-methods.html#cb44-12"></a><span class="st">  </span><span class="co"># Plug the formula</span></span>
<span id="cb44-13"><a href="tree-based-methods.html#cb44-13"></a><span class="st">  </span><span class="kw">plug_formula</span>(math_score <span class="op">~</span><span class="st"> </span>FISCED <span class="op">+</span><span class="st"> </span>HISEI <span class="op">+</span><span class="st"> </span>REPEAT) <span class="op">%&gt;%</span></span>
<span id="cb44-14"><a href="tree-based-methods.html#cb44-14"></a><span class="st">  </span><span class="co"># Plug the model</span></span>
<span id="cb44-15"><a href="tree-based-methods.html#cb44-15"></a><span class="st">  </span><span class="kw">plug_model</span>(mod1)</span>
<span id="cb44-16"><a href="tree-based-methods.html#cb44-16"></a></span>
<span id="cb44-17"><a href="tree-based-methods.html#cb44-17"></a>vanilla_fit &lt;-<span class="st"> </span><span class="kw">fit</span>(tflow)</span>
<span id="cb44-18"><a href="tree-based-methods.html#cb44-18"></a>tree &lt;-<span class="st"> </span><span class="kw">pull_tflow_fit</span>(vanilla_fit)<span class="op">$</span>fit</span>
<span id="cb44-19"><a href="tree-based-methods.html#cb44-19"></a><span class="kw">rpart.plot</span>(tree)</span></code></pre></div>
<p><img src="03_trees_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>All <code>plug_*</code> functions serve to build your machine learning workflow and the model <code>decision_tree</code> serves to define the decision tree and all of the arguments. <code>rpart.plot</code> on the other hand, is a function used specifically for plotting decision trees (that is why we loaded the package <code>rpart.plot</code> at the beginning). No need to delve much into this function. It just works if you pass it a decision tree model: that is why <code>pull</code> the model fit before calling it.</p>
<p>I’ve told all the good things about decision trees but they have important disadvantages. There are two that we’ll discuss in this chapter. The first one is that decision trees tend to overfit a lot. Just for the sake of exemplifying this, let’s switch to another example. Let’s say we’re trying to understand which variables are related to whether teachers set goals in the classroom. Substantially, this example might not make a lot of sense, but but let’s follow along just to show how much trees can overfit the data. This variable is named <code>ST102Q01TA</code>. Let’s plug it into our <code>tidyflow</code> and visualize the tree:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="tree-based-methods.html#cb45-1"></a><span class="co">## ST100Q01TA</span></span>
<span id="cb45-2"><a href="tree-based-methods.html#cb45-2"></a><span class="co">## ST102Q01TA</span></span>
<span id="cb45-3"><a href="tree-based-methods.html#cb45-3"></a><span class="co">## IC009Q07NA</span></span>
<span id="cb45-4"><a href="tree-based-methods.html#cb45-4"></a><span class="co">## ST011Q03TA</span></span>
<span id="cb45-5"><a href="tree-based-methods.html#cb45-5"></a><span class="co">## ST011Q05TA</span></span>
<span id="cb45-6"><a href="tree-based-methods.html#cb45-6"></a><span class="co">## ST011Q10TA</span></span>
<span id="cb45-7"><a href="tree-based-methods.html#cb45-7"></a></span>
<span id="cb45-8"><a href="tree-based-methods.html#cb45-8"></a><span class="co"># We can recicle the entire `tflow` from above and just</span></span>
<span id="cb45-9"><a href="tree-based-methods.html#cb45-9"></a><span class="co"># replace the formula:</span></span>
<span id="cb45-10"><a href="tree-based-methods.html#cb45-10"></a>tflow &lt;-</span>
<span id="cb45-11"><a href="tree-based-methods.html#cb45-11"></a><span class="st">  </span>tflow <span class="op">%&gt;%</span></span>
<span id="cb45-12"><a href="tree-based-methods.html#cb45-12"></a><span class="st">  </span><span class="kw">replace_formula</span>(ST102Q01TA <span class="op">~</span><span class="st"> </span>.)</span>
<span id="cb45-13"><a href="tree-based-methods.html#cb45-13"></a></span>
<span id="cb45-14"><a href="tree-based-methods.html#cb45-14"></a>fit_complex &lt;-<span class="st"> </span><span class="kw">fit</span>(tflow)</span>
<span id="cb45-15"><a href="tree-based-methods.html#cb45-15"></a>tree &lt;-<span class="st"> </span><span class="kw">pull_tflow_fit</span>(fit_complex)<span class="op">$</span>fit</span>
<span id="cb45-16"><a href="tree-based-methods.html#cb45-16"></a><span class="kw">rpart.plot</span>(tree)</span></code></pre></div>
<p><img src="03_trees_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>The tree is quite big compared to our previous example and makes the interpretation more difficult. However, equally important, some <strong>leaf nodes</strong> are very small. Decision trees can capture a lot of noise and mimic the data very closely. <span class="math inline">\(6\)</span> <strong>leaf nodes</strong> have less than <span class="math inline">\(3\%\)</span> of the sample. These are <strong>leaf nodes</strong> with very weak statistical power:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>What would happen if a tiny <span class="math inline">\(1\%\)</span> of those <strong>leaf nodes</strong> respondend <strong>slightly</strong> different? It is possible we get a complete different tree. Decision trees are not well known for being robust. In fact, it is one of its main weaknesses. However, decision trees have an argument called <code>min_n</code> that force the tree to discard any <strong>node</strong> that has a number of observations below your specified minimum. Let’s run the model above and set the minimum number of observation per <strong>node</strong> to be <span class="math inline">\(200\)</span>:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="tree-based-methods.html#cb46-1"></a>dectree &lt;-<span class="st"> </span><span class="kw">update</span>(mod1, <span class="dt">min_n =</span> <span class="dv">200</span>)</span>
<span id="cb46-2"><a href="tree-based-methods.html#cb46-2"></a>tflow &lt;-</span>
<span id="cb46-3"><a href="tree-based-methods.html#cb46-3"></a><span class="st">  </span>tflow <span class="op">%&gt;%</span></span>
<span id="cb46-4"><a href="tree-based-methods.html#cb46-4"></a><span class="st">  </span><span class="kw">replace_model</span>(dectree)</span>
<span id="cb46-5"><a href="tree-based-methods.html#cb46-5"></a></span>
<span id="cb46-6"><a href="tree-based-methods.html#cb46-6"></a>fit_complex &lt;-<span class="st"> </span><span class="kw">fit</span>(tflow)</span>
<span id="cb46-7"><a href="tree-based-methods.html#cb46-7"></a>tree &lt;-<span class="st"> </span><span class="kw">pull_tflow_fit</span>(fit_complex)<span class="op">$</span>fit</span>
<span id="cb46-8"><a href="tree-based-methods.html#cb46-8"></a><span class="kw">rpart.plot</span>(tree)</span></code></pre></div>
<p><img src="03_trees_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>The tree was reduced considerably now. There are fewer <strong>leaf nodes</strong> and all nodes have a greater sample size than before.</p>
<p>You might be wondering: what should the minimum sample size be? There is no easy answer for this. The rule of thumb should be relative to your data and research question. In particular, the identification of small nodes should be analyzed with care. Perhaps there <strong>is</strong> a group of outliers that consitute a node and it’s not a problem of statistical noise. By increasing the minimum sample size for each node you would be destroying that statistical finding.</p>
<p>For example, suppose we are studying welfare social expenditure as the dependent variable and then we had other independent variables, among which are country names. Scandinavian countries might group pretty well into a solitary node because they are super powers in welfare spending (these are Denmark, Norway, Sweden and Finland). If we increased the minimum sample size to <span class="math inline">\(10\)</span>, we might group them with Germany and France, which are completely different in substantive terms. The best rule of thumb I can recommend is no other than to study your problem at hand with great care and make decisions accordingly. It might make sense to increase the sample or it might not depending on the research question, the sample size, whether you’re exploring the data or whether you’re interested in predicting on new data.</p>
<p>Despite <code>min_n</code> helping to make the tree more robust, there are still several nodes with low sample sizes. Another way to approach this problem is through the depth of the tree. As can be seen from the previous plot, decision trees can create <strong>leaf nodes</strong> which are very small. In other more complicated scenarios, your tree might get huge. Yes, huge:</p>
<p><img src="img/large_tree.png" width="100%" /></p>
<p>More often that not, these huge trees are just overfitting the data. They are creating very small nodes that capture noise from the data and when you’re predicting on new data, they perform terribly bad. As well as the <code>min_n</code> argument, decision trees have another argument called <code>tree_depth</code>. This argument forces the tree to stop growing if it passes the maximum depth of the tree as measured in nodes. Let’s run our previous example with only a depth of three nodes:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="tree-based-methods.html#cb47-1"></a>dectree &lt;-<span class="st"> </span><span class="kw">update</span>(mod1, <span class="dt">min_n =</span> <span class="dv">200</span>, <span class="dt">tree_depth =</span> <span class="dv">3</span>)</span>
<span id="cb47-2"><a href="tree-based-methods.html#cb47-2"></a>tflow &lt;-</span>
<span id="cb47-3"><a href="tree-based-methods.html#cb47-3"></a><span class="st">  </span>tflow <span class="op">%&gt;%</span></span>
<span id="cb47-4"><a href="tree-based-methods.html#cb47-4"></a><span class="st">  </span><span class="kw">replace_model</span>(dectree)</span>
<span id="cb47-5"><a href="tree-based-methods.html#cb47-5"></a></span>
<span id="cb47-6"><a href="tree-based-methods.html#cb47-6"></a>fit_complex &lt;-<span class="st"> </span><span class="kw">fit</span>(tflow)</span>
<span id="cb47-7"><a href="tree-based-methods.html#cb47-7"></a>tree &lt;-<span class="st"> </span><span class="kw">pull_tflow_fit</span>(fit_complex)<span class="op">$</span>fit</span>
<span id="cb47-8"><a href="tree-based-methods.html#cb47-8"></a><span class="kw">rpart.plot</span>(tree)</span></code></pre></div>
<p><img src="03_trees_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>The tree was reduced considerably now in combination with the minimun number of respondents within each node. In fact, there is only one node that has a sample size lower than <span class="math inline">\(3\%\)</span>. The <code>min_n</code> and <code>tree_depth</code> can help you reduce the overfitting of your tree, but don’t think these are easy fixes. Decision trees are simply to easy to overfit the data and as we’ll see, there are more advanced tree methods that can help to fix this.</p>
<p>Note that we’ve been interpreting decision trees in a ‘subjective’ fashion. That is, we’ve been cutting the nodes of the trees from subjective criteria that makes sense to our research problem. This is how we social scientists would analyze the data. The tree should model our theoretical problem and make substantive sense. However, for machine learning, we have other criteria: how well it predicts. Let’s check how our model predicts at this point:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="tree-based-methods.html#cb48-1"></a>fit_complex <span class="op">%&gt;%</span></span>
<span id="cb48-2"><a href="tree-based-methods.html#cb48-2"></a><span class="st">  </span><span class="kw">predict_training</span>() <span class="op">%&gt;%</span></span>
<span id="cb48-3"><a href="tree-based-methods.html#cb48-3"></a><span class="st">  </span><span class="kw">rmse</span>(ST102Q01TA, .pred)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       0.514</code></pre>
<p>Our predictions for each set goals is off by around <span class="math inline">\(.5\)</span> in a scale of <span class="math inline">\(1\)</span> through <span class="math inline">\(4\)</span>. This is not terribly bad. For example, it means that for every child that answered a <span class="math inline">\(2\)</span>, on average, we have an error of around <span class="math inline">\(.5\)</span>. This means that any prediction for a single number runs the risk of being wrongly predicting either the number from above or below (a child with a <span class="math inline">\(2\)</span> might get a wrong prediction of <span class="math inline">\(3\)</span> or a <span class="math inline">\(1\)</span> but hardly a <span class="math inline">\(4\)</span>). To improve prediction, we can allow <code>tidyflow</code> to search for the best combination of <code>min_n</code> and <code>tree_depth</code> that maximizes prediction. Let’s perform a grid search for these two tuning values. However, let’s set the range of tuning values ourselves:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="tree-based-methods.html#cb50-1"></a>tune_mod &lt;-<span class="st"> </span><span class="kw">update</span>(dectree, <span class="dt">min_n =</span> <span class="kw">tune</span>(), <span class="dt">tree_depth =</span> <span class="kw">tune</span>())</span>
<span id="cb50-2"><a href="tree-based-methods.html#cb50-2"></a></span>
<span id="cb50-3"><a href="tree-based-methods.html#cb50-3"></a>tflow &lt;-</span>
<span id="cb50-4"><a href="tree-based-methods.html#cb50-4"></a><span class="st">  </span>tflow <span class="op">%&gt;%</span></span>
<span id="cb50-5"><a href="tree-based-methods.html#cb50-5"></a><span class="st">  </span><span class="kw">plug_resample</span>(vfold_cv, <span class="dt">v =</span> <span class="dv">5</span>) <span class="op">%&gt;%</span></span>
<span id="cb50-6"><a href="tree-based-methods.html#cb50-6"></a><span class="st">  </span><span class="kw">plug_grid</span>(</span>
<span id="cb50-7"><a href="tree-based-methods.html#cb50-7"></a>    expand.grid,</span>
<span id="cb50-8"><a href="tree-based-methods.html#cb50-8"></a>    <span class="dt">tree_depth =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">9</span>),</span>
<span id="cb50-9"><a href="tree-based-methods.html#cb50-9"></a>    <span class="dt">min_n =</span> <span class="kw">c</span>(<span class="dv">50</span>, <span class="dv">100</span>)</span>
<span id="cb50-10"><a href="tree-based-methods.html#cb50-10"></a>  ) <span class="op">%&gt;%</span></span>
<span id="cb50-11"><a href="tree-based-methods.html#cb50-11"></a><span class="st">  </span><span class="kw">replace_model</span>(tune_mod)</span>
<span id="cb50-12"><a href="tree-based-methods.html#cb50-12"></a></span>
<span id="cb50-13"><a href="tree-based-methods.html#cb50-13"></a>fit_tuned &lt;-<span class="st"> </span><span class="kw">fit</span>(tflow)</span>
<span id="cb50-14"><a href="tree-based-methods.html#cb50-14"></a></span>
<span id="cb50-15"><a href="tree-based-methods.html#cb50-15"></a>fit_tuned <span class="op">%&gt;%</span></span>
<span id="cb50-16"><a href="tree-based-methods.html#cb50-16"></a><span class="st">  </span><span class="kw">pull_tflow_fit_tuning</span>() <span class="op">%&gt;%</span></span>
<span id="cb50-17"><a href="tree-based-methods.html#cb50-17"></a><span class="st">  </span><span class="kw">show_best</span>(<span class="dt">metric =</span> <span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 x 7
##   tree_depth min_n .metric .estimator  mean     n std_err
##        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1          9    50 rmse    standard   0.459     5  0.0126
## 2          9   100 rmse    standard   0.459     5  0.0126
## 3          3    50 rmse    standard   0.518     5  0.0116
## 4          3   100 rmse    standard   0.518     5  0.0116
## 5          1    50 rmse    standard   0.649     5  0.0102</code></pre>
<p>It seems that our predictions on the training data were slightly overfitting the data, as the best error from the cross-validation search is centered around <code>0.459</code> with a standard error of <code>0.01</code>. Let’s explore whether the error changes between the minimum sample size and the tree depth:</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="tree-based-methods.html#cb52-1"></a>tree_depth_lvl &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;Tree depth: &quot;</span>, <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">9</span>))</span>
<span id="cb52-2"><a href="tree-based-methods.html#cb52-2"></a></span>
<span id="cb52-3"><a href="tree-based-methods.html#cb52-3"></a>fit_tuned <span class="op">%&gt;%</span></span>
<span id="cb52-4"><a href="tree-based-methods.html#cb52-4"></a><span class="st">  </span><span class="kw">pull_tflow_fit_tuning</span>() <span class="op">%&gt;%</span></span>
<span id="cb52-5"><a href="tree-based-methods.html#cb52-5"></a><span class="st">  </span><span class="kw">collect_metrics</span>() <span class="op">%&gt;%</span></span>
<span id="cb52-6"><a href="tree-based-methods.html#cb52-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">ci_low =</span> mean <span class="op">-</span><span class="st"> </span>(<span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>std_err),</span>
<span id="cb52-7"><a href="tree-based-methods.html#cb52-7"></a>         <span class="dt">ci_high =</span> mean <span class="op">+</span><span class="st"> </span>(<span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>std_err),</span>
<span id="cb52-8"><a href="tree-based-methods.html#cb52-8"></a>         <span class="dt">tree_depth =</span> <span class="kw">factor</span>(<span class="kw">paste0</span>(<span class="st">&quot;Tree depth: &quot;</span>, tree_depth), <span class="dt">levels =</span> tree_depth_lvl),</span>
<span id="cb52-9"><a href="tree-based-methods.html#cb52-9"></a>         <span class="dt">min_n =</span> <span class="kw">factor</span>(min_n, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;50&quot;</span>, <span class="st">&quot;100&quot;</span>))) <span class="op">%&gt;%</span></span>
<span id="cb52-10"><a href="tree-based-methods.html#cb52-10"></a><span class="st">  </span><span class="kw">filter</span>(.metric <span class="op">==</span><span class="st"> &quot;rmse&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb52-11"><a href="tree-based-methods.html#cb52-11"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(min_n, mean)) <span class="op">+</span></span>
<span id="cb52-12"><a href="tree-based-methods.html#cb52-12"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb52-13"><a href="tree-based-methods.html#cb52-13"></a><span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> ci_low, <span class="dt">ymax =</span> ci_high), <span class="dt">width =</span> <span class="fl">.1</span>) <span class="op">+</span></span>
<span id="cb52-14"><a href="tree-based-methods.html#cb52-14"></a><span class="st">  </span><span class="kw">scale_x_discrete</span>(<span class="st">&quot;Minimum sample size per node&quot;</span>) <span class="op">+</span></span>
<span id="cb52-15"><a href="tree-based-methods.html#cb52-15"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="st">&quot;Average RMSE&quot;</span>) <span class="op">+</span></span>
<span id="cb52-16"><a href="tree-based-methods.html#cb52-16"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>tree_depth, <span class="dt">nrow =</span> <span class="dv">1</span>) <span class="op">+</span></span>
<span id="cb52-17"><a href="tree-based-methods.html#cb52-17"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="03_trees_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>On the <code>x</code> axis we have the minimum sample size per node (these are the values for <code>min_n</code>) and on the <code>y</code> axis we have the error of the model through cross-validation (the <span class="math inline">\(RMSE\)</span>). The lower each points is on the plot, the better, since it means that the error is lower.</p>
<p>Let’s begin with the first plot on the left. The points represent the error of the model with different sample sizes for the nodes with a fixed tree depth of <span class="math inline">\(1\)</span>. For a tree depth of <span class="math inline">\(1\)</span>, the error of the model is around <code>.65</code>. However, as the number of trees increases (the additional <em>plots</em> to the right), the error comes down to nearly <code>.47</code> when there is a <code>tree_depth</code> of 9. It seems that the simplest model with the lowest <span class="math inline">\(RMSE\)</span> has a <code>tree_depth</code> of 9 and a minimum sample size of 50. We calculated this ourselves for this example, but <code>complete_tflow</code> can calculate this for you:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="tree-based-methods.html#cb53-1"></a>final_model &lt;-</span>
<span id="cb53-2"><a href="tree-based-methods.html#cb53-2"></a><span class="st">  </span>fit_tuned <span class="op">%&gt;%</span></span>
<span id="cb53-3"><a href="tree-based-methods.html#cb53-3"></a><span class="st">  </span><span class="kw">complete_tflow</span>(<span class="dt">metric =</span> <span class="st">&quot;rmse&quot;</span>,</span>
<span id="cb53-4"><a href="tree-based-methods.html#cb53-4"></a>                 tree_depth,</span>
<span id="cb53-5"><a href="tree-based-methods.html#cb53-5"></a>                 <span class="dt">method =</span> <span class="st">&quot;select_by_one_std_err&quot;</span>)</span>
<span id="cb53-6"><a href="tree-based-methods.html#cb53-6"></a></span>
<span id="cb53-7"><a href="tree-based-methods.html#cb53-7"></a>train_err &lt;-</span>
<span id="cb53-8"><a href="tree-based-methods.html#cb53-8"></a><span class="st">  </span>final_model <span class="op">%&gt;%</span></span>
<span id="cb53-9"><a href="tree-based-methods.html#cb53-9"></a><span class="st">  </span><span class="kw">predict_training</span>() <span class="op">%&gt;%</span></span>
<span id="cb53-10"><a href="tree-based-methods.html#cb53-10"></a><span class="st">  </span><span class="kw">rmse</span>(ST102Q01TA, .pred)</span>
<span id="cb53-11"><a href="tree-based-methods.html#cb53-11"></a></span>
<span id="cb53-12"><a href="tree-based-methods.html#cb53-12"></a>test_err &lt;-</span>
<span id="cb53-13"><a href="tree-based-methods.html#cb53-13"></a><span class="st">  </span>final_model <span class="op">%&gt;%</span></span>
<span id="cb53-14"><a href="tree-based-methods.html#cb53-14"></a><span class="st">  </span><span class="kw">predict_testing</span>() <span class="op">%&gt;%</span></span>
<span id="cb53-15"><a href="tree-based-methods.html#cb53-15"></a><span class="st">  </span><span class="kw">rmse</span>(ST102Q01TA, .pred)</span>
<span id="cb53-16"><a href="tree-based-methods.html#cb53-16"></a></span>
<span id="cb53-17"><a href="tree-based-methods.html#cb53-17"></a><span class="kw">c</span>(<span class="st">&quot;testing error&quot;</span> =<span class="st"> </span>test_err<span class="op">$</span>.estimate, <span class="st">&quot;training error&quot;</span> =<span class="st"> </span>train_err<span class="op">$</span>.estimate)</span></code></pre></div>
<pre><code>##  testing error training error 
##      0.4644939      0.4512248</code></pre>
<p>Our testing error and our training error have a difference of only <span class="math inline">\(0.01\)</span>, not bad. The cross-validation tuning seemed to have helped avoid a great deal of overfitting.</p>
<p>Before we go through the next section, I want to briefly mention an alternative to <code>tree_depth</code> and <code>min_n</code>. A technique called ‘tree pruning’ is also very common for modeling decision trees. It first grows a very large and complex tree and <strong>then</strong> starts pruning the leafs. This technique is also very useful but due to the lack of time, we won’t cover this in the course. You can check out the material on this technique from the resources outlined in the first paragraph of this section.</p>
<div id="advanced-how-do-trees-choose-where-to-split" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Advanced: how do trees choose where to split?</h3>
<p>Throughout most of the chapter we’ve seen that trees find optimal ‘splits’ that make the respondents very different between the splits and very similar within them. But how do decision trees make these splits? Let’s work out a simple example using the <code>HISEI</code> variable from the first model in this section.</p>
<p><code>HISEI</code> is an index for the socio-economic status of families. It’s continuous and has a distribution like this:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>As we saw in the first tree of this section, <code>HISEI</code> is the <strong>root node</strong>. To decide on the <strong>root node</strong>, the decision tree algorithm chooses a random location in the distribution of <code>HISEI</code> and draws a split:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>The two sides have an average <code>math_score</code> which serves as the baseline for how different these two groups are. At this point, the algorithm does something very simple: for each split, it calculates the <strong>R</strong>esidual <strong>S</strong>um of <strong>S</strong>quares (RSS). This is just the sum of the <code>math_score</code> of each respondent (<span class="math inline">\(math_i\)</span>) minus the average <code>math_score</code> (<span class="math inline">\(\hat{math}\)</span>) for that split squared. In other words, it applies the <span class="math inline">\(RSS\)</span> for each split:</p>
<p><span class="math display">\[\begin{equation}
RSS = \sum_{k = 1}^n(math_i - \hat{math})^2
\end{equation}\]</span></p>
<p>Each side of the split then has a corresponding <span class="math inline">\(RSS\)</span>:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>After that, it calculates the total <span class="math inline">\(RSS\)</span> of the split by adding the two <span class="math inline">\(RSS\)</span>:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>So far we should have a single random split with an associated <span class="math inline">\(RSS\)</span> for <span class="math inline">\(HISEI\)</span>. The decision tree algorithm is called <em>recursive binary splitting</em> because it is recursive: it repeats itself again many times. It repeats the strategy of <span class="math inline">\(Split\)</span> -&gt; <span class="math inline">\(RSS_{split}\)</span> -&gt; <span class="math inline">\(RSS_{total}\)</span> many times such that we get a distribution of splits and <span class="math inline">\(RSS\)</span> for <span class="math inline">\(HISEI\)</span>:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>This produces a distribution of random splits with an associated metric of fit (<span class="math inline">\(RSS\)</span>) for <span class="math inline">\(HISEI\)</span>. <em>Recursive binary splitting</em> applies this same logic to every single variable in the model such that you have a distribution of splits for every single variable:</p>
<pre><code>## # A tibble: 8 x 3
##   variable random_split total_rss            
##   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;                
## 1 &quot;HISEI&quot;  41.22        &quot;Total RSS: 34423362&quot;
## 2 &quot;HISEI&quot;  53.77        &quot;Total RSS: 34400218&quot;
## 3 &quot;HISEI&quot;  56.57        &quot;Total RSS: 32523560&quot;
## 4 &quot;&quot;       ...          &quot;&quot;                   
## 5 &quot;FISCED&quot; 2            &quot;Total RSS: 35901660&quot;
## 6 &quot;FISCED&quot; 1            &quot;Total RSS: 36085201&quot;
## 7 &quot;FISCED&quot; 5            &quot;Total RSS: 34083264&quot;
## 8 &quot;&quot;       ...          &quot;&quot;</code></pre>
<p>With such a distribution, the algorithm can objectively ask: which random split best separates the data into two branches with the lowest <span class="math inline">\(RSS\)</span>? And based on that answer, the first <strong>node</strong> is chosen. After this first node is chosen, two branches grow to both sides. The algorithm then applies exactly the same set of rules <em>recursively</em> for each branch until a maximum depth is reached.</p>
<p>Although this explanation will be in nearly all cases invisible to you, this intuition can help you understand better which criteria is used for choosing a split. For example, understanding how this splitting is done gives you insight into how outliers do not affect the selection of splits because the splitting criteria is random and navigates the entire distribution.</p>
<p>In addition, there might be cases where you might want to switch the <span class="math inline">\(RSS\)</span> for another loss function because it makes sense for your problem. For example, using decision trees with binary dependent variables merits another type of loss function: <em>Gini impurity</em>. We won’t delve into this but it serves as an example that these are things which are not fixed. These are decision that depend on your research problem and it might make sense to experiment with them if needed.</p>
</div>
</div>
<div id="bagging" class="section level2">
<h2><span class="header-section-number">3.2</span> Bagging</h2>
<p>The problem with decision trees is that even if you work really hard to avoid overfitting, they can be very susceptible to the exact format of the data. For some cases, you might even get completely different trees every time you run your model. Quite literally, running the same model might offer very different trees if some part of the sample changes. This small simluation predicts <code>math_score</code> on all variables in the <code>pisa</code> data set but samples different percentages of the whole sample:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>This drastic differences between trees is because decision trees have a lot of variance and very little bias. They learn the current data very well (little bias) but if you generalize them to new data, they can perform very badly (a lot of variance). This is where bagging, or <strong>B</strong>ootstrap <strong>Agg</strong>regation comes in.</p>
<p>Before we explain what bagging is all about, let’s spend a minute explaining what bootstrapping is. Let’s work out a manual example and limit our <code>pisa</code> dataset to only five rows, keep a few selected columns and add a unique id for each row:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="tree-based-methods.html#cb56-1"></a>sel_cols &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;math_score&quot;</span>, <span class="st">&quot;HISEI&quot;</span>, <span class="st">&quot;REPEAT&quot;</span>, <span class="st">&quot;IMMIG&quot;</span>, <span class="st">&quot;read_score&quot;</span>)</span>
<span id="cb56-2"><a href="tree-based-methods.html#cb56-2"></a>pisa_small &lt;-<span class="st"> </span>pisa[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, sel_cols]</span>
<span id="cb56-3"><a href="tree-based-methods.html#cb56-3"></a>pisa_small<span class="op">$</span>id &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">5</span></span>
<span id="cb56-4"><a href="tree-based-methods.html#cb56-4"></a>pisa_small</span></code></pre></div>
<pre><code>##   math_score HISEI REPEAT IMMIG read_score id
## 1   512.7125 28.60      0     1   544.2085  1
## 2   427.3615 59.89      0     1   432.2518  2
## 3   449.9545 39.02      0     1   503.9496  3
## 4   474.5553 26.60      0     1   437.7777  4
## 5   469.1545 76.65      0     1   535.9487  5</code></pre>
<p>Bootstraping is a statistical technique where you resample your data to such that some rows are randomly duplicated. We can do this manually in R:</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="tree-based-methods.html#cb58-1"></a><span class="co"># Sample from the number of rows in `pisa_small`</span></span>
<span id="cb58-2"><a href="tree-based-methods.html#cb58-2"></a><span class="co"># and allow certain numbers to be replaced.</span></span>
<span id="cb58-3"><a href="tree-based-methods.html#cb58-3"></a><span class="kw">set.seed</span>(<span class="dv">23551</span>)</span>
<span id="cb58-4"><a href="tree-based-methods.html#cb58-4"></a>row_index &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(pisa_small), <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span>
<span id="cb58-5"><a href="tree-based-methods.html#cb58-5"></a>pisa_small[row_index, ]</span></code></pre></div>
<pre><code>##     math_score HISEI REPEAT IMMIG read_score id
## 1     512.7125 28.60      0     1   544.2085  1
## 4     474.5553 26.60      0     1   437.7777  4
## 4.1   474.5553 26.60      0     1   437.7777  4
## 3     449.9545 39.02      0     1   503.9496  3
## 5     469.1545 76.65      0     1   535.9487  5</code></pre>
<p>We randomly sampled the same number of rows and got the respondent number four repeated twice. We can run this many times and get many <strong>resamples</strong> of our data:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="tree-based-methods.html#cb60-1"></a><span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="cf">function</span>(x) {</span>
<span id="cb60-2"><a href="tree-based-methods.html#cb60-2"></a>  row_index &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(pisa_small), <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span>
<span id="cb60-3"><a href="tree-based-methods.html#cb60-3"></a>  pisa_small[row_index, ]</span>
<span id="cb60-4"><a href="tree-based-methods.html#cb60-4"></a>})</span></code></pre></div>
<pre><code>## [[1]]
##     math_score HISEI REPEAT IMMIG read_score id
## 3     449.9545 39.02      0     1   503.9496  3
## 5     469.1545 76.65      0     1   535.9487  5
## 3.1   449.9545 39.02      0     1   503.9496  3
## 1     512.7125 28.60      0     1   544.2085  1
## 3.2   449.9545 39.02      0     1   503.9496  3
## 
## [[2]]
##     math_score HISEI REPEAT IMMIG read_score id
## 1     512.7125  28.6      0     1   544.2085  1
## 4     474.5553  26.6      0     1   437.7777  4
## 4.1   474.5553  26.6      0     1   437.7777  4
## 4.2   474.5553  26.6      0     1   437.7777  4
## 4.3   474.5553  26.6      0     1   437.7777  4</code></pre>
<p>Since the number of rows that we sample is random, in some instances we might the same row repeated 10 times, in others only 1 time and other even 0 times! This is what bootstrapping is all about. If we run <span class="math inline">\(10\)</span> bootstraps, it just means we got the same 10 datasets but with some rows repeated many times and others randomly removed.</p>
<p>Bootstrapping is mainly used to calculate statistics such as standard errors and standard deviations because it has very nice properties to estimate uncertainty in situations where its impossible to calculate it. However, it also has advantages for reducing the variance in models such as decision trees.</p>
<p>Let’s get back to how bagging works. Bagging works by bootstraping your data <span class="math inline">\(N\)</span> times and fitting <span class="math inline">\(N\)</span> decision trees. Each of these decision trees has a lot of variance because we allow the tree to overfit the data. The trick with bagging is that we <strong>average</strong> over the predictions of all the <span class="math inline">\(N\)</span> decision trees, improving the high variability of each single decision tree.</p>
<p>In the same spirit as before, let’s work out a manual example just so you can truly grasp that intuition. However, don’t worry, there are functions inside <code>tidymodels</code> and <code>tidyflow</code> that will perform all of this for you.</p>
<p>Let’s adapt the code from above to use the original <code>pisa</code> data, sample only 60% of the data in each bootstrap and generate 20 copies of our data with random picks of rows in each iteration:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="tree-based-methods.html#cb62-1"></a>pisa<span class="op">$</span>id &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(pisa)</span>
<span id="cb62-2"><a href="tree-based-methods.html#cb62-2"></a>bootstrap_pisa &lt;-</span>
<span id="cb62-3"><a href="tree-based-methods.html#cb62-3"></a><span class="st">  </span><span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">20</span>, <span class="cf">function</span>(x) {</span>
<span id="cb62-4"><a href="tree-based-methods.html#cb62-4"></a>    row_index &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(pisa) <span class="op">*</span><span class="st"> </span><span class="fl">.6</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span>
<span id="cb62-5"><a href="tree-based-methods.html#cb62-5"></a>    pisa[row_index, ]</span>
<span id="cb62-6"><a href="tree-based-methods.html#cb62-6"></a>  })</span></code></pre></div>
<p>The result is named <code>bootstrap_pisa</code> and is list with 20 data frames. You can inspect the first two with <code>bootstrap_pisa[[1]]</code> and <code>bootstrap_pisa[[2]]</code>. Inside each of these, there should be a data frame with 60% of the original number of rows of the <code>pisa</code> data where each row was randomly picked. Some of these might be repeated many times, other might just be there once and others might not even be there.</p>
<p>Let’s now loop over these 20 datasets, fit a decision tree to each one and predict on the original <code>pisa</code> data. The result of this loop should be 20 data frames each with a prediction for every respondent:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="tree-based-methods.html#cb63-1"></a>tflow &lt;-</span>
<span id="cb63-2"><a href="tree-based-methods.html#cb63-2"></a><span class="st">  </span><span class="kw">tidyflow</span>() <span class="op">%&gt;%</span></span>
<span id="cb63-3"><a href="tree-based-methods.html#cb63-3"></a><span class="st">  </span><span class="kw">plug_formula</span>(math_score <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span></span>
<span id="cb63-4"><a href="tree-based-methods.html#cb63-4"></a><span class="st">  </span><span class="kw">plug_model</span>(<span class="kw">decision_tree</span>(<span class="dt">mode =</span> <span class="st">&quot;regression&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">set_engine</span>(<span class="st">&quot;rpart&quot;</span>))</span>
<span id="cb63-5"><a href="tree-based-methods.html#cb63-5"></a></span>
<span id="cb63-6"><a href="tree-based-methods.html#cb63-6"></a>all_pred_models &lt;-</span>
<span id="cb63-7"><a href="tree-based-methods.html#cb63-7"></a><span class="st">  </span><span class="kw">lapply</span>(bootstrap_pisa, <span class="cf">function</span>(x) {</span>
<span id="cb63-8"><a href="tree-based-methods.html#cb63-8"></a>    small_model &lt;-</span>
<span id="cb63-9"><a href="tree-based-methods.html#cb63-9"></a><span class="st">      </span>tflow <span class="op">%&gt;%</span></span>
<span id="cb63-10"><a href="tree-based-methods.html#cb63-10"></a><span class="st">      </span><span class="kw">plug_data</span>(x) <span class="op">%&gt;%</span></span>
<span id="cb63-11"><a href="tree-based-methods.html#cb63-11"></a><span class="st">      </span><span class="kw">fit</span>()</span>
<span id="cb63-12"><a href="tree-based-methods.html#cb63-12"></a></span>
<span id="cb63-13"><a href="tree-based-methods.html#cb63-13"></a>    <span class="kw">cbind</span>(</span>
<span id="cb63-14"><a href="tree-based-methods.html#cb63-14"></a>      pisa[<span class="st">&quot;id&quot;</span>],</span>
<span id="cb63-15"><a href="tree-based-methods.html#cb63-15"></a>      <span class="kw">predict</span>(small_model, <span class="dt">new_data =</span> pisa)</span>
<span id="cb63-16"><a href="tree-based-methods.html#cb63-16"></a>    )</span>
<span id="cb63-17"><a href="tree-based-methods.html#cb63-17"></a>  })</span></code></pre></div>
<p>The first slot contains predictions for all respondents. Let’s look confirm that:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="tree-based-methods.html#cb64-1"></a><span class="kw">head</span>(all_pred_models[[<span class="dv">1</span>]])</span></code></pre></div>
<pre><code>##   id    .pred
## 1  1 493.6071
## 2  2 378.5172
## 3  3 440.5835
## 4  4 440.5835
## 5  5 493.6071
## 6  6 440.5835</code></pre>
<p>Let’s confirm the same thing for the second slot:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="tree-based-methods.html#cb66-1"></a><span class="kw">head</span>(all_pred_models[[<span class="dv">2</span>]])</span></code></pre></div>
<pre><code>##   id    .pred
## 1  1 486.7747
## 2  2 432.6909
## 3  3 432.6909
## 4  4 432.6909
## 5  5 486.7747
## 6  6 486.7747</code></pre>
<p>The second slot also contains predictions for all respondents but they are different from the first one because they are based on a random sample. This same logic is repeated 20 times such that every respondent has 20 predictions. The trick behind bagging is that it <strong>averages</strong> the prediction of each respondent over the 20 bootstraps.</p>
<p>This averaging has two advantages. First, it allows each single tree to grow as much as possible, allowing it to have a lot of variance and little bias. This has a good property which is little bias but a negative aspect, which is a lot of variance. Bagging compensates this high level of variance by averaging the predictions of all the small trees:</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="tree-based-methods.html#cb68-1"></a><span class="co"># Combine all the 20 predictions into one data frame</span></span>
<span id="cb68-2"><a href="tree-based-methods.html#cb68-2"></a>all_combined &lt;-<span class="st"> </span>all_pred_models[[<span class="dv">1</span>]]</span>
<span id="cb68-3"><a href="tree-based-methods.html#cb68-3"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_along</span>(all_pred_models)[<span class="op">-</span><span class="dv">1</span>]) {</span>
<span id="cb68-4"><a href="tree-based-methods.html#cb68-4"></a>  all_combined &lt;-<span class="st"> </span><span class="kw">cbind</span>(all_combined, all_pred_models[[i]][<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb68-5"><a href="tree-based-methods.html#cb68-5"></a>}</span>
<span id="cb68-6"><a href="tree-based-methods.html#cb68-6"></a></span>
<span id="cb68-7"><a href="tree-based-methods.html#cb68-7"></a><span class="co"># Average over the 20 predictions</span></span>
<span id="cb68-8"><a href="tree-based-methods.html#cb68-8"></a>res &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">id =</span> all_combined[<span class="dv">1</span>], <span class="dt">final_pred =</span> <span class="kw">rowMeans</span>(all_combined[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb68-9"><a href="tree-based-methods.html#cb68-9"></a></span>
<span id="cb68-10"><a href="tree-based-methods.html#cb68-10"></a><span class="kw">head</span>(res)</span></code></pre></div>
<pre><code>##   id final_pred
## 1  1   494.1934
## 2  2   403.6330
## 3  3   436.1936
## 4  4   443.5922
## 5  5   491.6506
## 6  6   457.9670</code></pre>
<p>We get a final prediction for each respondent. If we wanted to, we could calculate the standard deviation of these 20 predictions for each respondent and generate uncertainty intervals around each respondent’s predictions. More often than not, this is a good idea.</p>
<p>In the previous example we used 20 bootstraps for the sake of simplicity but generally speaking as the number of trees increases, the less variance we will have in the final prediction and thus a stronger prediction. We can see more clearly the power of combining many trees with the simulation below:</p>
<p><img src="img/bagging_sim.png" width="80%" style="display: block; margin: auto;" /></p>
<p>The <code>x</code> axis shows the number of bootstraps (or fitted trees, it’s the same) and the <code>y</code> axis shows the average <span class="math inline">\(RMSE\)</span> in <code>math_score</code> for each of these bagged trees. As we increase the number of small trees (or bootstraps, it’s the same), there is a substantial reduction in the error rate of <code>math_score</code>. This is an impressive reduction relative to our initial decision tree.</p>
<p>Having seen the power of increasing the number of trees, how many trees should your model use? For models which exhibit reasonable levels of variability (like our <code>math_score</code> example), <span class="math inline">\(100\)</span>-<span class="math inline">\(200\)</span> bootstraps is often enough to stabilize the error in the predictions. However, very unstable models might require up to <span class="math inline">\(500\)</span>.</p>
<p>Let’s fit the same model we implemented manually above using <code>tidymodels</code> and <code>tidyflow</code>. Bagged trees can be implemented with the function <code>bag_tree</code> from the package <code>baguette</code>. With this package we can control the number of bootstraps with the argument <code>times</code>. We can define our model as usual using <code>tidyflow</code>:</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="tree-based-methods.html#cb70-1"></a>btree &lt;-<span class="st"> </span><span class="kw">bag_tree</span>(<span class="dt">mode =</span> <span class="st">&quot;regression&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">set_engine</span>(<span class="st">&quot;rpart&quot;</span>, <span class="dt">times =</span> <span class="dv">50</span>)</span>
<span id="cb70-2"><a href="tree-based-methods.html#cb70-2"></a></span>
<span id="cb70-3"><a href="tree-based-methods.html#cb70-3"></a>tflow &lt;-</span>
<span id="cb70-4"><a href="tree-based-methods.html#cb70-4"></a><span class="st">  </span>pisa <span class="op">%&gt;%</span></span>
<span id="cb70-5"><a href="tree-based-methods.html#cb70-5"></a><span class="st">  </span><span class="kw">tidyflow</span>(<span class="dt">seed =</span> <span class="dv">566521</span>) <span class="op">%&gt;%</span></span>
<span id="cb70-6"><a href="tree-based-methods.html#cb70-6"></a><span class="st">  </span><span class="kw">plug_split</span>(initial_split) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb70-7"><a href="tree-based-methods.html#cb70-7"></a><span class="st">  </span><span class="kw">plug_formula</span>(math_score <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span></span>
<span id="cb70-8"><a href="tree-based-methods.html#cb70-8"></a><span class="st">  </span><span class="kw">plug_model</span>(btree)</span>
<span id="cb70-9"><a href="tree-based-methods.html#cb70-9"></a></span>
<span id="cb70-10"><a href="tree-based-methods.html#cb70-10"></a>tflow</span></code></pre></div>
<pre><code>## ══ Tidyflow ════════════════════════════════════════════════════════════════════
## Data: 4.84K rows x 502 columns
## Split: initial_split w/ default args
## Formula: math_score ~ .
## Resample: None
## Grid: None
## Model:
## Bagged Decision Tree Model Specification (regression)
## 
## Main Arguments:
##   cost_complexity = 0
##   min_n = 2
## 
## Engine-Specific Arguments:
##   times = 50
## 
## Computational engine: rpart</code></pre>
<p>You might be asking yourself, why don’t we define <code>bootstraps</code> inside <code>plug_resample</code>? After all,<code>bootstraps</code> <strong>is</strong> a resampling technique. We could do that but it doesn’t make sense in this context. <code>plug_resample</code> is aimed more towards doing grid search of tuning values together with <code>plug_grid</code>. Since <code>bag_trees</code> is not performing any type grid search but rather fitting a model many times and making predictions, it automatically incorporates this procedure inside <code>bag_trees</code>. If instead we were doing a grid search of let’s say, <code>min_n</code> and <code>tree_depth</code> for <code>bag_tree</code>, using <code>plug_resample</code> with <code>boostraps</code> would be perfectly reasonable.</p>
<p>Let’s fit both a simple decision tree and the bagged decision tree, predict on the training set and record the average <span class="math inline">\(RMSE\)</span> for both:</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="tree-based-methods.html#cb72-1"></a>res_btree &lt;-<span class="st"> </span>tflow <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>()</span>
<span id="cb72-2"><a href="tree-based-methods.html#cb72-2"></a>res_dtree &lt;-<span class="st"> </span>tflow <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">replace_model</span>(<span class="kw">decision_tree</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">set_engine</span>(<span class="st">&quot;rpart&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>()</span>
<span id="cb72-3"><a href="tree-based-methods.html#cb72-3"></a></span>
<span id="cb72-4"><a href="tree-based-methods.html#cb72-4"></a>rmse_dtree &lt;-</span>
<span id="cb72-5"><a href="tree-based-methods.html#cb72-5"></a><span class="st">  </span>res_dtree <span class="op">%&gt;%</span></span>
<span id="cb72-6"><a href="tree-based-methods.html#cb72-6"></a><span class="st">  </span><span class="kw">predict_training</span>() <span class="op">%&gt;%</span></span>
<span id="cb72-7"><a href="tree-based-methods.html#cb72-7"></a><span class="st">  </span><span class="kw">rmse</span>(math_score, .pred)</span>
<span id="cb72-8"><a href="tree-based-methods.html#cb72-8"></a></span>
<span id="cb72-9"><a href="tree-based-methods.html#cb72-9"></a>rmse_btree &lt;-</span>
<span id="cb72-10"><a href="tree-based-methods.html#cb72-10"></a><span class="st">  </span>res_btree <span class="op">%&gt;%</span></span>
<span id="cb72-11"><a href="tree-based-methods.html#cb72-11"></a><span class="st">  </span><span class="kw">predict_training</span>() <span class="op">%&gt;%</span></span>
<span id="cb72-12"><a href="tree-based-methods.html#cb72-12"></a><span class="st">  </span><span class="kw">rmse</span>(math_score, .pred)</span>
<span id="cb72-13"><a href="tree-based-methods.html#cb72-13"></a></span>
<span id="cb72-14"><a href="tree-based-methods.html#cb72-14"></a><span class="kw">c</span>(<span class="st">&quot;Decision tree&quot;</span> =<span class="st"> </span>rmse_dtree<span class="op">$</span>.estimate, <span class="st">&quot;Bagged decision tree&quot;</span> =<span class="st"> </span>rmse_btree<span class="op">$</span>.estimate)</span></code></pre></div>
<pre><code>##        Decision tree Bagged decision tree 
##             33.85131             11.33018</code></pre>
<p>The bagged decision tree improves the error rate from <span class="math inline">\(33\)</span> math test points to <span class="math inline">\(11\)</span>. That is a <span class="math inline">\(66\%\)</span> reduction in the error rate! That is an impressive improvement for such a simple extension of decision trees.</p>
<p>As all other models, bagging also has limitations. First, although bagged decision trees offer improved predictions over decision trees, they do this at the expense of interpretability. Unfortunately, there is no equivalent of an ‘average’ tree that we can visualize. Remember, we have <span class="math inline">\(100\)</span> predictions from <span class="math inline">\(100\)</span> different trees. It is not possible nor advisable to visualize <span class="math inline">\(100\)</span> trees. Instead, we can look at the average variable importance. Bagging offers the ‘contribution’ of each variable using loss functions. For continuous variables, it uses the <span class="math inline">\(RSS\)</span> (which we have described and used throughout this chapter) and for binary variables it uses the Gini index. We can look at the importance of the variables to get a notion of which variables are the important ones for good prediction:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="tree-based-methods.html#cb74-1"></a>res_btree <span class="op">%&gt;%</span></span>
<span id="cb74-2"><a href="tree-based-methods.html#cb74-2"></a><span class="st">  </span><span class="kw">pull_tflow_fit</span>() <span class="op">%&gt;%</span></span>
<span id="cb74-3"><a href="tree-based-methods.html#cb74-3"></a><span class="st">  </span>.[[<span class="st">&#39;fit&#39;</span>]] <span class="op">%&gt;%</span></span>
<span id="cb74-4"><a href="tree-based-methods.html#cb74-4"></a><span class="st">  </span><span class="kw">var_imp</span>()</span></code></pre></div>
<pre><code>## # A tibble: 501 x 4
##    term           value std.error  used
##    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;
##  1 scie_score 23363949.    75426.    50
##  2 read_score 17033482.    69939.    50
##  3 ST166Q03HA  5913918.    66479.    50
##  4 METASPAM    5671665.    68871.    50
##  5 IC152Q08HA  3850699.   304274.    49
##  6 PISADIFF    3046729.   362250.    50
##  7 IC010Q06NA  2691482.   355147.    50
##  8 ST013Q01TA   433681.   142604.    50
##  9 ESCS         329367.    16981.    50
## 10 HOMEPOS      258437.    11440.    50
## # … with 491 more rows</code></pre>
<p>Secondly, bagging might seem like a deal breaker for <strong>any</strong> type of model (you can apply it to any type of model such as logistic regression, regularized regression, etc..) but it works well only for models which are very unstable. For example, linear regression and logistic regression are very robust models. With enough sample size, running a bagged linear regression should return very similar estimates as a single fitted model.</p>
</div>
<div id="exercises-1" class="section level2">
<h2><span class="header-section-number">3.3</span> Exercises</h2>
<p>Explain that you have not performed any grid search for bagged trees but that they have to do it themselves.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-boehmke2019">
<p>Boehmke, Brad, and Brandon M Greenwell. 2019. <em>Hands-on Machine Learning with R</em>. CRC Press.</p>
</div>
<div id="ref-james2013">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regularization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="syllabus.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
