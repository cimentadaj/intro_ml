<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Tree-based methods | Machine Learning for Social Scientists</title>
  <meta name="description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Tree-based methods | Machine Learning for Social Scientists" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://cimentadaj.github.io/ml_socsci/" />
  
  <meta property="og:description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  <meta name="github-repo" content="cimentadaj/ml_socsci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Tree-based methods | Machine Learning for Social Scientists" />
  
  <meta name="twitter:description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  

<meta name="author" content="Jorge Cimentada" />


<meta name="date" content="2020-06-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regularization.html"/>
<link rel="next" href="syllabus.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.13/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning for Social Scientists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html"><i class="fa fa-check"></i><b>1</b> Machine Learning for Social Scientists</a><ul>
<li class="chapter" data-level="1.1" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#a-different-way-of-thinking"><i class="fa fa-check"></i><b>1.1</b> A different way of thinking</a></li>
<li class="chapter" data-level="1.2" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#split-your-data-into-trainingtesting"><i class="fa fa-check"></i><b>1.2</b> Split your data into training/testing</a></li>
<li class="chapter" data-level="1.3" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#cross-validation"><i class="fa fa-check"></i><b>1.3</b> Cross-validation</a></li>
<li class="chapter" data-level="1.4" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>1.4</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="1.5" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#an-example"><i class="fa fa-check"></i><b>1.5</b> An example</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>2</b> Regularization</a><ul>
<li class="chapter" data-level="2.1" data-path="regularization.html"><a href="regularization.html#ridge-regularization"><i class="fa fa-check"></i><b>2.1</b> Ridge regularization</a></li>
<li class="chapter" data-level="2.2" data-path="regularization.html"><a href="regularization.html#lasso-regularization"><i class="fa fa-check"></i><b>2.2</b> Lasso regularization</a></li>
<li class="chapter" data-level="2.3" data-path="regularization.html"><a href="regularization.html#elastic-net-regularization"><i class="fa fa-check"></i><b>2.3</b> Elastic Net regularization</a></li>
<li class="chapter" data-level="2.4" data-path="regularization.html"><a href="regularization.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>3</b> Tree-based methods</a><ul>
<li class="chapter" data-level="3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.1</b> Decision trees</a></li>
<li class="chapter" data-level="3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#todo"><i class="fa fa-check"></i><b>3.2</b> TODO</a></li>
<li class="chapter" data-level="3.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#left-off-here"><i class="fa fa-check"></i><b>3.3</b> LEFT OFF HERE</a><ul>
<li class="chapter" data-level="3.3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#how-do-they-really-work"><i class="fa fa-check"></i><b>3.3.1</b> How do they really work?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="syllabus.html"><a href="syllabus.html"><i class="fa fa-check"></i><b>4</b> Syllabus</a><ul>
<li class="chapter" data-level="4.1" data-path="syllabus.html"><a href="syllabus.html#course-description"><i class="fa fa-check"></i><b>4.1</b> Course description</a></li>
<li class="chapter" data-level="4.2" data-path="syllabus.html"><a href="syllabus.html#schedule"><i class="fa fa-check"></i><b>4.2</b> Schedule</a></li>
<li class="chapter" data-level="4.3" data-path="syllabus.html"><a href="syllabus.html#software"><i class="fa fa-check"></i><b>4.3</b> Software</a></li>
<li class="chapter" data-level="4.4" data-path="syllabus.html"><a href="syllabus.html#prerequisites"><i class="fa fa-check"></i><b>4.4</b> Prerequisites</a></li>
<li class="chapter" data-level="4.5" data-path="syllabus.html"><a href="syllabus.html#about-the-author"><i class="fa fa-check"></i><b>4.5</b> About the author</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Social Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tree-based-methods" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Tree-based methods</h1>
<p>In this chapter we will touch upon the most popular tree-based methods used in machine learning. Haven’t heard of the term “tree-based methods”? Do not panic. The idea behind tree-based methods is very simple and we’ll explain how they work step by step through the basics. Most of the material on this chapter was built upon <span class="citation">Boehmke and Greenwell (<a href="#ref-boehmke2019" role="doc-biblioref">2019</a>)</span> and <span class="citation">James et al. (<a href="#ref-james2013" role="doc-biblioref">2013</a>)</span>.</p>
<p>Before we begin, let’s load <code>tidyflow</code> and <code>tidymodels</code> and read the data that we’ll be using.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="tree-based-methods.html#cb43-1"></a><span class="kw">library</span>(tidymodels)</span>
<span id="cb43-2"><a href="tree-based-methods.html#cb43-2"></a><span class="kw">library</span>(tidyflow)</span>
<span id="cb43-3"><a href="tree-based-methods.html#cb43-3"></a><span class="kw">library</span>(rpart.plot)</span>
<span id="cb43-4"><a href="tree-based-methods.html#cb43-4"></a></span>
<span id="cb43-5"><a href="tree-based-methods.html#cb43-5"></a>data_link &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot;</span></span>
<span id="cb43-6"><a href="tree-based-methods.html#cb43-6"></a>pisa &lt;-<span class="st"> </span><span class="kw">read.csv</span>(data_link)</span></code></pre></div>
<div id="decision-trees" class="section level2">
<h2><span class="header-section-number">3.1</span> Decision trees</h2>
<p>Decision trees are simple models. In fact, they are even simpler than linear models. They require little statistical background and are in fact among the simplest models to communicate to a general audience. In particular, the visualizations used for decision trees are very powerful in conveying information and can even serve as an exploratory avenue for social research.</p>
</div>
<div id="todo" class="section level2">
<h2><span class="header-section-number">3.2</span> TODO</h2>
<p>Add here that there are no coefficients. WHAT? Yes, you heard it right!</p>
<p>Throughout this chapter, we’ll be using the PISA data set from the regularization chapter. On this example we’ll be focusing on predicting the <code>math_score</code> of students in the United States, based on the socio economic status of the parents (named <code>HISEI</code> in the data; the higher the <code>HISEI</code> variable, the higher the socio economic status), the father’s education (named <code>FISCED</code> in the data; coded as several categories from 0 to 6 where 6 is high education) and whether the child repeated a grade (named <code>REPEAT</code> in the data). On the other hand, <code>REPEAT</code> is a dummy variable where <code>1</code> means the child repeated a grade and <code>0</code> no repetition.</p>
<p>Decision trees, as their name conveys, are tree-like diagrams. The work by defining yes-or-no rules based on the data to try to predict the most common value in each final branch. Let’s begin learning about decision trees by looking at one:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>In this example the top-most box which says <code>HISEI &lt; 56</code> is the <strong>root node</strong>. This is the most important variable that predicts <code>math_score</code>. Inside the blue box you can see two numbers: <span class="math inline">\(100\%\)</span> which means that the entire sample is present in this <strong>node</strong> and the number <code>474</code>, the average test score for mathematics for the entire sample:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>On both sides of the root node (<code>HISEI &lt; 56</code>) there is a <code>yes</code> and a <code>no</code>. Decision trees work by <strong>partitioning</strong> variables into <code>yes-or-no</code> branches. The <code>yes</code> branch satisfies the name of <strong>root</strong> (<code>HISEI &lt; 56</code>) and always branches out to the left:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>In contrast, the <code>no</code> branch always branches out to the right:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>The criteria for separating into <code>yes-or-no</code> branches is that respondents must be very similar within branches and very different between branches (later in this chapter I will explain in detail which criteria is used and how). The decision tree figures out that respondents that have an <code>HISEI</code> below <span class="math inline">\(56\)</span> and above <span class="math inline">\(56\)</span> are the most different with respect to the mathematics score. The left branch (where there is a <code>yes</code> in the <strong>root node</strong>) are those which have a <code>HISEI</code> below 56 and the right branch (where there is a <code>no</code>) are those which have a <code>HISEI</code> above <span class="math inline">\(56\)</span>. Let’s call these two groups the low and high SES respectively. If we look at the two boxes that come down from these branches, the low SES branch has an average math score of <span class="math inline">\(446\)</span> while the high SES branch has an average test score of <span class="math inline">\(501\)</span>:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>For the sake of simplicity, let’s focus now on the branch of the low SES group (the left branch). The second node coming out of the low SES branch contains 50% of the sample and an average math score of <span class="math inline">\(446\)</span>. This is the node with the rule <code>REPEAT &gt;= 0.5</code>:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>This ‘intermediate’ node is called <strong>internal node</strong>. For calculating this <strong>internal node</strong>, the decision tree algorithm limits the entire data set to only those which have low SES (literally, the decision tree does something like <code>pisa[pisa$HISEI &lt; 56, ]</code>) and asks the same question that it did in the <strong>root node</strong>: of all the variables in the model which one separates two branches such that respondents are very similar within the branch but very different between the branches with respect to <code>math_score</code>?</p>
<p>For those with low SES background, this variable is whether the child repeated a grade or not. In particular, those coming from low SES background which repeated a grade, had an average math score of <span class="math inline">\(387\)</span> whereas those who didn’t have an average math score of <span class="math inline">\(456\)</span>:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>These two nodes at the bottom are called <strong>leaf nodes</strong> because they are like the ‘leafs of the tree’. <strong>Leaf nodes</strong> are of particular importance because they are the ones that dictate what the final value of <code>math_score</code> will be. Any new data that is predicted with this model will always give an average <code>math_score</code> of <span class="math inline">\(456\)</span> for those of low SES background who didn’t repeat a grade:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Similarly, any respondent from high SES background, with a highly educated father who didn’t repeat a grade, will get assigned a <code>math_score</code> of <span class="math inline">\(527\)</span>:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>That is it. That is a decision tree in it’s simplest form. It contains a <strong>root node</strong> and several <strong>internal</strong> and <strong>leaf nodes</strong> and it can be interpreted just as we just did. The right branch of the tree can be summarized with the same interpretation. For example, for high SES respondents, father’s education (<code>FISCED</code>) is more important than <code>REPEAT</code> to separate between math scores:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>This is the case because it comes first in the tree. Substantially, this might be due to the fact that there is higher variation in education credentials for parents of high SES background than for those of low SES background. We can see that those with the highest father’s education (<code>FISCED</code> above <span class="math inline">\(5.5\)</span>), the average math score is <span class="math inline">\(524\)</span> whereas those with father’s education below <span class="math inline">\(5.5\)</span> have a math score of <span class="math inline">\(478\)</span>:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>I hope that these examples show that decision trees are a great tool for exploratory analysis and I strongly believe they have an inmense potential for exploring interactions in social science research. In case you didn’t notice it, we literally just interpreted an interaction term that social scientists would routinely use in linear models. Without having to worry about statistical significance or plotting marginal effects, social scientists can use decision trees as an exploratory medium to understand interactions in an intuitive way.</p>
<p>You might be asking yourself, how do we fit these models and visualize them? <code>tidyflow</code> and <code>tidymodels</code> have got you covered. For example, for fitting the model from above, we can begin our <code>tidyflow</code>, add a split, a formula and define the decision tree:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="tree-based-methods.html#cb44-1"></a><span class="co"># Define the decision tree and tell it the the dependent</span></span>
<span id="cb44-2"><a href="tree-based-methods.html#cb44-2"></a><span class="co"># variable is continuous (&#39;mode&#39; = &#39;regression&#39;)</span></span>
<span id="cb44-3"><a href="tree-based-methods.html#cb44-3"></a>mod1 &lt;-<span class="st"> </span><span class="kw">set_engine</span>(<span class="kw">decision_tree</span>(<span class="dt">mode =</span> <span class="st">&quot;regression&quot;</span>), <span class="st">&quot;rpart&quot;</span>)</span>
<span id="cb44-4"><a href="tree-based-methods.html#cb44-4"></a></span>
<span id="cb44-5"><a href="tree-based-methods.html#cb44-5"></a>tflow &lt;-</span>
<span id="cb44-6"><a href="tree-based-methods.html#cb44-6"></a><span class="st">  </span><span class="co"># Plug the data</span></span>
<span id="cb44-7"><a href="tree-based-methods.html#cb44-7"></a><span class="st">  </span>pisa <span class="op">%&gt;%</span></span>
<span id="cb44-8"><a href="tree-based-methods.html#cb44-8"></a><span class="st">  </span><span class="co"># Begin the tidyflow</span></span>
<span id="cb44-9"><a href="tree-based-methods.html#cb44-9"></a><span class="st">  </span><span class="kw">tidyflow</span>(<span class="dt">seed =</span> <span class="dv">23151</span>) <span class="op">%&gt;%</span></span>
<span id="cb44-10"><a href="tree-based-methods.html#cb44-10"></a><span class="st">  </span><span class="co"># Separate the data into training/testing</span></span>
<span id="cb44-11"><a href="tree-based-methods.html#cb44-11"></a><span class="st">  </span><span class="kw">plug_split</span>(initial_split) <span class="op">%&gt;%</span></span>
<span id="cb44-12"><a href="tree-based-methods.html#cb44-12"></a><span class="st">  </span><span class="co"># Plug the formula</span></span>
<span id="cb44-13"><a href="tree-based-methods.html#cb44-13"></a><span class="st">  </span><span class="kw">plug_formula</span>(math_score <span class="op">~</span><span class="st"> </span>FISCED <span class="op">+</span><span class="st"> </span>HISEI <span class="op">+</span><span class="st"> </span>REPEAT) <span class="op">%&gt;%</span></span>
<span id="cb44-14"><a href="tree-based-methods.html#cb44-14"></a><span class="st">  </span><span class="co"># Plug the model</span></span>
<span id="cb44-15"><a href="tree-based-methods.html#cb44-15"></a><span class="st">  </span><span class="kw">plug_model</span>(mod1)</span>
<span id="cb44-16"><a href="tree-based-methods.html#cb44-16"></a></span>
<span id="cb44-17"><a href="tree-based-methods.html#cb44-17"></a>vanilla_fit &lt;-<span class="st"> </span><span class="kw">fit</span>(tflow)</span>
<span id="cb44-18"><a href="tree-based-methods.html#cb44-18"></a>tree &lt;-<span class="st"> </span><span class="kw">pull_tflow_fit</span>(vanilla_fit)<span class="op">$</span>fit</span>
<span id="cb44-19"><a href="tree-based-methods.html#cb44-19"></a><span class="kw">rpart.plot</span>(tree)</span></code></pre></div>
<p>If you read the chapter on reguralization, the only thing new here should be <code>rpart.plot</code>. All <code>plug_*</code> functions serve to build your machine learning workflow and the model <code>decision_tree</code> is the equivalent of <code>linear_reg</code> that we saw in the previous chapter. We are just recycling the same code for this model. <code>rpart.plot</code> on the other hand, is a function used specifically for plotting the decision tree (that is why we loaded the package <code>rpart.plot</code> at the beginning). No need to delve much into this function. It just works if you pass it a decision tree model: that is why <code>pull</code> the model fit before calling it.</p>
<p>Now I’ve told all the good things about decision trees but they are not a smoking gun. They have serious limitations. In particular, there are two that we’ll discuss in this chapter. The first one is that decision trees tend to overfit a lot. Substantially, this example doesn’t make much sense but look at the percentages in the <strong>leaf nodes</strong>:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="tree-based-methods.html#cb45-1"></a>tflow &lt;-</span>
<span id="cb45-2"><a href="tree-based-methods.html#cb45-2"></a><span class="st">  </span>tflow <span class="op">%&gt;%</span></span>
<span id="cb45-3"><a href="tree-based-methods.html#cb45-3"></a><span class="st">  </span><span class="kw">replace_formula</span>(ST163Q03HA <span class="op">~</span><span class="st"> </span>.)</span>
<span id="cb45-4"><a href="tree-based-methods.html#cb45-4"></a></span>
<span id="cb45-5"><a href="tree-based-methods.html#cb45-5"></a>fit_complex &lt;-<span class="st"> </span><span class="kw">fit</span>(tflow)</span>
<span id="cb45-6"><a href="tree-based-methods.html#cb45-6"></a>tree &lt;-<span class="st"> </span><span class="kw">pull_tflow_fit</span>(fit_complex)<span class="op">$</span>fit</span>
<span id="cb45-7"><a href="tree-based-methods.html#cb45-7"></a><span class="kw">rpart.plot</span>(tree)</span></code></pre></div>
<p><img src="03_trees_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Decision trees can capture a lot of noise. <span class="math inline">\(5\)</span> out of the <span class="math inline">\(8\)</span> <strong>leaf nodes</strong> have less than <span class="math inline">\(2\%\)</span> of the sample. These are <strong>leaf nodes</strong> with very weak statistical power:</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>What would happen if a tiny %1% of those <strong>leaf nodes</strong> respondend <strong>slightly</strong> different? It is possible we get a complete different tree. Decision trees are not well known for being robust. In fact, it is one of its main weaknesses. However, decision trees have an argument called <code>min_n</code> that force the tree to discard any <strong>node</strong> that has a number of observations below your minimum. Let’s run the model above and set the minimum number of observation per <strong>node</strong> to be <span class="math inline">\(200\)</span>:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="tree-based-methods.html#cb46-1"></a>dectree &lt;-<span class="st"> </span><span class="kw">update</span>(mod1, <span class="dt">min_n =</span> <span class="dv">250</span>)</span>
<span id="cb46-2"><a href="tree-based-methods.html#cb46-2"></a>tflow &lt;-</span>
<span id="cb46-3"><a href="tree-based-methods.html#cb46-3"></a><span class="st">  </span>tflow <span class="op">%&gt;%</span></span>
<span id="cb46-4"><a href="tree-based-methods.html#cb46-4"></a><span class="st">  </span><span class="kw">replace_model</span>(dectree)</span>
<span id="cb46-5"><a href="tree-based-methods.html#cb46-5"></a></span>
<span id="cb46-6"><a href="tree-based-methods.html#cb46-6"></a>fit_complex &lt;-<span class="st"> </span><span class="kw">fit</span>(tflow)</span>
<span id="cb46-7"><a href="tree-based-methods.html#cb46-7"></a>tree &lt;-<span class="st"> </span><span class="kw">pull_tflow_fit</span>(fit_complex)<span class="op">$</span>fit</span>
<span id="cb46-8"><a href="tree-based-methods.html#cb46-8"></a><span class="kw">rpart.plot</span>(tree)</span></code></pre></div>
<p><img src="03_trees_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>The tree was reduced considerably now. There are fewer <strong>leaf nodes</strong> and all nodes have a greater sample size than before.</p>
<p>You might be wondering: what should the minimum sample size be? It depends. The rule of thumb should be relative to your data. In particular, the identification of small nodes should be analyzed with care. Perhaps there <strong>is</strong> a group of outliers that consitute a node and it’s not a problem of statistical noise. By increasing the minimum sample size for each node you would be destroying that statistical finding. For example, suppose we are studying welfare social expenditure as the dependent variable and then we had other independent variables, among which are country names. Scandinavian countries might group pretty well into a solitary node because they are super powers in welfare spending (these are Denmark, Norway, Sweden and Finland). If we increased the minimum sample size to <span class="math inline">\(10\)</span>, we might group them with Germany and France, which are completely different in substantive terms. The best rule of thumb I can recommend is no other than to study your problem at hand with great care and make decisions accordingly. It might make sense to increase the sample or it might not depending on the research question, the total sample size, whether you’re exploring the data or whether you’re interested in predicting on new data.</p>
<p>The second problem with decision trees is the tree depth. As can be seen from the previous plot, decision trees can create <strong>leaf nodes</strong> which are very small. In other more complicated scenarios, your tree might get huge. Yes, huge:</p>
<p><img src="img/large_tree.png" width="100%" /></p>
<p>More often that not, these huge trees are just overfitting the data. They are creating very small nodes that capture noise from the data and when you’re predicting on new data, they perform terribly bad.</p>
</div>
<div id="left-off-here" class="section level2">
<h2><span class="header-section-number">3.3</span> LEFT OFF HERE</h2>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="tree-based-methods.html#cb47-1"></a>dectree2 &lt;-<span class="st"> </span><span class="kw">set_engine</span>(<span class="kw">update</span>(mod1, <span class="dt">min_n =</span> <span class="dv">200</span>, <span class="dt">tree_depth =</span> <span class="dv">5</span>), <span class="st">&quot;rpart&quot;</span>, <span class="dt">model =</span> <span class="ot">TRUE</span>)</span>
<span id="cb47-2"><a href="tree-based-methods.html#cb47-2"></a></span>
<span id="cb47-3"><a href="tree-based-methods.html#cb47-3"></a>tflow &lt;-</span>
<span id="cb47-4"><a href="tree-based-methods.html#cb47-4"></a><span class="st">  </span>tflow <span class="op">%&gt;%</span></span>
<span id="cb47-5"><a href="tree-based-methods.html#cb47-5"></a><span class="st">  </span><span class="kw">replace_formula</span>(math_score <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>scie_score <span class="op">-</span><span class="st"> </span>read_score) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb47-6"><a href="tree-based-methods.html#cb47-6"></a><span class="st">  </span><span class="kw">replace_model</span>(dectree2)</span>
<span id="cb47-7"><a href="tree-based-methods.html#cb47-7"></a></span>
<span id="cb47-8"><a href="tree-based-methods.html#cb47-8"></a>fit_complex &lt;-<span class="st"> </span><span class="kw">fit</span>(tflow)</span>
<span id="cb47-9"><a href="tree-based-methods.html#cb47-9"></a>tree &lt;-<span class="st"> </span><span class="kw">pull_tflow_fit</span>(fit_complex)<span class="op">$</span>fit</span>
<span id="cb47-10"><a href="tree-based-methods.html#cb47-10"></a><span class="kw">rpart.plot</span>(tree)</span></code></pre></div>
<p><img src="03_trees_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Due to lack of space, we won’t cover tree pruning but it is a very important technique used to generate very complex trees and ‘prune’ them to avoid overfitting.</p>
<div id="how-do-they-really-work" class="section level3">
<h3><span class="header-section-number">3.3.1</span> How do they really work?</h3>
<p>Before we go into</p>
<p><img src="03_trees_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Then we apply a random split</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="tree-based-methods.html#cb48-1"></a>p2</span></code></pre></div>
<p><img src="03_trees_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="tree-based-methods.html#cb49-1"></a>p3</span></code></pre></div>
<p><img src="03_trees_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="tree-based-methods.html#cb50-1"></a>p4</span></code></pre></div>
<p><img src="03_trees_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="tree-based-methods.html#cb51-1"></a>p_many</span></code></pre></div>
<p><img src="03_trees_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre><code>## # A tibble: 8 x 3
##   variable random_split total_rss            
##   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;                
## 1 &quot;HISEI&quot;  41.22        &quot;Total RSS: 34423362&quot;
## 2 &quot;HISEI&quot;  53.77        &quot;Total RSS: 34400218&quot;
## 3 &quot;HISEI&quot;  56.57        &quot;Total RSS: 32523560&quot;
## 4 &quot;&quot;       ...          &quot;&quot;                   
## 5 &quot;FISCED&quot; 2            &quot;Total RSS: 35901660&quot;
## 6 &quot;FISCED&quot; 1            &quot;Total RSS: 36085201&quot;
## 7 &quot;FISCED&quot; 5            &quot;Total RSS: 34083264&quot;
## 8 &quot;&quot;       ...          &quot;&quot;</code></pre>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-boehmke2019">
<p>Boehmke, Brad, and Brandon M Greenwell. 2019. <em>Hands-on Machine Learning with R</em>. CRC Press.</p>
</div>
<div id="ref-james2013">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regularization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="syllabus.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
