<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Machine Learning for Social Scientists | Machine Learning for Social Scientists</title>
  <meta name="description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Machine Learning for Social Scientists | Machine Learning for Social Scientists" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://cimentadaj.github.io/ml_socsci/" />
  
  <meta property="og:description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  <meta name="github-repo" content="cimentadaj/ml_socsci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Machine Learning for Social Scientists | Machine Learning for Social Scientists" />
  
  <meta name="twitter:description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  

<meta name="author" content="Jorge Cimentada" />


<meta name="date" content="2020-07-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="regularization.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.13/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning for Social Scientists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html"><i class="fa fa-check"></i><b>1</b> Machine Learning for Social Scientists</a><ul>
<li class="chapter" data-level="1.1" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#a-different-way-of-thinking"><i class="fa fa-check"></i><b>1.1</b> A different way of thinking</a></li>
<li class="chapter" data-level="1.2" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#split-your-data-into-trainingtesting"><i class="fa fa-check"></i><b>1.2</b> Split your data into training/testing</a></li>
<li class="chapter" data-level="1.3" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#cross-validation"><i class="fa fa-check"></i><b>1.3</b> Cross-validation</a></li>
<li class="chapter" data-level="1.4" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>1.4</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="1.5" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#an-example"><i class="fa fa-check"></i><b>1.5</b> An example</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>2</b> Regularization</a><ul>
<li class="chapter" data-level="2.1" data-path="regularization.html"><a href="regularization.html#ridge-regularization"><i class="fa fa-check"></i><b>2.1</b> Ridge regularization</a></li>
<li class="chapter" data-level="2.2" data-path="regularization.html"><a href="regularization.html#lasso-regularization"><i class="fa fa-check"></i><b>2.2</b> Lasso regularization</a></li>
<li class="chapter" data-level="2.3" data-path="regularization.html"><a href="regularization.html#elastic-net-regularization"><i class="fa fa-check"></i><b>2.3</b> Elastic Net regularization</a></li>
<li class="chapter" data-level="2.4" data-path="regularization.html"><a href="regularization.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>3</b> Tree-based methods</a><ul>
<li class="chapter" data-level="3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.1</b> Decision trees</a><ul>
<li class="chapter" data-level="3.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#advancedsplit"><i class="fa fa-check"></i><b>3.1.1</b> Advanced: how do trees choose where to split?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging"><i class="fa fa-check"></i><b>3.2</b> Bagging</a></li>
<li class="chapter" data-level="3.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests"><i class="fa fa-check"></i><b>3.3</b> Random Forests</a></li>
<li class="chapter" data-level="3.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting"><i class="fa fa-check"></i><b>3.4</b> Boosting</a></li>
<li class="chapter" data-level="3.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercises-1"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="loss-functions.html"><a href="loss-functions.html"><i class="fa fa-check"></i><b>4</b> Loss functions</a><ul>
<li class="chapter" data-level="4.1" data-path="loss-functions.html"><a href="loss-functions.html#continuous-loss-functions"><i class="fa fa-check"></i><b>4.1</b> Continuous loss functions</a><ul>
<li class="chapter" data-level="4.1.1" data-path="loss-functions.html"><a href="loss-functions.html#root-mean-square-error-rmse"><i class="fa fa-check"></i><b>4.1.1</b> Root Mean Square Error (RMSE)</a></li>
<li class="chapter" data-level="4.1.2" data-path="loss-functions.html"><a href="loss-functions.html#mean-absolute-error"><i class="fa fa-check"></i><b>4.1.2</b> Mean Absolute Error</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="loss-functions.html"><a href="loss-functions.html#binary-loss-functions"><i class="fa fa-check"></i><b>4.2</b> Binary loss functions</a><ul>
<li class="chapter" data-level="4.2.1" data-path="loss-functions.html"><a href="loss-functions.html#confusion-matrices"><i class="fa fa-check"></i><b>4.2.1</b> Confusion Matrices</a></li>
<li class="chapter" data-level="4.2.2" data-path="loss-functions.html"><a href="loss-functions.html#roc-curves-and-area-under-the-curve"><i class="fa fa-check"></i><b>4.2.2</b> ROC Curves and Area Under the Curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html"><i class="fa fa-check"></i><b>5</b> Unsupervised methods</a><ul>
<li class="chapter" data-level="5.1" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>5.1</b> Principal Component Analysis (PCA)</a></li>
<li class="chapter" data-level="5.2" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html#k-means-clustering"><i class="fa fa-check"></i><b>5.2</b> K-Means Clustering</a></li>
<li class="chapter" data-level="5.3" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html#exercises-2"><i class="fa fa-check"></i><b>5.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="no-free-lunch.html"><a href="no-free-lunch.html"><i class="fa fa-check"></i><b>6</b> No free lunch</a><ul>
<li class="chapter" data-level="6.1" data-path="no-free-lunch.html"><a href="no-free-lunch.html#causal-inference"><i class="fa fa-check"></i><b>6.1</b> Causal Inference</a></li>
<li class="chapter" data-level="6.2" data-path="no-free-lunch.html"><a href="no-free-lunch.html#explaining-complex-models"><i class="fa fa-check"></i><b>6.2</b> Explaining complex models</a></li>
<li class="chapter" data-level="6.3" data-path="no-free-lunch.html"><a href="no-free-lunch.html#inference"><i class="fa fa-check"></i><b>6.3</b> Inference</a></li>
<li class="chapter" data-level="6.4" data-path="no-free-lunch.html"><a href="no-free-lunch.html#prediction"><i class="fa fa-check"></i><b>6.4</b> Prediction</a></li>
<li class="chapter" data-level="6.5" data-path="no-free-lunch.html"><a href="no-free-lunch.html#prediction-challenge"><i class="fa fa-check"></i><b>6.5</b> Prediction challenge</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="syllabus.html"><a href="syllabus.html"><i class="fa fa-check"></i><b>7</b> Syllabus</a><ul>
<li class="chapter" data-level="7.1" data-path="syllabus.html"><a href="syllabus.html#course-description"><i class="fa fa-check"></i><b>7.1</b> Course description</a></li>
<li class="chapter" data-level="7.2" data-path="syllabus.html"><a href="syllabus.html#schedule"><i class="fa fa-check"></i><b>7.2</b> Schedule</a></li>
<li class="chapter" data-level="7.3" data-path="syllabus.html"><a href="syllabus.html#software"><i class="fa fa-check"></i><b>7.3</b> Software</a></li>
<li class="chapter" data-level="7.4" data-path="syllabus.html"><a href="syllabus.html#prerequisites"><i class="fa fa-check"></i><b>7.4</b> Prerequisites</a></li>
<li class="chapter" data-level="7.5" data-path="syllabus.html"><a href="syllabus.html#about-the-author"><i class="fa fa-check"></i><b>7.5</b> About the author</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Social Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machine-learning-for-social-scientists" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Machine Learning for Social Scientists</h1>
<p>Machine Learning practitioners and social scientists share many things in common. These shared traits are mostly related to the transformation, analysis and evaluation of statistical models. In fact, when many of my fellow social scientists take any introductory course on machine learning, I often hear that many of the things they get taught are very common in their social statistics classes. This is good news! This means that you already have a foot inside the field without even knowing it. Machine Learning practitioners use many of the same statistical model we use and also many of transformation techniques that we use. However, there are important differences on how we analyze data and how we answer our questions. In this chapter I will elaborate on how machine learning practitioners have developed strategies different from social scientists for analyzing their data, how their analysis workflow compares to ours and finally, a tour around their way of thinking, which has evolved to be very different from ours.</p>
<p>I hope that by understanding the strategies and techniques that machine learning practitioners use, social scientists would expand their analysis toolbox, allowing us to complement their way of thinking with our strong research design skills and modeling techniques.</p>
<div id="a-different-way-of-thinking" class="section level2">
<h2><span class="header-section-number">1.1</span> A different way of thinking</h2>
<p>The first question we want to ask ourselves is, what is machine learning? Machine Learning bears indeed a fancy name which brings to mind thoughts related to artificial intelligence and robots. However, as you’ll see throughout the course, most terms and models used in machine learning are actually what we know as <strong>statistical models</strong>. The overarching difference in the definition of machine learning and social statistics is not the models or new strategies for analyzing data. It is the main objective of the analysis. What is machine learning after all?</p>
<blockquote>
<p>Using statistical methods to learn the data enough to be able to predict it accurately on new data</p>
</blockquote>
<p>That sounds somewhat familiar to us social scientists. Perhaps our goal is not to predict our data but it is certainly to <strong>learn it</strong> and <strong>understand it</strong>. In particular, social scientists are interested in figuring out if our theoretical description of a problem fits the data we have collected or have at hand. We do that by carefully building a model that explains the problem really well such that we can extrapolate an explanation for the problem from the data. Our gold standard to check whether we did a good job is to collect the exact same data again and see if our final models replicates. How does this differ from the way of thinking of machine learning practitioners? The main objective in a machine learning problem is accurate predictions; that is, regardless of how well they <strong>understand</strong> a problem, they want to learn the data enough to predict it well. Prediction problems are usually concerned with <strong>building and tweaking</strong> a model that predicts a dependent variable accurately on your data, such that when <strong>new data</strong> arrives, the model can predict it just as accurately. This does not mean that machine learning practitioners don’t have domain-specific knowledge of what they’re trying to predict (they have to select variables to include in a model just as we do). However, ‘parsimonious models’ (that is, simple and interpretable models) are not something they’re limited to (in contrast, social scientists hardly experiment with non-interpretable models). They might use models which contain up to hundreds of variables if that increases predictive accuracy. Although that might sound counter-intuitive to social scientists, more and more ground is being gained by this type of thinking in the social sciences <span class="citation">(Watts <a href="#ref-watts2014" role="doc-biblioref">2014</a>; Yarkoni and Westfall <a href="#ref-yarkoni2017" role="doc-biblioref">2017</a>)</span>.</p>
<p>The difference between how we both approach research questions is the problem of inference versus prediction <span class="citation">(Breiman and others <a href="#ref-breiman2001" role="doc-biblioref">2001</a>)</span>. That is the fundamental difference between the approach used by social scientists and practitioners of machine learning. However, for having such drastic differences in our objective, we share a lot of common strategies. For example, here’s the typical workflow of a social scientist:</p>
<p><img src="img/socsci_wflow1_smaller.svg" width="99%" style="display: block; margin: auto;" /></p>
<p>This is our safe zone: we understand these steps and we’ve exercised them many times. We begin by importing our data and immediately start to clean it. This involves, for example, collapsing fine grained groups into bigger categories, transforming variables using logarithms and creating new variables which reflect important concepts from our theoretical model. Once we’re confident with our set of variables, we begin the iterative process of visualizing our data, fitting statistical models and evaluating the fit of the model. This is an iterative process because the results of our model might give us ideas on new variables or how to recode an existing variable. This prompts us to repeat the same process again with the aim of carefully building a model that fits the data well. Well, let me break it to you but this same process is very familiar to the machine learning process:</p>
<p><img src="img/socsci_wflow3_smaller.svg" width="99%" style="display: block; margin: auto;" /></p>
<p>They import their data, they wrangle their data, they fit statistical models, and they evaluate the fit of their models. They might have different names for the same things but in essence, they are more or less the same. For example, here are some common terms in the machine learning literature which have exact equivalents in social statistics:</p>
<ul>
<li>Features –&gt; Variables</li>
<li>Feature Engineering –&gt; Creating Variables</li>
<li>Learning Algorithms –&gt; Statistical Models</li>
<li>Supervised Learning –&gt; Models that have a dependent variable</li>
<li>Unsupervised Learning –&gt; Models that don’t have a dependent variable, such as clustering</li>
<li>Classifiers –&gt; Models for predicting categorical variables, such as logistic regression</li>
</ul>
<p>and you’ll find more around. These are the common steps which you’ll find between both fields. However, machine learning practitioners have developed extra steps which help them achieve their goal of predicting new data well:</p>
<p><img src="img/socsci_wflow4_smaller.svg" width="99%" style="display: block; margin: auto;" /></p>
<ul>
<li>Training/Testing data –&gt; Unknown to us</li>
<li>Cross-validation –&gt; Unknown to us</li>
<li>Grid search –&gt; Unknown to us</li>
<li>Loss functions –&gt; Model fit –&gt; Known to us but are not predominant (<span class="math inline">\(RMSE\)</span>, <span class="math inline">\(R^2\)</span>, etc…)</li>
</ul>
<p>These are very useful concepts and we’ll focus on those in this introduction. In this introduction I won’t delve into the statistical models (learning algorithms) used in machine learning as these will be discussed in later chapters but I wanted to highlight that although they share similarities with the models used in social statistics, there are many models used in the machine learning literature which are unknown to us. Let’s delve into each of these three new concepts.</p>
<p>Before we beginning explaining these concepts and using R, let’s load the packages we’ll use in this chapter:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="machine-learning-for-social-scientists.html#cb1-1"></a><span class="co"># All these packages can be installed with `install.packages`</span></span>
<span id="cb1-2"><a href="machine-learning-for-social-scientists.html#cb1-2"></a><span class="kw">library</span>(ggplot2)</span>
<span id="cb1-3"><a href="machine-learning-for-social-scientists.html#cb1-3"></a><span class="kw">library</span>(patchwork)</span>
<span id="cb1-4"><a href="machine-learning-for-social-scientists.html#cb1-4"></a><span class="kw">library</span>(scales)</span>
<span id="cb1-5"><a href="machine-learning-for-social-scientists.html#cb1-5"></a><span class="kw">library</span>(tidymodels)</span></code></pre></div>
</div>
<div id="split-your-data-into-trainingtesting" class="section level2">
<h2><span class="header-section-number">1.2</span> Split your data into training/testing</h2>
<p>Since the main objective in machine learning is to predict data accurately, all of their strategies are geared towards avoiding overfitting/underfitting the data. In other words, they want to capture all the signal and ignore the noise:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="machine-learning-for-social-scientists.html#cb2-1"></a><span class="kw">set.seed</span>(<span class="dv">2313</span>)</span>
<span id="cb2-2"><a href="machine-learning-for-social-scientists.html#cb2-2"></a>n &lt;-<span class="st"> </span><span class="dv">500</span></span>
<span id="cb2-3"><a href="machine-learning-for-social-scientists.html#cb2-3"></a>x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n)</span>
<span id="cb2-4"><a href="machine-learning-for-social-scientists.html#cb2-4"></a>y &lt;-<span class="st"> </span>x<span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dt">sd =</span> <span class="dv">3</span>)</span>
<span id="cb2-5"><a href="machine-learning-for-social-scientists.html#cb2-5"></a>age &lt;-<span class="st"> </span><span class="kw">rescale</span>(x, <span class="dt">to =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">100</span>))</span>
<span id="cb2-6"><a href="machine-learning-for-social-scientists.html#cb2-6"></a>income &lt;-<span class="st"> </span><span class="kw">rescale</span>(y, <span class="dt">to =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">5000</span>))</span>
<span id="cb2-7"><a href="machine-learning-for-social-scientists.html#cb2-7"></a></span>
<span id="cb2-8"><a href="machine-learning-for-social-scientists.html#cb2-8"></a>age_inc &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">age =</span> age, <span class="dt">income =</span> income)</span>
<span id="cb2-9"><a href="machine-learning-for-social-scientists.html#cb2-9"></a></span>
<span id="cb2-10"><a href="machine-learning-for-social-scientists.html#cb2-10"></a>y_axis &lt;-<span class="st"> </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> <span class="kw">dollar_format</span>(<span class="dt">suffix =</span> <span class="st">&quot;€&quot;</span>, <span class="dt">prefix =</span> <span class="st">&quot;&quot;</span>),</span>
<span id="cb2-11"><a href="machine-learning-for-social-scientists.html#cb2-11"></a>                             <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">5000</span>),</span>
<span id="cb2-12"><a href="machine-learning-for-social-scientists.html#cb2-12"></a>                             <span class="dt">name =</span> <span class="st">&quot;Income&quot;</span>)</span>
<span id="cb2-13"><a href="machine-learning-for-social-scientists.html#cb2-13"></a></span>
<span id="cb2-14"><a href="machine-learning-for-social-scientists.html#cb2-14"></a>x_axis &lt;-<span class="st"> </span><span class="kw">scale_x_continuous</span>(<span class="dt">name =</span> <span class="st">&quot;Age&quot;</span>)</span>
<span id="cb2-15"><a href="machine-learning-for-social-scientists.html#cb2-15"></a></span>
<span id="cb2-16"><a href="machine-learning-for-social-scientists.html#cb2-16"></a>bad_fit &lt;-</span>
<span id="cb2-17"><a href="machine-learning-for-social-scientists.html#cb2-17"></a><span class="st">  </span><span class="kw">ggplot</span>(age_inc, <span class="kw">aes</span>(age, income)) <span class="op">+</span></span>
<span id="cb2-18"><a href="machine-learning-for-social-scientists.html#cb2-18"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb2-19"><a href="machine-learning-for-social-scientists.html#cb2-19"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span></span>
<span id="cb2-20"><a href="machine-learning-for-social-scientists.html#cb2-20"></a><span class="st">  </span>y_axis <span class="op">+</span></span>
<span id="cb2-21"><a href="machine-learning-for-social-scientists.html#cb2-21"></a><span class="st">  </span>x_axis <span class="op">+</span><span class="st">  </span></span>
<span id="cb2-22"><a href="machine-learning-for-social-scientists.html#cb2-22"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Underfit&quot;</span>) <span class="op">+</span></span>
<span id="cb2-23"><a href="machine-learning-for-social-scientists.html#cb2-23"></a><span class="st">  </span><span class="kw">theme_linedraw</span>()</span>
<span id="cb2-24"><a href="machine-learning-for-social-scientists.html#cb2-24"></a></span>
<span id="cb2-25"><a href="machine-learning-for-social-scientists.html#cb2-25"></a>overfit &lt;-</span>
<span id="cb2-26"><a href="machine-learning-for-social-scientists.html#cb2-26"></a><span class="st">  </span><span class="kw">ggplot</span>(age_inc, <span class="kw">aes</span>(age, income)) <span class="op">+</span></span>
<span id="cb2-27"><a href="machine-learning-for-social-scientists.html#cb2-27"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb2-28"><a href="machine-learning-for-social-scientists.html#cb2-28"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="dt">span =</span> <span class="fl">0.015</span>) <span class="op">+</span></span>
<span id="cb2-29"><a href="machine-learning-for-social-scientists.html#cb2-29"></a><span class="st">  </span>y_axis <span class="op">+</span></span>
<span id="cb2-30"><a href="machine-learning-for-social-scientists.html#cb2-30"></a><span class="st">  </span>x_axis <span class="op">+</span><span class="st">  </span></span>
<span id="cb2-31"><a href="machine-learning-for-social-scientists.html#cb2-31"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Overfit&quot;</span>) <span class="op">+</span></span>
<span id="cb2-32"><a href="machine-learning-for-social-scientists.html#cb2-32"></a><span class="st">  </span><span class="kw">theme_linedraw</span>()</span>
<span id="cb2-33"><a href="machine-learning-for-social-scientists.html#cb2-33"></a></span>
<span id="cb2-34"><a href="machine-learning-for-social-scientists.html#cb2-34"></a>goodfit &lt;-</span>
<span id="cb2-35"><a href="machine-learning-for-social-scientists.html#cb2-35"></a><span class="st">  </span><span class="kw">ggplot</span>(age_inc, <span class="kw">aes</span>(age, income)) <span class="op">+</span></span>
<span id="cb2-36"><a href="machine-learning-for-social-scientists.html#cb2-36"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb2-37"><a href="machine-learning-for-social-scientists.html#cb2-37"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="dt">span =</span> <span class="fl">0.9</span>) <span class="op">+</span></span>
<span id="cb2-38"><a href="machine-learning-for-social-scientists.html#cb2-38"></a><span class="st">  </span>y_axis <span class="op">+</span></span>
<span id="cb2-39"><a href="machine-learning-for-social-scientists.html#cb2-39"></a><span class="st">  </span>x_axis <span class="op">+</span><span class="st">  </span></span>
<span id="cb2-40"><a href="machine-learning-for-social-scientists.html#cb2-40"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Ideal fit&quot;</span>) <span class="op">+</span></span>
<span id="cb2-41"><a href="machine-learning-for-social-scientists.html#cb2-41"></a><span class="st">  </span><span class="kw">theme_linedraw</span>()</span>
<span id="cb2-42"><a href="machine-learning-for-social-scientists.html#cb2-42"></a></span>
<span id="cb2-43"><a href="machine-learning-for-social-scientists.html#cb2-43"></a>bad_fit <span class="op">+</span><span class="st"> </span>overfit <span class="op">+</span><span class="st"> </span>goodfit</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:overfitplots"></span>
<img src="figs/overfitplots-1.png" alt="Different ways of fitting your data" width="99%" />
<p class="caption">
Figure 1.1: Different ways of fitting your data
</p>
</div>
<p>The first panel of figure <a href="machine-learning-for-social-scientists.html#fig:overfitplots">1.1</a> shows a model which is not flexible, as it fits a straight line without capturing the subtle non-linearities of the data. The middle panel is <strong>too</strong> flexible as it captures much of the random noise of the non-linear relationship. Finally, the third panel shows the ideal fit, where the fitted line is flexible enough to capture the non-linear relationship in the data yet it it is mainly unaffected by the random noise in the data. Although social scientists are aware of these concepts, we really don’t think about them a lot. When we perform statistical modeling we don’t really think about whether we’re overfitting or underfitting: we’re mostly paying attention to whether the statistical relationships make sense.</p>
<p>For example, how would social scientists fit a model? They would take the entire data</p>
<p><img src="img/raw_data_wnote.svg" width="40%" style="display: block; margin: auto;" /></p>
<p>and fit the model on it. How do you know you’re overfitting? Well, one very easy and naive approach is to randomly divide your data into two chunks called training and testing:</p>
<p><img src="img/train_testing_df.svg" width="80%" style="display: block; margin: auto;" /></p>
<p>The training data usually consists of a random sample of around ~70% of the initial data and the testing data the remaining ~30% of the initial data. If a particular row is in the training data, it <strong>must not</strong> be on the testing data. In contrast, if a particular row is in the testing data, it <strong>shouldn’t</strong> be in the training data either. Why should splitting the data into two chunks help us fix the problem of overfitting? Because you can elaborate your model in the training set as much as you want, and when you’re confident enough, the testing data can serve as an <strong>unseen, pristine source of data</strong> on which you can evaluate your model. If fitting your model on the testing data shows that your model was too optimistic, you were probably overfitting the data.</p>
<p>Let’s go through the steps one by one. Fit your model in the training data (remember, that’s a random sample of about 70% of the initial data)</p>
<p><img src="img/training_df.svg" width="40%" style="display: block; margin: auto;" /></p>
<p>evaluate the model fit and make the same changes you would do on your complete data: create new variables, recode variables, etc. You can think of this chunk as the complete data to perform your analysis. It is the equivalent of the initial data where social scientists fit their models. Once you’re very comfortable with your model, the best recipe for checking whether your model was overfitting is to use this fitted model to predict on <strong>the other chunk of data</strong> (the testing data):</p>
<p><img src="img/testing_df.svg" width="40%" style="display: block; margin: auto;" /></p>
<p>If you tweaked your model in such a way that it learned the noise of your training data, it will perform poorly on the testing data, since your the model didn’t capture the overall trend in the training data but rather the noise.</p>
<p>It’s time to introduce how we perform these steps in R. For this, we’ll use the package <code>tidyflow</code>, a package created for this book. It aims to have a simple and intuitive workflow for machine learning which you’ll learn through this book. You can install the package with the code below:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="machine-learning-for-social-scientists.html#cb3-1"></a><span class="kw">install.packages</span>(<span class="st">&quot;devtools&quot;</span>)</span>
<span id="cb3-2"><a href="machine-learning-for-social-scientists.html#cb3-2"></a>devtools<span class="op">::</span><span class="kw">install_github</span>(<span class="st">&quot;cimentadaj/tidyflow&quot;</span>)</span></code></pre></div>
<p>In R we can build a machine learning ‘workflow’ with the function <code>tidyflow</code>. To this workflow, we can plug in steps that you can execute. In our example, to plug in the step of partitioning the data into training and testing, you can use <code>plug_split</code> with the function <code>initial_split</code>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="machine-learning-for-social-scientists.html#cb4-1"></a><span class="kw">library</span>(tidyflow)</span>
<span id="cb4-2"><a href="machine-learning-for-social-scientists.html#cb4-2"></a></span>
<span id="cb4-3"><a href="machine-learning-for-social-scientists.html#cb4-3"></a>ml_flow &lt;-</span>
<span id="cb4-4"><a href="machine-learning-for-social-scientists.html#cb4-4"></a><span class="st">  </span>age_inc <span class="op">%&gt;%</span></span>
<span id="cb4-5"><a href="machine-learning-for-social-scientists.html#cb4-5"></a><span class="st">  </span><span class="kw">tidyflow</span>(<span class="dt">seed =</span> <span class="dv">2313</span>) <span class="op">%&gt;%</span></span>
<span id="cb4-6"><a href="machine-learning-for-social-scientists.html#cb4-6"></a><span class="st">  </span><span class="kw">plug_split</span>(initial_split)</span>
<span id="cb4-7"><a href="machine-learning-for-social-scientists.html#cb4-7"></a></span>
<span id="cb4-8"><a href="machine-learning-for-social-scientists.html#cb4-8"></a>ml_flow</span></code></pre></div>
<pre><code>## ══ Tidyflow ════════════════════════════════════════════════════════════════════
## Data: 500 rows x 2 columns
## Split: initial_split w/ default args
## Recipe/Formula: None
## Resample: None
## Grid: None
## Model: None</code></pre>
<p><code>tidyflow</code> already knows that <code>age_inc</code> is the main data source and that we need to apply the training/testing split with <code>initial_split</code>. You can think of this as plan that will be executed once you tell it to.</p>
<p>Let’s get back to our example and suppose that you fit your model several times on the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data, tweaking it to improve performance (when I say tweaking I mean applying transformations, including new variables, recoding old variables, including polynomials, etc..). When you think you’re ready, you use this model to predict on the <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data and find out that the model was indeed overfitting the data because you cannot predict the <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data as well as the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data. You then go back to the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data, tweak some more, run some models again and when you think the model is ready again, you predict on your <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data again and find that it improved. Then you repeat the process again, <span class="math inline">\(3\)</span>, <span class="math inline">\(4\)</span>, <span class="math inline">\(5\)</span> <span class="math inline">\(N\)</span> times. If you do that, you will, in very subtle ways, start to <strong>overfit</strong> your model on the <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data! In other words, you’re fitting a model <span class="math inline">\(N\)</span> times on your <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data, evaluating its fit on the <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data and then <strong>tweaking</strong> again to improve the prediction on the <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data. The <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data should serve as the final dataset to compare your model: you should not tweak the model again after seeing how your model fits the <strong>unseen</strong> <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data.</p>
<p>That doesn’t sound right. It seems we have too few “degrees of freedom” to test the accuracy of our model. We can tweak the model in the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data as much as we want but we only have <strong>one</strong> attempt at testing our model against the <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data. How can we evaluate, then, whether we’re overfitting with the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data alone, then? <strong>Enter cross-validation</strong></p>
</div>
<div id="cross-validation" class="section level2">
<h2><span class="header-section-number">1.3</span> Cross-validation</h2>
<p>The idea behind cross-validation is to allow the user to check whether they’re overfitting the data without predicting on the <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data. How does it work? First, we <strong>only</strong> work with our <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data</p>
<p><img src="img/training_df.svg" width="40%" style="display: block; margin: auto;" /></p>
<p>and replicate the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data 10 times</p>
<p><img src="img/train_cv2_smaller.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>The 10 rectangular red rows below the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data, contain an exact replica of the initial <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data. That is, if the initial <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data has 500 rows and 10 columns, then each of these red rectangled rows also has 500 rows and 10 columns. The idea behind this approach is that for each rectangled row, you can use 70% of the data to fit your model and then predict on the remaining 30%. For example, for the first rectangled row, you would fit your initial model model with some tweak (let’s say, adding a squared term to the age variable to check if that improves fit) on the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data and then predict on the <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data to evaluate the fit:</p>
<p><img src="img/train_cv3_smaller.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>Since we fit a model to the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b>data of each rectangled row and then predict on the <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data of each rectangled row, we can record how well our model is doing for each of our replicate data sets. For example, for the first row we record the <span class="math inline">\(RMSE\)</span> of the prediction on the testing data. For the second rectangled row, fit the exact same model (that is, including the age squared term) on 70% of the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data, predict on the <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data and record the <span class="math inline">\(RMSE\)</span>. And then repeat the same iteration for every rectangled row:</p>
<p><img src="img/train_cv6_smaller.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>After you’ve fitted the model and evaluated the model 10 times, you have 10 values of the <span class="math inline">\(RMSE\)</span>. With these 10 values you can calculate the average <span class="math inline">\(RMSE\)</span> and standard error of your model’s performance.</p>
<p>Note that with this approach, the <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data changes in each rectangled row, making sure that each ~30% chunk of the data passes through the <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> dataset at some point during the predictions. This is done to ensure the predictions are as balanced as possible.</p>
<p>This approach offers a way to iterate as many times as you want on tweaking your model and predicting on the cross-validated <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data without actually predicting on the initial <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> dataset. This is the least bad approach that is <strong>currently</strong> accepted in the literature.</p>
<p>Why is it the least bad approach? Because if we tweak the model on these 10 replicas one time, then a second time, then a third time, etc…, we’ll also start overfitting on each of these 10 slots! The superiority of this approach over tweaking on the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data is that since we have 10 replicas, we can take the average of model fit metrics and also obtain standard errors. This allows to have a somewhat balanced account of how our model fit is doing and the uncertainty around it.</p>
<p>That said, since we will always overfit in someway using a cross-validation approach, the final error of your model fit on the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data will always be over optimistic (lower error than what you will actually have, if you predicted on the <strong>pristine</strong> <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data.</p>
<p>Based on our previous <code>tidyflow</code>, we can plug in a cross-validation step with <code>plug_resample</code>. There are many different cross-validation techniques but let’s focus on the one from our example (replicating the data 10 times). For that, we use the function <code>vfold_cv</code>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="machine-learning-for-social-scientists.html#cb6-1"></a>ml_flow &lt;-</span>
<span id="cb6-2"><a href="machine-learning-for-social-scientists.html#cb6-2"></a><span class="st">  </span>ml_flow <span class="op">%&gt;%</span></span>
<span id="cb6-3"><a href="machine-learning-for-social-scientists.html#cb6-3"></a><span class="st">  </span><span class="kw">plug_resample</span>(vfold_cv)</span>
<span id="cb6-4"><a href="machine-learning-for-social-scientists.html#cb6-4"></a></span>
<span id="cb6-5"><a href="machine-learning-for-social-scientists.html#cb6-5"></a>ml_flow</span></code></pre></div>
<pre><code>## ══ Tidyflow ════════════════════════════════════════════════════════════════════
## Data: 500 rows x 2 columns
## Split: initial_split w/ default args
## Recipe/Formula: None
## Resample: vfold_cv w/ default args
## Grid: None
## Model: None</code></pre>
</div>
<div id="bias-variance-tradeoff" class="section level2">
<h2><span class="header-section-number">1.4</span> Bias-Variance Tradeoff</h2>
<p>Before we elaborate a complete coded example, it’s important to talk about the concept of bias-variance tradeoff used in machine learning problems. As was shown in figure <a href="machine-learning-for-social-scientists.html#fig:overfitplots">1.1</a>, we want the ideal fit without overfitting or underfitting the data. In some instances, fitting the data that well is very difficult because we don’t have variables that reflect the data generating process or because the relationship is too complex. In that case, for machine learning problems, you might want to either underfit or overfit slightly, depending on your problem.</p>
<p>Overfitting your data has some value, which is that we learn the data very well. This is often called a model with a lot of flexibility. A model that can learn all the small intricacies of the data is often called a <strong>flexible</strong> model. There is <strong>very</strong> little bias in a model like this one, since we learn the data very very well. However, at the expense of bias, overfitting has <strong>a lot</strong> of variance. If we predict on a new dataset using the overfitted model, we’ll find a completely different result from the initial model. If we repeat the same on another dataset, we’ll find another different result. That is why models which can be very flexible are considered to have very little bias and a lot of variance:</p>
<p><img src="figs/unnamed-chunk-16-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>The model above fits the criteria:</p>
<p><img src="figs/unnamed-chunk-17-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>On the other hand, models which are <strong>not</strong> flexible, have more bias and less variance. One familiar example of this is the linear model. By fitting a straight line through the data, the variance is very small: if we run the same exact model on a new data, the fitted line is robust to slight changes in the data (outliers, small changes in the tails of the distribution, etc..). However, the fitted line doesn’t really capture the subtle trends in the data (assuming the relationship is non-linear, which is in most cases). That is why non-flexible models are often called to have high bias and low variance:</p>
<p><img src="figs/unnamed-chunk-18-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>or in other words:</p>
<p><img src="figs/unnamed-chunk-19-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>In reality, what we usually want is something located in the middle of these two extremes: we want a model that is neither too flexible that overfits the data nor too inflexible that misses the signal. There is really no magical recipe to achieving the perfect model and our best approach is to understand our model’s performance using techniques such as cross-validation to assess how much our model is overfitting/underfitting the data. Even experienced machine learning practitioners can build models that overfit the data (one notable example is the results from the Fragile Families Challenge, see <strong>HEREEE</strong> put the plot of the paper where overfitting is huge).</p>
</div>
<div id="an-example" class="section level2">
<h2><span class="header-section-number">1.5</span> An example</h2>
<p>Let’s combine all the new steps into a complete pipeline of machine learning in R. We can do that by finishing the <code>tidyflow</code> we’ve been developing so far. Let’s use the data <code>age_inc</code> which has the age of a person and their income. We want to predict their income based on their age. The rectangular data looks like this:</p>
<div id="htmlwidget-4c09a4ce4c01f2b64aad" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-4c09a4ce4c01f2b64aad">{"x":{"filter":"none","data":[["1","2","3","4","5","6"],[44,29,66,66,57,39],[2203,2162,2441,2938,2574,2091]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>age<\/th>\n      <th>income<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[1,2]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>The relationship between these two variables is non-linear, showing a variant of the Mincer equation <span class="citation">(Mincer <a href="#ref-mincer1958" role="doc-biblioref">1958</a>)</span> where income is a non-linear function of age:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="machine-learning-for-social-scientists.html#cb8-1"></a><span class="co"># age_inc was defined above, and it is reused here</span></span>
<span id="cb8-2"><a href="machine-learning-for-social-scientists.html#cb8-2"></a>age_inc <span class="op">%&gt;%</span></span>
<span id="cb8-3"><a href="machine-learning-for-social-scientists.html#cb8-3"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(age, income)) <span class="op">+</span></span>
<span id="cb8-4"><a href="machine-learning-for-social-scientists.html#cb8-4"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb8-5"><a href="machine-learning-for-social-scientists.html#cb8-5"></a><span class="st">  </span><span class="kw">theme_linedraw</span>()</span></code></pre></div>
<p><img src="figs/unnamed-chunk-21-1.png" width="80%" height="90%" style="display: block; margin: auto;" /></p>
<p>Since <code>ml_flow</code> is a series of steps, it allows you to remove any of them. Let’s remove the cross-validation step with <code>drop_resample</code>:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="machine-learning-for-social-scientists.html#cb9-1"></a>ml_flow &lt;-</span>
<span id="cb9-2"><a href="machine-learning-for-social-scientists.html#cb9-2"></a><span class="st">  </span>ml_flow <span class="op">%&gt;%</span></span>
<span id="cb9-3"><a href="machine-learning-for-social-scientists.html#cb9-3"></a><span class="st">  </span><span class="kw">drop_resample</span>()</span>
<span id="cb9-4"><a href="machine-learning-for-social-scientists.html#cb9-4"></a></span>
<span id="cb9-5"><a href="machine-learning-for-social-scientists.html#cb9-5"></a>ml_flow</span></code></pre></div>
<pre><code>## ══ Tidyflow ════════════════════════════════════════════════════════════════════
## Data: 500 rows x 2 columns
## Split: initial_split w/ default args
## Recipe/Formula: None
## Resample: None
## Grid: None
## Model: None</code></pre>
<p>Let’s begin running some models. The first model we’d like run is a simple regression <code>income ~ age</code> on the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data and plot the fitted values.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="machine-learning-for-social-scientists.html#cb11-1"></a><span class="co"># Run the model</span></span>
<span id="cb11-2"><a href="machine-learning-for-social-scientists.html#cb11-2"></a>m1 &lt;-</span>
<span id="cb11-3"><a href="machine-learning-for-social-scientists.html#cb11-3"></a><span class="st">  </span>ml_flow <span class="op">%&gt;%</span></span>
<span id="cb11-4"><a href="machine-learning-for-social-scientists.html#cb11-4"></a><span class="st">  </span><span class="kw">plug_recipe</span>(<span class="op">~</span><span class="st"> </span><span class="kw">recipe</span>(income <span class="op">~</span><span class="st"> </span>age, <span class="dt">data =</span> .)) <span class="op">%&gt;%</span><span class="st">  </span><span class="co"># Add the formula</span></span>
<span id="cb11-5"><a href="machine-learning-for-social-scientists.html#cb11-5"></a><span class="st">  </span><span class="kw">plug_model</span>(<span class="kw">set_engine</span>(<span class="kw">linear_reg</span>(), <span class="st">&quot;lm&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># Define the linear regression</span></span>
<span id="cb11-6"><a href="machine-learning-for-social-scientists.html#cb11-6"></a><span class="st">  </span><span class="kw">fit</span>() <span class="co"># Fit model</span></span>
<span id="cb11-7"><a href="machine-learning-for-social-scientists.html#cb11-7"></a></span>
<span id="cb11-8"><a href="machine-learning-for-social-scientists.html#cb11-8"></a><span class="co"># Predict on the training data</span></span>
<span id="cb11-9"><a href="machine-learning-for-social-scientists.html#cb11-9"></a>m1_res &lt;-</span>
<span id="cb11-10"><a href="machine-learning-for-social-scientists.html#cb11-10"></a><span class="st">  </span>m1 <span class="op">%&gt;%</span></span>
<span id="cb11-11"><a href="machine-learning-for-social-scientists.html#cb11-11"></a><span class="st">  </span><span class="kw">predict_training</span>()</span>
<span id="cb11-12"><a href="machine-learning-for-social-scientists.html#cb11-12"></a></span>
<span id="cb11-13"><a href="machine-learning-for-social-scientists.html#cb11-13"></a>m1_res</span></code></pre></div>
<pre><code>## # A tibble: 375 x 3
##      age income .pred
##    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
##  1 56.9   2574. 2650.
##  2 38.9   2091. 2298.
##  3 62.8   2328. 2767.
##  4 84.9   3548. 3200.
##  5 65.3   2440. 2815.
##  6 93.8   3866. 3374.
##  7  7.92  1511. 1692.
##  8 78.3   3134. 3070.
##  9 55.7   2777. 2628.
## 10 42.2   2918. 2362.
## # … with 365 more rows</code></pre>
<p>The result of <code>predict_training</code> is the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data from <code>age_inc</code> with one new column: the predicted values of the model. Let’s visualize the predictions:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="machine-learning-for-social-scientists.html#cb13-1"></a><span class="co"># Visualize the result</span></span>
<span id="cb13-2"><a href="machine-learning-for-social-scientists.html#cb13-2"></a>m1_res <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb13-3"><a href="machine-learning-for-social-scientists.html#cb13-3"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(age, income)) <span class="op">+</span></span>
<span id="cb13-4"><a href="machine-learning-for-social-scientists.html#cb13-4"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> .pred), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">size =</span> <span class="dv">2</span>) <span class="op">+</span></span>
<span id="cb13-5"><a href="machine-learning-for-social-scientists.html#cb13-5"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb13-6"><a href="machine-learning-for-social-scientists.html#cb13-6"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">name =</span> <span class="st">&quot;Age&quot;</span>) <span class="op">+</span></span>
<span id="cb13-7"><a href="machine-learning-for-social-scientists.html#cb13-7"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">name =</span> <span class="st">&quot;Income&quot;</span>,</span>
<span id="cb13-8"><a href="machine-learning-for-social-scientists.html#cb13-8"></a>                     <span class="dt">label =</span> <span class="kw">dollar_format</span>(<span class="dt">suffix =</span> <span class="st">&quot;€&quot;</span>, <span class="dt">prefix =</span> <span class="st">&quot;&quot;</span>)) <span class="op">+</span></span>
<span id="cb13-9"><a href="machine-learning-for-social-scientists.html#cb13-9"></a><span class="st">  </span><span class="kw">theme_linedraw</span>()</span></code></pre></div>
<p><img src="figs/unnamed-chunk-24-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>It seems we’re underfitting the relationship. To measure the <strong>fit</strong> of the model, we’ll use the Root Mean Square Error (RMSE). Remember it?</p>
<p><span class="math display">\[ RMSE = \sqrt{\sum_{i = 1}^n{\frac{(\hat{y} - y)^2}{N}}} \]</span></p>
<p>Without going into too many details, it is the average difference between each dot from the plot from the value same value in the fitted line. The current <span class="math inline">\(RMSE\)</span> of our model is 379.59. This means that on average our predictions are off by around 379.59 euros. The fitted line is underfitting the relationship because it cannot capture the non-linear trend in the data. How do we increase the fit? We could add non-linear terms to the model, for example <span class="math inline">\(age^2\)</span>, <span class="math inline">\(age^3\)</span>, …, <span class="math inline">\(age^{10}\)</span>.</p>
<p>However, remember, by fitting very high non-linear terms to the data, we might get lower error from the model on the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data but that’s because the model is <strong>learning</strong> the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data so much that it starts to capture noise rather than the signal. This means that when we predict on the <strong>unseen</strong> <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data, our model would not know how to identify the signal in the data and have a higher <span class="math inline">\(RMSE\)</span> error. How can we be sure we’re picking the best model specification?</p>
<p><strong>This is where cross-validation comes in!</strong></p>
<p>We can use the function <code>vfold_cv</code> to separate the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data into 10 cross-validation sets, where each one has a <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> and <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> data.</p>
<pre><code>## #  10-fold cross-validation 
## # A tibble: 10 x 2
##    splits           id    
##    &lt;list&gt;           &lt;chr&gt; 
##  1 &lt;split [337/38]&gt; Fold01
##  2 &lt;split [337/38]&gt; Fold02
##  3 &lt;split [337/38]&gt; Fold03
##  4 &lt;split [337/38]&gt; Fold04
##  5 &lt;split [337/38]&gt; Fold05
##  6 &lt;split [338/37]&gt; Fold06
##  7 &lt;split [338/37]&gt; Fold07
##  8 &lt;split [338/37]&gt; Fold08
##  9 &lt;split [338/37]&gt; Fold09
## 10 &lt;split [338/37]&gt; Fold10</code></pre>
<p>Each of those <code>split</code> objects (there are 10) contains a <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> and <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> set. This is the equivalent of the image we saw before:</p>
<p><img src="img/train_cv4_smaller.svg" width="75%" style="display: block; margin: auto;" /></p>
<p><br></p>
<p>The next thing we have to do is train the same model on the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data of each of these cross-validated sets, use these trained models to predict on the 10 <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> sets and record the error rate using our <span class="math inline">\(RMSE\)</span> metric. But don’t worry, you don’t have to do that all of that manually, <code>tidyflow</code> can leverage many packages to do that for you:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="machine-learning-for-social-scientists.html#cb15-1"></a><span class="co"># Define the formula of your model and specify that the polynomial</span></span>
<span id="cb15-2"><a href="machine-learning-for-social-scientists.html#cb15-2"></a><span class="co"># value will be &#39;tuned&#39;. That is, we will try several values</span></span>
<span id="cb15-3"><a href="machine-learning-for-social-scientists.html#cb15-3"></a><span class="co"># instead of only one.</span></span>
<span id="cb15-4"><a href="machine-learning-for-social-scientists.html#cb15-4"></a>rcp &lt;-</span>
<span id="cb15-5"><a href="machine-learning-for-social-scientists.html#cb15-5"></a><span class="st">  </span><span class="er">~</span><span class="st"> </span><span class="kw">recipe</span>(income <span class="op">~</span><span class="st"> </span>age, <span class="dt">data =</span> .) <span class="op">%&gt;%</span></span>
<span id="cb15-6"><a href="machine-learning-for-social-scientists.html#cb15-6"></a><span class="st">    </span><span class="kw">step_poly</span>(age, <span class="dt">degree =</span> <span class="kw">tune</span>())</span>
<span id="cb15-7"><a href="machine-learning-for-social-scientists.html#cb15-7"></a></span>
<span id="cb15-8"><a href="machine-learning-for-social-scientists.html#cb15-8"></a>m2 &lt;-</span>
<span id="cb15-9"><a href="machine-learning-for-social-scientists.html#cb15-9"></a><span class="st">  </span>m1 <span class="op">%&gt;%</span></span>
<span id="cb15-10"><a href="machine-learning-for-social-scientists.html#cb15-10"></a><span class="st">  </span><span class="co"># Add the cross-validation step</span></span>
<span id="cb15-11"><a href="machine-learning-for-social-scientists.html#cb15-11"></a><span class="st">  </span><span class="kw">plug_resample</span>(vfold_cv) <span class="op">%&gt;%</span></span>
<span id="cb15-12"><a href="machine-learning-for-social-scientists.html#cb15-12"></a><span class="st">  </span><span class="co"># Replace the initial recipe with the one with several polynomials  </span></span>
<span id="cb15-13"><a href="machine-learning-for-social-scientists.html#cb15-13"></a><span class="st">  </span><span class="kw">replace_recipe</span>(rcp) <span class="op">%&gt;%</span></span>
<span id="cb15-14"><a href="machine-learning-for-social-scientists.html#cb15-14"></a><span class="st">  </span><span class="co"># Here we define the values we will try, from 2 to 10</span></span>
<span id="cb15-15"><a href="machine-learning-for-social-scientists.html#cb15-15"></a><span class="st">  </span><span class="kw">plug_grid</span>(expand.grid, <span class="dt">degree =</span> <span class="dv">2</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span></span>
<span id="cb15-16"><a href="machine-learning-for-social-scientists.html#cb15-16"></a><span class="st">  </span><span class="co"># Fit the model</span></span>
<span id="cb15-17"><a href="machine-learning-for-social-scientists.html#cb15-17"></a><span class="st">  </span><span class="kw">fit</span>()</span>
<span id="cb15-18"><a href="machine-learning-for-social-scientists.html#cb15-18"></a>  </span>
<span id="cb15-19"><a href="machine-learning-for-social-scientists.html#cb15-19"></a><span class="co"># Visualize the result</span></span>
<span id="cb15-20"><a href="machine-learning-for-social-scientists.html#cb15-20"></a>m2 <span class="op">%&gt;%</span></span>
<span id="cb15-21"><a href="machine-learning-for-social-scientists.html#cb15-21"></a><span class="st">  </span><span class="kw">pull_tflow_fit_tuning</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># Extract all models with different &#39;degree&#39; values</span></span>
<span id="cb15-22"><a href="machine-learning-for-social-scientists.html#cb15-22"></a><span class="st">  </span><span class="kw">autoplot</span>() <span class="op">+</span></span>
<span id="cb15-23"><a href="machine-learning-for-social-scientists.html#cb15-23"></a><span class="st">  </span><span class="kw">theme_linedraw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:metricslm"></span>
<img src="figs/metricslm-1.png" alt="Average evaluation metrics of predicting on the testing data through the 10 cross-validation sets" width="99%" />
<p class="caption">
Figure 1.2: Average evaluation metrics of predicting on the testing data through the 10 cross-validation sets
</p>
</div>
<p>Figure <a href="machine-learning-for-social-scientists.html#fig:metricslm">1.2</a> shows the error rate for the <span class="math inline">\(RMSE\)</span> and the <span class="math inline">\(R^2\)</span>. For the <span class="math inline">\(RMSE\)</span> (left panel), the resulting error terms show that any polynomial above 2 has very similar error rates. However, there is a point in which adding <span class="math inline">\(age^9\)</span> and <span class="math inline">\(age^{10}\)</span> increases the error rate. This decrease in fit as complexity increases can also been see with the <span class="math inline">\(R^2\)</span> (right panel), as it decreases with higher polynomials. This is a good example where a lot of flexibility (fitting the non-linear trend <strong>very</strong> well), increases accuracy on the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> set but shows a lot variability on the <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> set. The <span class="math inline">\(RMSE\)</span> that we see in figure <a href="machine-learning-for-social-scientists.html#fig:metricslm">1.2</a> is the average <span class="math inline">\(RMSE\)</span> from predicting on the <b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> set using the model fitted on the <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data in the 10 cross validated sets.</p>
<p>Given that most of the polynomial terms have similar error terms, we usually would go for the simplest model, that is, the model with <span class="math inline">\(age^3\)</span>. We can run the model on the entire <b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> data with 3 non-linear terms and check the fit:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="machine-learning-for-social-scientists.html#cb16-1"></a><span class="co"># Fit the final model with degrees = 3</span></span>
<span id="cb16-2"><a href="machine-learning-for-social-scientists.html#cb16-2"></a>res_m2 &lt;-<span class="st"> </span><span class="kw">complete_tflow</span>(m2, <span class="dt">best_params =</span> <span class="kw">data.frame</span>(<span class="dt">degree =</span> <span class="dv">3</span>))</span>
<span id="cb16-3"><a href="machine-learning-for-social-scientists.html#cb16-3"></a></span>
<span id="cb16-4"><a href="machine-learning-for-social-scientists.html#cb16-4"></a>res_m2 <span class="op">%&gt;%</span></span>
<span id="cb16-5"><a href="machine-learning-for-social-scientists.html#cb16-5"></a><span class="st">  </span><span class="kw">predict_training</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb16-6"><a href="machine-learning-for-social-scientists.html#cb16-6"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(age, income)) <span class="op">+</span></span>
<span id="cb16-7"><a href="machine-learning-for-social-scientists.html#cb16-7"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> .pred), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">size =</span> <span class="dv">2</span>) <span class="op">+</span></span>
<span id="cb16-8"><a href="machine-learning-for-social-scientists.html#cb16-8"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb16-9"><a href="machine-learning-for-social-scientists.html#cb16-9"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">name =</span> <span class="st">&quot;Age&quot;</span>) <span class="op">+</span></span>
<span id="cb16-10"><a href="machine-learning-for-social-scientists.html#cb16-10"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">name =</span> <span class="st">&quot;Income&quot;</span>,</span>
<span id="cb16-11"><a href="machine-learning-for-social-scientists.html#cb16-11"></a>                     <span class="dt">label =</span> <span class="kw">dollar_format</span>(<span class="dt">suffix =</span> <span class="st">&quot;€&quot;</span>, <span class="dt">prefix =</span> <span class="st">&quot;&quot;</span>)) <span class="op">+</span></span>
<span id="cb16-12"><a href="machine-learning-for-social-scientists.html#cb16-12"></a><span class="st">  </span><span class="kw">theme_linedraw</span>()</span></code></pre></div>
<p><img src="figs/unnamed-chunk-29-1.png" width="80%" height="80%" style="display: block; margin: auto;" /></p>
<p>The <span class="math inline">\(RMSE\)</span> on the <strong><b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b></strong> data for the three polynomial model is 279.87. We need to compare that to our <strong><b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b></strong> <span class="math inline">\(RMSE\)</span>.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="machine-learning-for-social-scientists.html#cb17-1"></a>res_m2 <span class="op">%&gt;%</span></span>
<span id="cb17-2"><a href="machine-learning-for-social-scientists.html#cb17-2"></a><span class="st">  </span><span class="kw">predict_testing</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb17-3"><a href="machine-learning-for-social-scientists.html#cb17-3"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(age, income)) <span class="op">+</span></span>
<span id="cb17-4"><a href="machine-learning-for-social-scientists.html#cb17-4"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> .pred), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">size =</span> <span class="dv">2</span>) <span class="op">+</span></span>
<span id="cb17-5"><a href="machine-learning-for-social-scientists.html#cb17-5"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb17-6"><a href="machine-learning-for-social-scientists.html#cb17-6"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">name =</span> <span class="st">&quot;Age&quot;</span>) <span class="op">+</span></span>
<span id="cb17-7"><a href="machine-learning-for-social-scientists.html#cb17-7"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">name =</span> <span class="st">&quot;Income&quot;</span>,</span>
<span id="cb17-8"><a href="machine-learning-for-social-scientists.html#cb17-8"></a>                     <span class="dt">label =</span> <span class="kw">dollar_format</span>(<span class="dt">suffix =</span> <span class="st">&quot;€&quot;</span>, <span class="dt">prefix =</span> <span class="st">&quot;&quot;</span>)) <span class="op">+</span></span>
<span id="cb17-9"><a href="machine-learning-for-social-scientists.html#cb17-9"></a><span class="st">  </span><span class="kw">theme_linedraw</span>()</span></code></pre></div>
<p><img src="figs/unnamed-chunk-31-1.png" width="80%" height="70%" style="display: block; margin: auto;" /></p>
<ul>
<li><b><span style="color: red; -webkit-text-stroke: 0.3px black;">training</span></b> <span class="math inline">\(RMSE\)</span> is 279.87</li>
<li><b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> <span class="math inline">\(RMSE\)</span> is 311.03</li>
</ul>
<p><b><span style="color: #D4FF2A; -webkit-text-stroke: 0.3px black;">testing</span></b> <span class="math inline">\(RMSE\)</span> will almost always be higher, since we always overfit the data in some way through cross-validation.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-breiman2001">
<p>Breiman, Leo, and others. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” <em>Statistical Science</em> 16 (3): 199–231.</p>
</div>
<div id="ref-mincer1958">
<p>Mincer, Jacob. 1958. “Investment in Human Capital and Personal Income Distribution.” <em>Journal of Political Economy</em> 66 (4): 281–302. <a href="https://doi.org/10.1086/258055">https://doi.org/10.1086/258055</a>.</p>
</div>
<div id="ref-watts2014">
<p>Watts, Duncan J. 2014. “Common Sense and Sociological Explanations.” <em>American Journal of Sociology</em> 120 (2): 313–51.</p>
</div>
<div id="ref-yarkoni2017">
<p>Yarkoni, Tal, and Jacob Westfall. 2017. “Choosing Prediction over Explanation in Psychology: Lessons from Machine Learning.” <em>Perspectives on Psychological Science</em> 12 (6): 1100–1122.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regularization.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
