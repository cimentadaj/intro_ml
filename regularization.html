<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Regularization | Machine Learning for Social Scientists</title>
  <meta name="description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  <meta name="generator" content="bookdown 0.17.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Regularization | Machine Learning for Social Scientists" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://cimentadaj.github.io/ml_socsci/" />
  
  <meta property="og:description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  <meta name="github-repo" content="cimentadaj/ml_socsci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Regularization | Machine Learning for Social Scientists" />
  
  <meta name="twitter:description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  

<meta name="author" content="Jorge Cimentada" />


<meta name="date" content="2020-01-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="syllabus.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning for Social Scientists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>1</b> Regularization</a><ul>
<li class="chapter" data-level="1.1" data-path="regularization.html"><a href="regularization.html#ridge-regularization"><i class="fa fa-check"></i><b>1.1</b> Ridge regularization</a></li>
<li class="chapter" data-level="1.2" data-path="regularization.html"><a href="regularization.html#lasso-regularization"><i class="fa fa-check"></i><b>1.2</b> Lasso regularization</a></li>
<li class="chapter" data-level="1.3" data-path="regularization.html"><a href="regularization.html#elastic-net-regularization"><i class="fa fa-check"></i><b>1.3</b> Elastic Net regularization</a></li>
<li class="chapter" data-level="1.4" data-path="regularization.html"><a href="regularization.html#exercises"><i class="fa fa-check"></i><b>1.4</b> Exercises</a><ul>
<li class="chapter" data-level="1.4.1" data-path="regularization.html"><a href="regularization.html#split-the-data-into-testtraining-data"><i class="fa fa-check"></i><b>1.4.1</b> Split the data into test/training data</a></li>
<li class="chapter" data-level="1.4.2" data-path="regularization.html"><a href="regularization.html#run-a-ridge-regression-with-non-cognitive-as-the-dependent-variable"><i class="fa fa-check"></i><b>1.4.2</b> Run a ridge regression with non-cognitive as the dependent variable</a></li>
<li class="chapter" data-level="1.4.3" data-path="regularization.html"><a href="regularization.html#which-are-the-most-important-variables"><i class="fa fa-check"></i><b>1.4.3</b> Which are the most important variables?</a></li>
<li class="chapter" data-level="1.4.4" data-path="regularization.html"><a href="regularization.html#run-a-lasso-regression-with-the-same-specification-as-above"><i class="fa fa-check"></i><b>1.4.4</b> Run a lasso regression with the same specification as above</a></li>
<li class="chapter" data-level="1.4.5" data-path="regularization.html"><a href="regularization.html#run-an-elastic-net-regression-on-non-cognitive-skills"><i class="fa fa-check"></i><b>1.4.5</b> Run an elastic net regression on non cognitive skills</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="regularization.html"><a href="regularization.html#bibliography"><i class="fa fa-check"></i><b>1.5</b> Bibliography</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="syllabus.html"><a href="syllabus.html"><i class="fa fa-check"></i><b>2</b> Syllabus</a><ul>
<li class="chapter" data-level="2.1" data-path="syllabus.html"><a href="syllabus.html#course-description"><i class="fa fa-check"></i><b>2.1</b> Course description</a></li>
<li class="chapter" data-level="2.2" data-path="syllabus.html"><a href="syllabus.html#schedule"><i class="fa fa-check"></i><b>2.2</b> Schedule</a></li>
<li class="chapter" data-level="2.3" data-path="syllabus.html"><a href="syllabus.html#software"><i class="fa fa-check"></i><b>2.3</b> Software:</a></li>
<li class="chapter" data-level="2.4" data-path="syllabus.html"><a href="syllabus.html#prerequisites"><i class="fa fa-check"></i><b>2.4</b> Prerequisites:</a></li>
<li class="chapter" data-level="2.5" data-path="syllabus.html"><a href="syllabus.html#about-the-author"><i class="fa fa-check"></i><b>2.5</b> About the author</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Social Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regularization" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Regularization</h1>
<p>Regularization is a common topic in machine learning and bayesian statistics. In this document, we will describe the three most common regularized linear models in the machine learning literature and introduce them in the context of the PISA data set. At the end of the document you’ll find exercises that will put your knowledge to the test. Most of this material is built upon Boehmke &amp; Greenwell (2019) and Friedman et al. (2001).</p>
<div id="ridge-regularization" class="section level2">
<h2><span class="header-section-number">1.1</span> Ridge regularization</h2>
<p>Do no let others fool you into thinking that ridge regression is a fancy artificial intelligence algorithm. Are you familiar with linear regression? If you are, then ridge regression is just a very <strong>simple</strong> adaptation of linear regression.</p>
<p>The whole aim of linear regression, or Ordinary Least Squares (OLS), is to minimize the sum of the squared residuals. In other words, fit <code>N</code> number of regression lines to the data and keep only the one that has the lowest sum of squared residuals. In simple formula jargon, OLS tries to <strong>minimize</strong> this:</p>
<span class="math display">\[\begin{equation}
RSS = \sum_{k = 1}^n(actual_i - predicted_i)^2
\end{equation}\]</span>
<p>For each fitted regression line, you compare the predicted value (<span class="math inline">\(predicted_i\)</span>) versus the actual value (<span class="math inline">\(actual_i\)</span>), square it, and add it all up. Each fitted regression line then has an associated Residual Sum of Squares (RSS) and the linear model chooses the line with the lowest RSS.</p>
<blockquote>
<p>Note: Social scientists are familiar with the RSS and call it just by it’s name. However, be aware that in machine learning jargon, the RSS belongs to a general family called <strong>loss functions</strong>. Loss functions are metrics that evaluate the <strong>fit</strong> of your model and there are many around (such as AIC, BIC or R2).</p>
</blockquote>
<p>Ridge regression takes the previous RSS loss function and adds one term:</p>
<span class="math display">\[\begin{equation}
RSS + \lambda \sum_{k = 1}^n \beta^2_j
\end{equation}\]</span>
<p>The new term is called a <em>shrinkage penalty</em> because it forces each coefficient <span class="math inline">\(\beta_j\)</span> closer to zero by squaring it. The shrinkage part is clearer once you think of this term as forcing each coefficient to be as small as possible but also considering having the smallest Residual Sum of Squares (RSS). In other words, we want the smallest coefficients that don’t affect the fit of the line (RSS).</p>
<p>An intuitive example is to think of RSS and <span class="math inline">\(\sum_{k = 1}^n \beta^2_j\)</span> as to separate things. RSS estimates how the model fits the data and <span class="math inline">\(\sum_{k = 1}^n \beta^2_j\)</span> limits how much you overfit the data. Finally, the little <span class="math inline">\(\lambda\)</span> between these two terms can be interpreted as a “weight”. The higher the lambda, the higher the weight that will be given to the shrinkage term of the equation. If <span class="math inline">\(\lambda\)</span> is 0, then multiplying 0 by <span class="math inline">\(\sum_{k = 1}^n \beta^2_j\)</span> will always return zero, forcing our previous equation to simply be reduced to the single term <span class="math inline">\(RSS\)</span>.</p>
<p>Why is there a need to “limit” how well the model fits the data? Because we, social scientists and data scientists, very commonly <strong>overfit</strong> the data. The plot below shows a simulation from <a href="https://drsimonj.svbtle.com/ridge-regression-with-glmnet">Simon Jackson</a> where we can see that when tested on a training set, OLS and Ridge tend to overfit the data. However, when tested on the test data, Ridge regression has lower out of sample error as the <span class="math inline">\(R2\)</span> is higher for models with different observations.</p>
<p><img src="figs/unnamed-chunk-1-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>The strength of the ridge regression comes from the fact that it compromises fitting the training data really well for improved generalization. In other words, we increase <strong>bias</strong> (because we force the coefficients to be smaller) for lower <strong>variance</strong> (but we make it more general). In other words, the whole gist behind ridge regression is penalizing very large coefficients for better generalization.</p>
<p>Having that intuition in mind, the predictors of the ridge regression need to be standardized. Why is this the case? Because due to the scale of a predictor, its coefficient can be more penalized than other predictors. Suppose that you have the income of a particular person (measured in thousands per months) and time spent with their families (measured in seconds) and you’re trying to predict happiness. A one unit increase in salary could be penalized much more than a one unit increase in time spent with their families <strong>just</strong> because a one unit increase in salary can be much bigger due to it’s metric.</p>
<p>In R, you can fit a ridge regression (and nearly all other machine learning models) through the <code>caret</code> package. Let’s load the packages that we will work with and read the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret) <span class="co"># Fitting machine learning models</span>
<span class="kw">library</span>(rsample) <span class="co"># Create data partitions</span>
<span class="kw">library</span>(vip) <span class="co"># For figuring out important variables for prediction</span>

data_link &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot;</span>
pisa &lt;-<span class="st"> </span><span class="kw">read.csv</span>(data_link)</code></pre></div>
<p>First thing we do is separate the training and test data. All of our modelling will be performed on the training data and the test data is saved for later (the test data must be completely ignored until you have your final tuned model).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Separate training/testing split</span>

<span class="co"># Place a seed for reproducing the results</span>
<span class="kw">set.seed</span>(<span class="dv">23141</span>)
split_pisa &lt;-<span class="st"> </span><span class="kw">initial_split</span>(<span class="dt">data =</span> pisa, <span class="dt">prop =</span> .<span class="dv">7</span>)
pisa_test &lt;-<span class="st"> </span><span class="kw">testing</span>(split_pisa)
pisa_train &lt;-<span class="st"> </span><span class="kw">training</span>(split_pisa)</code></pre></div>
<p>The ridge regression has a parameter called <code>lambda</code> which needs to be set by us. <code>lambda</code> is the “weight” term in the ridge equation, which controls how much weight do we want to give to the “shrinkage penalty”. If this lambda is 0, it means we attach <strong>no</strong> weight to the penalty term and swe will get the same result over OLS. Let’s try that:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">############################# Ridge regression ################################
###############################################################################

ridge_grid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="co"># Here we specify the lambda to be zero</span>
  <span class="dt">lambda =</span> <span class="dv">0</span>,
  <span class="co"># Here we specify the type of penalized regression: 0 is ridge regression</span>
  <span class="dt">alpha =</span> <span class="dv">0</span>
)

<span class="co"># The train function accepts several arguments</span>
ridge_mod &lt;-<span class="st"> </span><span class="kw">train</span>(
  <span class="co"># math_score is the dependen variable and all other are independent variables</span>
  math_score <span class="op">~</span><span class="st"> </span>MISCED <span class="op">+</span><span class="st"> </span>FISCED <span class="op">+</span><span class="st"> </span>HISEI <span class="op">+</span><span class="st"> </span>REPEAT <span class="op">+</span><span class="st"> </span>IMMIG <span class="op">+</span><span class="st"> </span>DURECEC <span class="op">+</span><span class="st"> </span>BSMJ,
  <span class="co"># The training data</span>
  <span class="dt">data =</span> pisa_train,
  <span class="co"># The R package that runs the ridge regression</span>
  <span class="dt">method =</span> <span class="st">&quot;glmnet&quot;</span>,
  <span class="co"># Here is where we pass the lambda argument</span>
  <span class="dt">tuneGrid =</span> ridge_grid,
  <span class="dt">lambda =</span> <span class="dv">0</span>,
  <span class="co"># Here is where the function standardizes the predictors before</span>
  <span class="co"># fitting the models</span>
  <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;none&quot;</span>)
)

<span class="co"># Get ridge coefficients</span>
res &lt;-<span class="st"> </span>ridge_mod<span class="op">$</span>finalModel
ridge_coef &lt;-<span class="st"> </span><span class="kw">predict</span>(res, <span class="dt">s =</span> <span class="dv">0</span>, <span class="dt">type =</span> <span class="st">&quot;coefficients&quot;</span>)

############################# Linear model ####################################
###############################################################################

iv_vars &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;MISCED&quot;</span>, <span class="st">&quot;FISCED&quot;</span>, <span class="st">&quot;HISEI&quot;</span>, <span class="st">&quot;REPEAT&quot;</span>, <span class="st">&quot;IMMIG&quot;</span>, <span class="st">&quot;DURECEC&quot;</span>, <span class="st">&quot;BSMJ&quot;</span>)
pisa_tst &lt;-<span class="st"> </span>pisa_train
pisa_tst[iv_vars] &lt;-<span class="st"> </span><span class="kw">scale</span>(pisa_tst[iv_vars])

lm_coef &lt;-<span class="st"> </span><span class="kw">coef</span>(
  <span class="kw">lm</span>(math_score <span class="op">~</span><span class="st"> </span>MISCED <span class="op">+</span><span class="st"> </span>FISCED <span class="op">+</span><span class="st"> </span>HISEI <span class="op">+</span><span class="st"> </span>REPEAT <span class="op">+</span><span class="st"> </span>IMMIG <span class="op">+</span><span class="st"> </span>DURECEC <span class="op">+</span><span class="st"> </span>BSMJ,
     <span class="dt">data =</span> pisa_tst)
)

############################# Comparing model #################################
###############################################################################

comparison &lt;-
<span class="st">  </span><span class="kw">data.frame</span>(<span class="dt">coefs =</span> <span class="kw">names</span>(lm_coef),
             <span class="st">`</span><span class="dt">Linear coefficients</span><span class="st">`</span> =<span class="st"> </span><span class="kw">unname</span>(<span class="kw">round</span>(lm_coef, <span class="dv">2</span>)),
             <span class="st">`</span><span class="dt">Ridge coefficients</span><span class="st">`</span> =<span class="st"> </span><span class="kw">round</span>(<span class="kw">as.vector</span>(ridge_coef), <span class="dv">2</span>))

knitr<span class="op">::</span><span class="kw">kable</span>(comparison)</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">coefs</th>
<th align="right">Linear.coefficients</th>
<th align="right">Ridge.coefficients</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">473.05</td>
<td align="right">473.05</td>
</tr>
<tr class="even">
<td align="left">MISCED</td>
<td align="right">2.94</td>
<td align="right">2.94</td>
</tr>
<tr class="odd">
<td align="left">FISCED</td>
<td align="right">11.78</td>
<td align="right">11.78</td>
</tr>
<tr class="even">
<td align="left">HISEI</td>
<td align="right">18.07</td>
<td align="right">18.07</td>
</tr>
<tr class="odd">
<td align="left">REPEAT</td>
<td align="right">-22.09</td>
<td align="right">-22.09</td>
</tr>
<tr class="even">
<td align="left">IMMIG</td>
<td align="right">6.01</td>
<td align="right">6.01</td>
</tr>
<tr class="odd">
<td align="left">DURECEC</td>
<td align="right">0.55</td>
<td align="right">0.55</td>
</tr>
<tr class="even">
<td align="left">BSMJ</td>
<td align="right">10.62</td>
<td align="right">10.62</td>
</tr>
</tbody>
</table>
<p>Coming from a social science background, it might seem counterintuitive that the researcher has to specify tuning parameters for the model. In traditional social science statistics, models usually estimate similar values internally and the user doesn’t have to think about them. However, there are strategies already implemented to explore the combination of many possible values. With our previous example, we just have to add a number of lambda values and <code>train</code> will find the best one:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">663421</span>)

ridge_grid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="co"># Here we specify the lambda to several possible values</span>
  <span class="dt">lambda =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dt">length.out =</span> <span class="dv">300</span>),
  <span class="co"># Here we specify the type of penalized regression: 0 is ridge regression</span>
  <span class="dt">alpha =</span> <span class="dv">0</span>
)

ridge_mod &lt;-<span class="st"> </span><span class="kw">train</span>(
  math_score <span class="op">~</span><span class="st"> </span>MISCED <span class="op">+</span><span class="st"> </span>FISCED <span class="op">+</span><span class="st"> </span>HISEI <span class="op">+</span><span class="st"> </span>REPEAT <span class="op">+</span><span class="st"> </span>IMMIG <span class="op">+</span><span class="st"> </span>DURECEC <span class="op">+</span><span class="st"> </span>BSMJ,
  <span class="dt">data =</span> pisa_train,
  <span class="dt">method =</span> <span class="st">&quot;glmnet&quot;</span>,
  <span class="dt">tuneGrid =</span> ridge_grid,
  <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
  <span class="co"># Performs cross validation through all grid parameters</span>
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>)
)

<span class="kw">plot</span>(ridge_mod<span class="op">$</span>finalModel, <span class="dt">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="dt">label =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p><img src="figs/unnamed-chunk-5-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Here we can see how our coefficients are affected by increasing weight of the <code>lambda</code> parameter. And we can figure out the best lambda inspecting <code>bestTune</code> inside <code>ridge_mod</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">best_lambda_ridge &lt;-<span class="st"> </span>ridge_mod<span class="op">$</span>bestTune<span class="op">$</span>lambda
best_lambda_ridge</code></pre></div>
<pre><code>## [1] 2.67893</code></pre>
<p>However, there’s no need to rerun the model with this optimal value; since <code>train</code> <strong>had</strong> to run that model, it saves it as the most optimal:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">holdout_ridge &lt;-
<span class="st">  </span><span class="kw">RMSE</span>(
    <span class="kw">predict</span>(ridge_mod, pisa_test, <span class="dt">s =</span> best_lambda_ridge),
    pisa_test<span class="op">$</span>math_score
  )

train_rmse_ridge &lt;-
<span class="st">  </span>ridge_mod<span class="op">$</span>results <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(lambda <span class="op">==</span><span class="st"> </span>best_lambda_ridge) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pull</span>(RMSE)

<span class="kw">c</span>(<span class="dt">holdout_rmse =</span> holdout_ridge, <span class="dt">train_rmse =</span> train_rmse_ridge)</code></pre></div>
<pre><code>## holdout_rmse   train_rmse 
##     79.11585     76.37490</code></pre>
<p>The holdout RMSE will always be higher than the training RMSE as the training set nearly always <strong>memorizes</strong> the data better for the training.</p>
</div>
<div id="lasso-regularization" class="section level2">
<h2><span class="header-section-number">1.2</span> Lasso regularization</h2>
<p>The Lasso regularization is very similar to the ridge regularization where only one thing changes: the penalty term. Instead of squaring the coefficients in the penalty term, the lasso regularization takes the absolute value of the coefficient.</p>
<span class="math display">\[\begin{equation}
RSS + \lambda \sum_{k = 1}^n |\beta_j|
\end{equation}\]</span>
<p>Althought it might not be self-evident from this, the lasso reguralization has an important distinction: it can force a coefficient to be zero. This means that lasso does a selection of variables which have big coefficients while not compromising the RSS of the model. The problem with ridge regression is that as the number of variables increases, the training error will almost always decrease but the test error will not.</p>
<p>For example, if we define the same model from above using a lasso, you’ll see that it forces coefficients to be <strong>exactly zero</strong> if they don’t add anything relative to the RSS of the model. This means that variables which do not add anything to the model will be excluded unless they add explanatory power that compensates the size of their coefficient. Here’s the same lasso example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">663421</span>)

lasso_grid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="co"># Here we specify the lambda to several possible values</span>
  <span class="dt">lambda =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dt">length.out =</span> <span class="dv">300</span>),
  <span class="co"># Here we specify the type of penalized regression: 1 is lasso regression</span>
  <span class="dt">alpha =</span> <span class="dv">1</span>
)

lasso_mod &lt;-<span class="st"> </span><span class="kw">train</span>(
  math_score <span class="op">~</span><span class="st"> </span>MISCED <span class="op">+</span><span class="st"> </span>FISCED <span class="op">+</span><span class="st"> </span>HISEI <span class="op">+</span><span class="st"> </span>REPEAT <span class="op">+</span><span class="st"> </span>IMMIG <span class="op">+</span><span class="st"> </span>DURECEC <span class="op">+</span><span class="st"> </span>BSMJ,
  <span class="dt">data =</span> pisa_train,
  <span class="dt">method =</span> <span class="st">&quot;glmnet&quot;</span>,
  <span class="dt">tuneGrid =</span> lasso_grid,
  <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>)
)

<span class="kw">plot</span>(lasso_mod<span class="op">$</span>finalModel, <span class="dt">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="dt">label =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p><img src="figs/unnamed-chunk-8-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>In contrast to the ridge regression, where coefficients are forced to be close to zero, the lasso penalty actually forces some coefficients <strong>to be zero</strong>. This property means that the lasso makes a <strong>selection of the variables with the higher coefficients</strong> and eliminates those which do not have a strong relationship. Lasso is usually better at model interpretation because it removes redundant variables while ridge can be useful if you want to keep a number of variables in the model, despite them being weak predictors (as controls, for example).</p>
<p>The lasso actually works exactly as the ridge in the <code>caret</code> package, meaning that it automatically checks the most optimal value for lambda:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">best_lambda_lasso &lt;-<span class="st"> </span>lasso_mod<span class="op">$</span>bestTune<span class="op">$</span>lambda
best_lambda_lasso</code></pre></div>
<pre><code>## [1] 0.1906355</code></pre>
<p>To actually check the final model and which variables are kept, we can access it:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">holdout_lasso &lt;-
<span class="st">  </span><span class="kw">RMSE</span>(
    <span class="kw">predict</span>(lasso_mod, pisa_test, <span class="dt">s =</span> best_lambda_lasso),
    pisa_test<span class="op">$</span>math_score
  )

train_rmse_lasso &lt;-
<span class="st">  </span>lasso_mod<span class="op">$</span>results <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(lambda <span class="op">==</span><span class="st"> </span>best_lambda_lasso) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pull</span>(RMSE)

<span class="kw">c</span>(<span class="dt">holdout_rmse =</span> holdout_lasso, <span class="dt">train_rmse =</span> train_rmse_lasso)</code></pre></div>
<pre><code>## holdout_rmse   train_rmse 
##     79.13141     76.31036</code></pre>
<p>So far, we can check which model is performing better:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_comparison &lt;-
<span class="st">  </span><span class="kw">data.frame</span>(
    <span class="dt">type =</span> <span class="kw">c</span>(<span class="st">&quot;test RMSE&quot;</span>, <span class="st">&quot;training RMSE&quot;</span>),
    <span class="dt">ridge =</span> <span class="kw">c</span>(holdout_ridge, train_rmse_ridge),
    <span class="dt">lasso =</span> <span class="kw">c</span>(holdout_lasso, train_rmse_lasso)
  )

model_comparison</code></pre></div>
<pre><code>##            type    ridge    lasso
## 1     test RMSE 79.11585 79.13141
## 2 training RMSE 76.37490 76.31036</code></pre>
<p>Currently the ridge regression has a very minor advantaged over the lasso yet the difference is probably within the margin of error. Depending on your aim, you might want to choose either of the models. For example, if our models contained a lot of variables, lasso might be more interpretable as it reduces the number of variables. However, if you have reasons to believe that keeping all variables in the model is important, then ridge provides an advantage.</p>
</div>
<div id="elastic-net-regularization" class="section level2">
<h2><span class="header-section-number">1.3</span> Elastic Net regularization</h2>
<p>If you’re aware of ridge and lasso, then elastic net regularization is a logical step. Elastic Net (the name sounds fancy, but it is also an adaptation of OLS) combines both penalties to form one single equation.</p>
<p>Here we define our ridge penalty:</p>
<p><span class="math display">\[ridge = \lambda \sum_{k = 1}^n |\beta_j|\]</span></p>
<p>And here we define our lasso penalty:</p>
<p><span class="math display">\[lasso = \lambda \sum_{k = 1}^n \beta_j^2\]</span></p>
<p>Elastic net regularization is the addition of these two penalties in comparison to the RSS:</p>
<p><span class="math display">\[RSS + lasso + ridge\]</span></p>
<p>I think the best explanation for elastic net reguarlization comes from Boehmke &amp; Greenwell (2019):</p>
<blockquote>
<p>Although lasso models perform feature selection, when two strongly correlated features are pushed towards zero, one may be pushed fully to zero while the other remains in the model. Furthermore, the process of one being in and one being out is not very systematic. In contrast, the ridge regression penalty is a little more effective in systematically handling correlated features together. Consequently, the advantage of the elastic net penalty is that it enables effective regularization via the ridge penalty with the feature selection characteristics of the lasso penalty.</p>
</blockquote>
<p>Essentially, you now have two tuning parameters. In the grid of values, instead of specifying an alpha of <code>0</code> (ridge) or <code>1</code> (lasso), <code>caret</code> will slide through several values of <code>alpha</code> ranging from 0 to 1 and compare that to several values of <code>lambda</code>.</p>
<p>However, <code>train</code> can already take care of this and calculate the most optimal value automatically with specifying a grid of values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">663421</span>)

elnet_mod &lt;-<span class="st"> </span><span class="kw">train</span>(
  math_score <span class="op">~</span><span class="st"> </span>MISCED <span class="op">+</span><span class="st"> </span>FISCED <span class="op">+</span><span class="st"> </span>HISEI <span class="op">+</span><span class="st"> </span>REPEAT <span class="op">+</span><span class="st"> </span>IMMIG <span class="op">+</span><span class="st"> </span>DURECEC <span class="op">+</span><span class="st"> </span>BSMJ,
  <span class="dt">data =</span> pisa_train,
  <span class="dt">method =</span> <span class="st">&quot;glmnet&quot;</span>,
  <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>),
  <span class="co"># Here 25 means that it will try 25 values of</span>
  <span class="co"># alpha and then N numbers of alpha</span>
  <span class="dt">tuneLength =</span> <span class="dv">25</span>
)

best_lambda_elnet &lt;-<span class="st"> </span>elnet_mod<span class="op">$</span>bestTune<span class="op">$</span>lambda

holdout_elnet &lt;-
<span class="st">  </span><span class="kw">RMSE</span>(
    <span class="kw">predict</span>(elnet_mod, pisa_test),
    pisa_test<span class="op">$</span>math_score
  )

train_rmse_elnet &lt;-
<span class="st">  </span>elnet_mod<span class="op">$</span>results <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(alpha <span class="op">==</span><span class="st"> </span>elnet_mod<span class="op">$</span>bestTune<span class="op">$</span>alpha, lambda <span class="op">==</span><span class="st"> </span>best_lambda_elnet) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pull</span>(RMSE)

<span class="kw">c</span>(<span class="dt">holdout_rmse =</span> holdout_elnet, <span class="dt">train_rmse =</span> train_rmse_elnet)</code></pre></div>
<pre><code>## holdout_rmse   train_rmse 
##     79.12763     76.31005</code></pre>
<p>The RMSE of the elastic net is somewhat lower than then ridge and lasso but also probably within the margin of error. Let’s compare it visually:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_comparison<span class="op">$</span>elnet &lt;-<span class="st"> </span><span class="kw">c</span>(holdout_elnet, train_rmse_elnet)
model_comparison</code></pre></div>
<pre><code>##            type    ridge    lasso    elnet
## 1     test RMSE 79.11585 79.13141 79.12763
## 2 training RMSE 76.37490 76.31036 76.31005</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_comparison <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pivot_longer</span>(<span class="op">-</span>type) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(name, value, <span class="dt">color =</span> type, <span class="dt">group =</span> type)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">position =</span> <span class="st">&quot;dodge&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">name =</span> <span class="st">&quot;RMSE&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_discrete</span>(<span class="dt">name =</span> <span class="st">&quot;Models&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>()</code></pre></div>
<p><img src="figs/unnamed-chunk-13-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">1.4</span> Exercises</h2>
<p>The <a href="https://www.fragilefamilieschallenge.org/">Fragile Families Challenge</a> is a study that aimed to predict a series of indicators of children at age 15 only using data from ages 0 to 9. With this challenge, the principal investigators wanted to test whether skills such as cognitive and non-cognitive abilities were correctly predicted. With that idea in mind, they were interested in following up children that beat the ‘predictions’: those children that exceeded the model’s prediction, for example given their initial conditions.</p>
<p>Using a similarly constructed non-cognitive proxy, I’ve created a non-cognitive index using the PISA 2018 for the United States which is the average of the questions:</p>
<ul>
<li>ST182Q03HA - I find satisfaction in working as hard as I can.</li>
<li>ST182Q04HA - Once I start a task, I persist until it is finished.</li>
<li>ST182Q05HA - Part of the enjoyment I get from doing things is when I improve on my past performance.</li>
<li>ST182Q06HA - If I am not good at something, I would rather keep struggling to master it than move on to something I may […]</li>
</ul>
<p>The scale of the index goes from 1 to 4, where 4 the student strongly agrees and 1 is they completely disagree. In other words, this index shows that the higher the value, the higher the non cognitive skills.</p>
<p>In these series of exercises you will have to try different models that predict this index of non-cognitive skills, choose the best model and look at the most important variables.</p>
<p>First, read in the data with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data_link &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot;</span></code></pre></div>
<div id="split-the-data-into-testtraining-data" class="section level3">
<h3><span class="header-section-number">1.4.1</span> Split the data into test/training data</h3>
<p>Remember to set the seed to <code>2341</code> so that everyone can compare their results.</p>
</div>
<div id="run-a-ridge-regression-with-non-cognitive-as-the-dependent-variable" class="section level3">
<h3><span class="header-section-number">1.4.2</span> Run a ridge regression with non-cognitive as the dependent variable</h3>
<p>Use as many variables as you want (you can reuse the previous variables from the examples or pick all of them). A formula of the like <code>noncogn ~ .</code> will regress <code>noncogn</code> on all variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 1) Define ridge grid of values for lambda</span>
ridge_grid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">lambda =</span> 
  <span class="dt">alpha =</span> <span class="dv">0</span>
)

<span class="co"># 2) Use the train function to train the model on the *training set*</span>

<span class="co"># 3) Extract the best lambda and calculate the RMSE on the test set</span>

<span class="co"># 4) Extract the RMSE of the training set</span>

<span class="co"># 5) Compare both holdout and training RMSE</span></code></pre></div>
</div>
<div id="which-are-the-most-important-variables" class="section level3">
<h3><span class="header-section-number">1.4.3</span> Which are the most important variables?</h3>
<p>Comment on their coefficients and whether they make sense to be included in the model.</p>
</div>
<div id="run-a-lasso-regression-with-the-same-specification-as-above" class="section level3">
<h3><span class="header-section-number">1.4.4</span> Run a lasso regression with the same specification as above</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Define ridge grid of values for lambda</span>
lasso_grid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">lambda =</span> 
  <span class="dt">alpha =</span> <span class="dv">1</span>
)

<span class="co"># Reproduce previous steps</span></code></pre></div>
<p>Which model is performing better? Ridge or Lasso? Are the same variables the strongest predictors across models? Which variables are the strongest predictors?</p>
</div>
<div id="run-an-elastic-net-regression-on-non-cognitive-skills" class="section level3">
<h3><span class="header-section-number">1.4.5</span> Run an elastic net regression on non cognitive skills</h3>
<p>Since <code>train</code> already takes care of trying all possible values, there’s no need to pass a grid of lambda values. It is only needed to set the <code>tuneLength</code> to a number of alpha values.</p>
</div>
</div>
<div id="bibliography" class="section level2">
<h2><span class="header-section-number">1.5</span> Bibliography</h2>
<p>Boehmke, B., &amp; Greenwell, B. M. (2019). Hands-On Machine Learning with R. CRC Press.</p>
<p>Friedman, J., Hastie, T., &amp; Tibshirani, R. (2001). The elements of statistical learning (Vol. 1, No. 10). New York: Springer series in statistics.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="syllabus.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ml_socsci.pdf", "ml_socsci.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
