[
["index.html", "Machine Learning for Social Scientists Preface", " Machine Learning for Social Scientists Jorge Cimentada 2020-06-23 Preface Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists. These are intended to introduce social scientists to concepts in machine learning using traditional social science examples and datasets. Currently, it is not intended to be a book but rather supporting material for the course. Perhaps it evolves enough to be a book some day. Day 1 - Morning: Slides Day 1 - Afternoon: Slides Day 2 - Morning: Slides Day 2 - Afternoon: Slides Day 3 - Morning: Slides Day 3 - Afternoon: Slides "],
["machine-learning-for-social-scientists.html", "Chapter 1 Machine Learning for Social Scientists 1.1 A different way of thinking 1.2 Split your data into training/testing 1.3 Cross-validation 1.4 Bias-Variance Tradeoff 1.5 An example", " Chapter 1 Machine Learning for Social Scientists Machine Learning practitioners and social scientists share many things in common. These shared traits are mostly related to the transformation, analysis and evaluation of statistical models. In fact, when many of my fellow social scientists take any introductory course on machine learning, I often hear that many of the things they get taught are very common in their social statistics classes. This is good news! This means that you already have a foot inside the field without even knowing it. Machine Learning practitioners use many of the same statistical model we use and also many of transformation techniques that we use. However, there are important differences on how we analyze data and how we answer our questions. In this chapter I will elaborate on how machine learning practitioners have developed strategies different from social scientists for analyzing their data, how their analysis workflow compares to ours and finally, a tour around their way of thinking, which has evolved to be very different from ours. I hope that by understanding the strategies and techniques that machine learning practitioners use, social scientists would expand their analysis toolbox, allowing us to complement their way of thinking with our strong research design skills and modelling techniques. 1.1 A different way of thinking The first question we want to ask ourselves is, what is machine learning? Machine Learning bears indeed a fancy name which brings to mind thoughts related to artificial intelligence and robots. However, as you’ll see throughout the course, most terms and models used in machine learning are actually what we know as statistical models. The overaching difference in the definition of machine learning and social statistics is not the models or new strategies for analyzing data. It is the main objective of the analysis. What is machine learning after all? Using statistical methods to learn the data enough to be able to predict it accurately on new data That sounds somewhat familiar to us social scientists. Perhaps our goal is not to predict our data but it is certainly to learn it and understand it. In particular, social scientists are interested in figuring out if our theoretical description of a problem fits the data we have collected or have at hand. We do that by carefully building a model that explains the problem really well such that we can extrapolate an explanation for the problem from the data. Our gold standard to check whether we did a good job is to collect the exact same data again and see if our final models replicates. How does this differ from the way of thinking of machine learning practitioners? The main objective in a machine learning problem is accurate predictions; that is, regardless of how well they understand a problem, they want to learn the data enough to predict it well. Prediction problems are usually concerned with building and tweaking a model that predicts a dependent variable accurately on your data, such that when new data arrives, the model can predict it just as accurately. This does not mean that machine learning practitioners don’t have domain-specific knowledge of what they’re trying to predict (they have to select variables to include in a model just as we do). However, ‘parsimonious models’ (that is, simple and interpretable models) are not something they’re limited to (in contrast, social scientists hardly experiment with non-interpretable models). They might use models which contain up to hundreds of variables if that increases predictive accuracy. Although that might sound counter-intuitive to social scientists, more and more ground is being gained by this type of thinking in the social sciences (Watts 2014; Yarkoni and Westfall 2017). The difference between how we both approach research questions is the problem of inference versus prediction (Breiman and others 2001). That is the fundamental difference between the approach used by social scientists and practitioners of machine learning. However, for having such drastic differences in our objective, we share a lot of common strategies. For example, here’s the typical workflow of a social scientist: This is our safe zone: we understand these steps and we’ve exercised them many times. We begin by importing our data and inmediately start to clean it. This involves, for example, collapsing fine grained groups into bigger categories, transforming variables using logarithms and creating new variables which reflect important concepts from our theoretical model. Once we’re confident with our set of variables, we begin the iterative process of visualizing our data, fitting statistical models and evaluating the fit of the model. This is an iterative process because the results of our model might give us ideas on new variables or how to recode an existing variable. This prompts us to repeat the same process again with the aim of carefully building a model that fits the data well. Well, let me break it to you but this same process is very familiar to the machine learning process: They import their data, they wrangle their data, they fit statistical models, and they evaluate the fit of their models. They might have different names for the same things but in essence, they are more or less the same. For example, here are some common terms in the machine learning literature which have exact equivalents in social statistics: Features –&gt; Variables Feature Engineering –&gt; Creating Variables Learning Algorithms –&gt; Statistical Models Supervised Learning –&gt; Models that have a dependent variable Unsupervised Learning –&gt; Models that don’t have a dependent variable, such as clustering Classifiers –&gt; Models for predicting categorical variables, such as logistic regression and you’ll find more around. These are the common steps which you’ll find between both fields. However, machine learning practioners have developed extra steps which help them achieve their goal of predicting new data well: Training/Testing data –&gt; Unknown to us Cross-validation –&gt; Unknown to us Grid search –&gt; Unknown to us Loss functions –&gt; Model fit –&gt; Known to us but are not predominant (\\(RMSE\\), \\(R^2\\), etc…) These are very useful concepts and we’ll focus on those in this introduction. In this introduction I won’t delve into the statistical models (learning algorithms) used in machine learning as these will be discussed in later chapters but I wanted to highlight that although they share similarities with the models used in social statistics, there are many models used in the machine learning literature which are unknown to us. Let’s delve into each of these three new concepts. Before we beginning explaining these concepts and using R, let’s load the packages we’ll use in this chapter: # All these packages can be installed with `install.packages` library(ggplot2) library(patchwork) library(scales) library(tidymodels) 1.2 Split your data into training/testing Since the main objective in machine learning is to predict data accurately, all of their strategies are geared towards avoiding overfitting/underfitting the data. In other words, they want to capture all the signal and ignore the noise: set.seed(2313) n &lt;- 500 x &lt;- rnorm(n) y &lt;- x^3 + rnorm(n, sd = 3) age &lt;- rescale(x, to = c(0, 100)) income &lt;- rescale(y, to = c(0, 5000)) age_inc &lt;- data.frame(age = age, income = income) y_axis &lt;- scale_y_continuous(labels = dollar_format(suffix = &quot;€&quot;, prefix = &quot;&quot;), limits = c(0, 5000), name = &quot;Income&quot;) x_axis &lt;- scale_x_continuous(name = &quot;Age&quot;) bad_fit &lt;- ggplot(age_inc, aes(age, income)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + y_axis + x_axis + ggtitle(&quot;Underfit&quot;) + theme_linedraw() overfit &lt;- ggplot(age_inc, aes(age, income)) + geom_point() + geom_smooth(method = &quot;loess&quot;, span = 0.015) + y_axis + x_axis + ggtitle(&quot;Overfit&quot;) + theme_linedraw() goodfit &lt;- ggplot(age_inc, aes(age, income)) + geom_point() + geom_smooth(method = &quot;loess&quot;, span = 0.9) + y_axis + x_axis + ggtitle(&quot;Ideal fit&quot;) + theme_linedraw() bad_fit + overfit + goodfit Figure 1.1: Different ways of fitting your data The first panel of figure 1.1 shows a model which is not flexible, as it fits a straight line without capturing the subtle non-linearities of the data. The middle panel is too flexible as it captures much of the random noise of the non-linear relationship. Finally, the third panel shows the ideal fit, where the fitted line is flexible enough to capture the non-linear relationship in the data yet it it is mainly unaffected by the random noise in the data. Although social scientists are aware of these concepts, we really don’t think about them a lot. When we perform statistical modelling we don’t really think about whether we’re overfitting or underfitting: we’re mostly paying attention to whether the statistical relationships make sense. For example, how would social scientists fit a model? They would take the entire data and fit the model on it. How do you know you’re overfitting? Well, one very easy and naive approach is to randomly divide your data into two chunks called training and testing: The training data usually consists of a random sample of around ~70% of the initial data and the testing data the remaining ~30% of the initial data. If a particular row is in the training data, it must not be on the testing data. In contrast, if a particular row is in the testing data, it shouldn’t be in the training data either. Why should splitting the data into two chunks help us fix the problem of overfitting? Because you can elaborate your model in the training set as much as you want, and when you’re confident enough, the testing data can serve as an unseen, pristine source of data on which you can evaluate your model. If fitting your model on the testing data shows that your model was too optimistic, you were probably overfitting the data. Let’s go through the steps one by one. Fit your model in the training data (remember, that’s a random sample of about 70% of the initial data) evaluate the model fit and make the same changes you would do on your complete data: create new variables, recode variables, etc. You can think of this chunk as the complete data to perform your analysis. It is the equivalent of the initial data where social scientists fit their models. Once you’re very comfortable with your model, the best recipe for checking whether your model was overfitting is to use this fitted model to predict on the other chunk of data (the testing data): If you tweaked your model in such a way that it learned the noise of your training data, it will perform poorly on the testing data, since your the model didn’t capture the overall trend in the training data but rather the noise. It’s time to introduce how we perform these steps in R. For this, we’ll use the package tidyflow, a package created for this book. It aims to have a simple and intuitive workflow for machine learning which you’ll learn through this book. You can install the package with the code below: install.packages(&quot;devtools&quot;) devtools::install_github(&quot;cimentadaj/tidyflow&quot;) In R we can build a machine learning ‘workflow’ with the function tidyflow. To this workflow, we can plug in steps that you can execute. In our example, to plug in the step of partitioning the data into training and testing, you can use plug_split with the function initial_split: library(tidyflow) ml_flow &lt;- age_inc %&gt;% tidyflow(seed = 2313) %&gt;% plug_split(initial_split) ml_flow ## ══ Tidyflow ════════════════════════════════════════════════════════════════════ ## Data: 500 rows x 2 columns ## Split: initial_split w/ default args ## Recipe/Formula: None ## Resample: None ## Grid: None ## Model: None tidyflow already knows that age_inc is the main data source and that we need to apply the training/testing split with initial_split. You can think of this as plan that will be executed once you tell it to. Let’s get back to our example and suppose that you fit your model several times on the training data, tweaking it to improve performance (when I say tweaking I mean applying transformations, including new variables, recoding old variables, including polynomials, etc..). When you think you’re ready, you use this model to predict on the testing data and find out that the model was indeed overfitting the data because you cannot predict the testing data as well as the training data. You then go back to the training data, tweak some more, run some models again and when you think the model is ready again, you predict on your testing data again and find that it improved. Then you repeate the process again, \\(3\\), \\(4\\), \\(5\\) \\(N\\) times. If you do that, you will, in very subtle ways, start to overfit your model on the testing data! In other words, you’re fitting a model \\(N\\) times on your training data, evaluating its fit on the testing data and then tweaking again to improve the prediction on the testing data. The testing data should serve as the final dataset to compare your model: you should not tweak the model again after seeing how your model fits the unseen testing data. That doesn’t sound right. It seems we have too few “degrees of freedom” to test the accuracy of our model. We can tweak the model in the training data as much as we want but we only have one attempt at testing our model against the testing data. How can we evaluate, then, whether we’re overfitting with the training data alone, then? Enter cross-validation 1.3 Cross-validation The idea behind cross-validation is to allow the user to check whether they’re overfitting the data without predicting on the testing data. How does it work? First, we only work with our training data and replicate the training data 10 times The 10 rectangular red rows below the training data, contain an exact replica of the initial training data. That is, if the initial training data has 500 rows and 10 columns, then each of these red rectangled rows also has 500 rows and 10 columns. The idea behind this approach is that for each rectangled row, you can use 70% of the data to fit your model and then predict on the remaining 30%. For example, for the first rectangled row, you would fit your initial model model with some tweak (let’s say, adding a squared term to the age variable to check if that improves fit) on the training data and then predict on the testing data to evaluate the fit: Since we fit a model to the trainingdata of each rectangled row and then predict on the testing data of each rectangled row, we can record how well our model is doing for each of our replicate data sets. For example, for the first row we record the \\(RMSE\\) of the prediction on the testing data. For the second rectangled row, fit the exact same model (that is, including the age squared term) on 70% of the training data, predict on the testing data and record the \\(RMSE\\). And then repeat the same iteration for every rectangled row: After you’ve fitted the model and evaluated the model 10 times, you have 10 values of the \\(RMSE\\). With these 10 values you can calculate the average \\(RMSE\\) and standard error of your model’s performance. Note that with this approach, the testing data changes in each rectangled row, making sure that each ~30% chunk of the data passes through the testing dataset at some point during the predictions. This is done to ensure the predictions are as balanced as possible. This approach offers a way to iterate as many times as you want on tweaking your model and predicting on the cross-validated testing data without actually predicting on the initial testing dataset. This is the least bad approach that is currently accepted in the literature. Why is it the least bad approach? Because if we tweak the model on these 10 replicas one time, then a second time, then a third time, etc…, we’ll also start overfitting on each of these 10 slots! The superiority of this approach over tweaking on the training data is that since we have 10 replicas, we can take the average of model fit metrics and also obtain standard errors. This allows to have a somewhat balanced account of how our model fit is doing and the uncertainty around it. That said, since we will always overfit in someway using a cross-validation approach, the final error of your model fit on the training data will always be over optimistic (lower error than what you will actually have, if you predicted on the pristine testing data. Based on our previous tidyflow, we can plug in a cross-validation step with plug_resample. There are many different cross-validation techniques but let’s focus on the one from our example (replicating the data 10 times). For that, we use the function vfold_cv: ml_flow &lt;- ml_flow %&gt;% plug_resample(vfold_cv) ml_flow ## ══ Tidyflow ════════════════════════════════════════════════════════════════════ ## Data: 500 rows x 2 columns ## Split: initial_split w/ default args ## Recipe/Formula: None ## Resample: vfold_cv w/ default args ## Grid: None ## Model: None 1.4 Bias-Variance Tradeoff Before we elaborate a complete coded example, it’s important to talk about the concept of bias-variance tradeoff used in machine learning problems. As was shown in figure 1.1, we want the ideal fit without overfitting or underfitting the data. In some instances, fitting the data that well is very difficult because we don’t have variables that reflect the data generating process or because the relationship is too complex. In that case, for machine learning problems, you might want to either underfit or overfit slightly, depending on your problem. Overfitting your data has some value, which is that we learn the data very well. This is often called a model with a lot of flexibility. A model that can learn all the small intricacies of the data is often called a flexible model. There is very little bias in a model like this one, since we learn the data very very well. However, at the expense of bias, overfitting has a lot of variance. If we predict on a new dataset using the overfitted model, we’ll find a completely different result from the initial model. If we repeat the same on another dataset, we’ll find another different result. That is why models which can be very flexible are considered to have very little bias and a lot of variance: The model above fits the criteria: On the other hand, models which are not flexible, have more bias and less variance. One familiar example of this is the linear model. By fitting a straight line through the data, the variance is very small: if we run the same exact model on a new data, the fitted line is robust to slight changes in the data (outliers, small changes in the tails of the distribution, etc..). However, the fitted line doesn’t really capture the subtle trends in the data (assuming the relationship is non-linear, which is in most cases). That is why non-flexible models are often called to have high bias and low variance: or in other words: In reality, what we usually want is something located in the middle of these two extremes: we want a model that is neither too flexible that overfits the data nor too unflexible that misses the signal. There is really no magical recipe to achieving the perfect model and our best approach is to understand our model’s performance using techniques such as cross-validation to assess how much our model is overfitting/underfitting the data. Even experiencied machine learning practitioners can build models that overfit the data (one notable example is the results from the Fragile Families Challenge, see HEREEE put the plot of the paper where overfitting is huge). 1.5 An example Let’s combine all the new steps into a complete pipeline of machine learning in R. We can do that by finishing the tidyflow we’ve been developing so far. Let’s use the data age_inc which has the age of a person and their income. We want to predict their income based on their age. The rectangular data looks like this: The relationship between these two variables is non-linear, showing a variant of the Mincer equation (Mincer 1958) where income is a non-linear function of age: # age_inc was defined above, and it is reused here age_inc %&gt;% ggplot(aes(age, income)) + geom_point() + theme_linedraw() Since ml_flow is a series of steps, it allows you to remove any of them. Let’s remove the cross-validation step with drop_resample: ml_flow &lt;- ml_flow %&gt;% drop_resample() ml_flow ## ══ Tidyflow ════════════════════════════════════════════════════════════════════ ## Data: 500 rows x 2 columns ## Split: initial_split w/ default args ## Recipe/Formula: None ## Resample: None ## Grid: None ## Model: None Let’s begin running some models. The first model we’d like run is a simple regression income ~ age on the training data and plot the fitted values. # Run the model m1 &lt;- ml_flow %&gt;% plug_recipe(~ recipe(income ~ age, data = .)) %&gt;% # Add the formula plug_model(set_engine(linear_reg(), &quot;lm&quot;)) %&gt;% # Define the linear regression fit() # Fit model # Predict on the training data m1_res &lt;- m1 %&gt;% predict_training() m1_res ## # A tibble: 375 x 3 ## age income .pred ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 56.9 2574. 2650. ## 2 38.9 2091. 2298. ## 3 62.8 2328. 2767. ## 4 84.9 3548. 3200. ## 5 65.3 2440. 2815. ## 6 93.8 3866. 3374. ## 7 7.92 1511. 1692. ## 8 78.3 3134. 3070. ## 9 55.7 2777. 2628. ## 10 42.2 2918. 2362. ## # … with 365 more rows The result of predict_training is the training data from age_inc with one new column: the predicted values of the model. Let’s visualize the predictions: # Visualize the result m1_res %&gt;% ggplot(aes(age, income)) + geom_line(aes(y = .pred), color = &quot;red&quot;, size = 2) + geom_point() + scale_x_continuous(name = &quot;Age&quot;) + scale_y_continuous(name = &quot;Income&quot;, label = dollar_format(suffix = &quot;€&quot;, prefix = &quot;&quot;)) + theme_linedraw() It seems we’re underfitting the relationship. To measure the fit of the model, we’ll use the Root Mean Square Error (RMSE). Remember it? \\[ RMSE = \\sqrt{\\sum_{i = 1}^n{\\frac{(\\hat{y} - y)^2}{N}}} \\] Without going into too many details, it is the average difference between each dot from the plot from the value same value in the fitted line. The current \\(RMSE\\) of our model is 379.59. This means that on average our predictions are off by around 379.59 euros. The fitted line is underfitting the relationship because it cannot capture the non-linear trend in the data. How do we increase the fit? We could add non-linear terms to the model, for example \\(age^2\\), \\(age^3\\), …, \\(age^{10}\\). However, remember, by fitting very high non-linear terms to the data, we might get lower error from the model on the training data but that’s because the model is learning the training data so much that it starts to capture noise rather than the signal. This means that when we predict on the unseen testing data, our model would not know how to identify the signal in the data and have a higher \\(RMSE\\) error. How can we be sure we’re picking the best model specification? This is where cross-validation comes in! We can use the function vfold_cv to separate the training data into 10 cross-validation sets, where each one has a training and testing data. ## # 10-fold cross-validation ## # A tibble: 10 x 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [337/38]&gt; Fold01 ## 2 &lt;split [337/38]&gt; Fold02 ## 3 &lt;split [337/38]&gt; Fold03 ## 4 &lt;split [337/38]&gt; Fold04 ## 5 &lt;split [337/38]&gt; Fold05 ## 6 &lt;split [338/37]&gt; Fold06 ## 7 &lt;split [338/37]&gt; Fold07 ## 8 &lt;split [338/37]&gt; Fold08 ## 9 &lt;split [338/37]&gt; Fold09 ## 10 &lt;split [338/37]&gt; Fold10 Each of those split objects (there are 10) contains a training and testing set. This is the equivalent of the image we saw before: The next thing we have to do is train the same model on the training data of each of these cross-validated sets, use these trained models to predict on the 10 testing sets and record the error rate using our \\(RMSE\\) metric. But don’t worry, you don’t have to do that all of that manually, tidyflow can leverage many packages to do that for you: # Define the formula of your model and specify that the polynomial # value will be &#39;tuned&#39;. That is, we will try several values # instead of only one. rcp &lt;- ~ recipe(income ~ age, data = .) %&gt;% step_poly(age, degree = tune()) m2 &lt;- m1 %&gt;% # Add the cross-validation step plug_resample(vfold_cv) %&gt;% # Replace the initial recipe with the one with several polynomials replace_recipe(rcp) %&gt;% # Here we define the values we will try, from 2 to 10 plug_grid(expand.grid, degree = 2:10) %&gt;% # Fit the model fit() # Visualize the result m2 %&gt;% pull_tflow_fit_tuning() %&gt;% # Extract all models with different &#39;degree&#39; values autoplot() + theme_linedraw() Figure 1.2: Average evaluation metrics of predicting on the testing data through the 10 cross-validation sets Figure 1.2 shows the error rate for the \\(RMSE\\) and the \\(R^2\\). For the \\(RMSE\\) (left panel), the resulting error terms show that any polynomial above 2 has very similar error rates. However, there is a point in which adding \\(age^9\\) and \\(age^{10}\\) increases the error rate. This decrease in fit as complexity increases can also been see with the \\(R^2\\) (right panel), as it decreases with higher polynomials. This is a good example where a lot of flexibility (fitting the non-linear trend very well), increases accuracy on the training set but shows a lot variability on the testing set. The \\(RMSE\\) that we see in figure 1.2 is the average \\(RMSE\\) from predicting on the testing set using the model fitted on the training data in the 10 cross validated sets. Given that most of the polynomial terms have similar error terms, we usually would go for the simplest model, that is, the model with \\(age^3\\). We can run the model on the entire training data with 3 non-linear terms and check the fit: # Fit the final model with degrees = 3 res_m2 &lt;- complete_tflow(m2, best_params = data.frame(degree = 3)) res_m2 %&gt;% predict_training() %&gt;% ggplot(aes(age, income)) + geom_line(aes(y = .pred), color = &quot;red&quot;, size = 2) + geom_point() + scale_x_continuous(name = &quot;Age&quot;) + scale_y_continuous(name = &quot;Income&quot;, label = dollar_format(suffix = &quot;€&quot;, prefix = &quot;&quot;)) + theme_linedraw() The \\(RMSE\\) on the training data for the three polynomial model is 279.87. We need to compare that to our testing \\(RMSE\\). res_m2 %&gt;% predict_testing() %&gt;% ggplot(aes(age, income)) + geom_line(aes(y = .pred), color = &quot;red&quot;, size = 2) + geom_point() + scale_x_continuous(name = &quot;Age&quot;) + scale_y_continuous(name = &quot;Income&quot;, label = dollar_format(suffix = &quot;€&quot;, prefix = &quot;&quot;)) + theme_linedraw() training \\(RMSE\\) is 279.87 testing \\(RMSE\\) is 311.03 testing \\(RMSE\\) will almost always be higher, since we always overfit the data in some way through cross-validation. References "],
["regularization.html", "Chapter 2 Regularization 2.1 Ridge regularization 2.2 Lasso regularization 2.3 Elastic Net regularization 2.4 Exercises", " Chapter 2 Regularization Regularization is a common topic in machine learning and bayesian statistics. In this chapter, we will describe the three most common regularized linear models in the machine learning literature and introduce them in the context of the PISA data set. At the end of the document you’ll find exercises that will put your knowledge to the test. Most of the material is built upon Boehmke and Greenwell (2019) and James et al. (2013), which can be used as reference. 2.1 Ridge regularization Do no let others fool you into thinking that ridge regression is a fancy artificial intelligence algorithm. Are you familiar with linear regression? If you are, then ridge regression is just an adaptation of linear regression. The whole aim of linear regression, or Ordinary Least Squares (OLS), is to minimize the sum of the squared residuals. In other words, fit N number of regression lines to the data and keep only the one that has the lowest sum of squared residuals. In simple formula jargon, OLS tries to minimize this: \\[\\begin{equation} RSS = \\sum_{k = 1}^n(actual_i - predicted_i)^2 \\end{equation}\\] For each fitted regression line, you compare the predicted value (\\(predicted_i\\)) versus the actual value (\\(actual_i\\)), square it, and add it all up. Each fitted regression line then has an associated Residual Sum of Squares (RSS) and the linear model chooses the line with the lowest RSS. Note: Social scientists are familiar with the RSS and call it just by it’s name. However, be aware that in machine learning jargon, the RSS belongs to a general family called loss functions. Loss functions are metrics that evaluate the fit of your model and there are many around (such as AIC, BIC or R2). Ridge regression takes the previous RSS loss function and adds one term: \\[\\begin{equation} RSS + \\lambda \\sum_{k = 1}^n \\beta^2_j \\end{equation}\\] The term on the right is called a shrinkage penalty because it forces each coefficient \\(\\beta_j\\) closer to zero by squaring it. The shrinkage part is clearer once you think of this term as forcing each coefficient to be as small as possible without compromising the Residual Sum of Squares (RSS). In other words, we want the smallest coefficients that don’t affect the fit of the line (RSS). An intuitive example is to think of RSS and \\(\\sum_{k = 1}^n \\beta^2_j\\) as two separate things. RSS estimates how the model fits the data and \\(\\sum_{k = 1}^n \\beta^2_j\\) limits how much you overfit the data. Finally, the \\(\\lambda\\) between these two terms (called lambda) can be interpreted as a “weight”. The higher the lambda, the higher the weight that will be given to the shrinkage term of the equation. If \\(\\lambda\\) is 0, then multiplying 0 by \\(\\sum_{k = 1}^n \\beta^2_j\\) will always return zero, forcing our previous equation to simply be reduced to the single term \\(RSS\\). Why is there a need to “limit” how well the model fits the data? Because we, social scientists and data scientists, very commonly overfit the data. The plot below shows a simulation from Simon Jackson where we can see that when tested on a training set, OLS and Ridge tend to overfit the data. However, when tested on the test data, ridge regression has lower out of sample error as the \\(R2\\) is higher for models with different observations. The strength of the ridge regression comes from the fact that it compromises fitting the training data really well for improved generalization. In other words, we increase bias (because we force the coefficients to be smaller) for lower variance (making our predictions more robust). In other words, the whole gist behind ridge regression is penalizing very large coefficients for better generalization on new data. Having that intuition in mind, there is one important thing to keep in mind: the predictors of the ridge regression need to be standardized. Why is this the case? Because due to the scale of a predictor, its coefficient can be more penalized than other predictors. Suppose that you have the income of a particular person (measured in thousands per months) and time spent with their families (measured in seconds) and you’re trying to predict happiness. A one unit increase in salary could be penalized much more than a one unit increase in time spent with their families just because a one unit increase in salary can be much bigger due to it’s metric. In R, you can fit a ridge regression using tidymodels and tidyflow. Let’s load the packages that we will work with and read the data: library(tidymodels) library(tidyflow) data_link &lt;- &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot; pisa &lt;- read.csv(data_link) We will construct our tidyflow step by step. We begin with the data and then separate the training and test data. All of our modelling will be performed on the training data and the test data is saved for later (the test data must be completely ignored until you have your final tuned model). The second step is specifying the variables in the model and scaling all of them, as I have explained, we want to normalize all variables such that no variable gets more penalized than other due to their metric. # Specify all variables and scale rcp &lt;- # Define dependent (math_score) and independent variables ~ recipe(math_score ~ MISCED + FISCED + HISEI + REPEAT + IMMIG + DURECEC + BSMJ, data = .) %&gt;% # Scale all predictors (already knows it&#39;s the independent variables) step_scale(all_predictors()) tflow &lt;- tidyflow(seed = 231141) %&gt;% plug_data(pisa) %&gt;% plug_split(initial_split, prop = .7) %&gt;% # Add the recipe with all variables and scale plug_recipe(rcp) tflow ## ══ Tidyflow ════════════════════════════════════════════════════════════════════ ## Data: 4.84K rows x 501 columns ## Split: initial_split w/ prop = ~0.7 ## Recipe: available ## Resample: None ## Grid: None ## Model: None The argument prop controls the proportion of the sample that will be in the training data. Here we specify it to be .7, 70% of the data. The third step is specifying the tuning parameters. The ridge regression has a parameter called penalty which needs to be set by us. penalty is the “weight” term in the ridge equation, which controls how much weight do we want to give to the “shrinkage penalty” (this is the \\(\\lambda\\) from the equation). If this penalty is set to 0, it means we attach no weight to the penalty term and we will get the same result over OLS. Let’s try that: ############################# Ridge regression ################################ ############################################################################### regularized_reg &lt;- set_engine( # mixture specifies the type of penalized regression: 0 is ridge regression linear_reg(penalty = 0, mixture = 0), &quot;glmnet&quot; ) model1 &lt;- tflow %&gt;% plug_model(regularized_reg) %&gt;% fit() # Get ridge coefficients mod &lt;- model1 %&gt;% pull_tflow_fit() %&gt;% .[[&quot;fit&quot;]] ridge_coef &lt;- predict(mod, s = 0, type = &quot;coefficients&quot;) ############################# Linear model #################################### ############################################################################### model2 &lt;- tflow %&gt;% plug_model(set_engine(linear_reg(), &quot;lm&quot;)) %&gt;% fit() lm_coef &lt;- model2 %&gt;% pull_tflow_fit() %&gt;% .[[&quot;fit&quot;]] %&gt;% coef() ############################# Comparing model ################################# ############################################################################### comparison &lt;- data.frame(coefs = names(lm_coef), `Linear coefficients` = unname(round(lm_coef, 2)), `Ridge coefficients` = round(as.vector(ridge_coef), 2)) knitr::kable(comparison) coefs Linear.coefficients Ridge.coefficients (Intercept) 329.37 331.55 MISCED 3.88 4.17 FISCED 11.93 11.61 HISEI 17.85 17.36 REPEAT -22.03 -21.41 IMMIG 6.66 6.41 DURECEC -0.33 -0.27 BSMJ 9.10 8.96 Coming from a social science background, it might seem counterintuitive that the researcher has to specify tuning parameters for the model. In traditional social science statistics, models usually estimate similar values internally and the user doesn’t have to think about them. However, there are strategies already implemented to explore the combination of many possible values. With our previous example, we have to add tune() to the penalty argument and add a grid for the model to search for the best one: # Here we add the cross-validation and grid tflow &lt;- tflow %&gt;% # Cross-validation plug_resample(vfold_cv, v = 5) %&gt;% # Grid plug_grid(grid_regular) regularized_reg &lt;- update(regularized_reg, penalty = tune()) res &lt;- tflow %&gt;% # Update the model to specify that `penalty` will be tuned plug_model(regularized_reg) %&gt;% fit() final_ridge &lt;- complete_tflow(res, metric = &quot;rmse&quot;) final_ridge %&gt;% pull_tflow_fit() %&gt;% .[[&quot;fit&quot;]] %&gt;% plot(xvar = &quot;lambda&quot;, label = TRUE) Here we can see how our coefficients are affected by increasing the weight of the penalty parameter. Each of those lines are the coefficients for the variables. The x axis contains the penalty values and we can see how as the penalty increases, the size of the coefficients is shrinking to be close to zero. By around the log of penalty around 8 nearly all coefficients are shrinked very close to zero. This plot is just an exercise to understand how the ridge regression works. In other words, we can figure out the best lambda automatically: best_tune &lt;- res %&gt;% pull_tflow_fit_tuning() %&gt;% select_best(metric = &quot;rmse&quot;) best_tune ## # A tibble: 1 x 1 ## penalty ## &lt;dbl&gt; ## 1 0.0000000001 However, there’s no need to calculate this, as complete_tflow figures it out for you (as you can see in the code chunk above, complete_tflow extracts this automatically and fits the best model). We can calculate the \\(RMSE\\) of the training data from the best model and compare it to the predictions on the testing data: train_rmse_ridge &lt;- final_ridge %&gt;% predict_training() %&gt;% rmse(math_score, .pred) holdout_ridge &lt;- final_ridge %&gt;% predict_testing() %&gt;% rmse(math_score, .pred) train_rmse_ridge$type &lt;- &quot;training&quot; holdout_ridge$type &lt;- &quot;testing&quot; ridge &lt;- as.data.frame(rbind(train_rmse_ridge, holdout_ridge)) ridge$model &lt;- &quot;ridge&quot; ridge ## .metric .estimator .estimate type model ## 1 rmse standard 76.64458 training ridge ## 2 rmse standard 78.21517 testing ridge The testing error (RMSE) is higher than the training error, as expected, as the training set nearly always memorizes the data better for the training. 2.2 Lasso regularization The Lasso regularization is very similar to the ridge regularization where only one thing changes: the penalty term. Instead of squaring the coefficients in the penalty term, the lasso regularization takes the absolute value of the coefficient. \\[\\begin{equation} RSS + \\lambda \\sum_{k = 1}^n |\\beta_j| \\end{equation}\\] Althought it might not be self-evident from this, the lasso reguralization has an important distinction: it can force a coefficient to be exactly zero. This means that lasso does a selection of variables which have big coefficients while not compromising the RSS of the model. The problem with ridge regression is that as the number of variables increases, the training error will almost always improve but the test error will not. For example, if we define the same model from above using a lasso, you’ll see that it forces coefficients to be exactly zero if they don’t add anything relative to the RSS of the model. This means that variables which do not add anything to the model will be excluded unless they add explanatory power that compensates the size of their coefficient. Here’s the same lasso example: regularized_reg &lt;- update(regularized_reg, mixture = 1) res &lt;- tflow %&gt;% plug_model(regularized_reg) %&gt;% fit() final_lasso &lt;- complete_tflow(res, metric = &quot;rmse&quot;) final_lasso %&gt;% pull_tflow_fit() %&gt;% .[[&quot;fit&quot;]] %&gt;% plot(xvar = &quot;lambda&quot;, label = TRUE) In contrast to the ridge regression, where coefficients are forced to be close to zero, the lasso penalty actually forces some coefficients to be zero. This property means that the lasso makes a selection of the variables with the higher coefficients and eliminates those which do not have a strong relationship. Lasso is usually better at model interpretation because it removes redundant variables while ridge can be useful if you want to keep a number of variables in the model, despite them being weak predictors (as controls, for example). To check the final model and it’s error, we can recicle the code from above and adapt it to the lasso: train_rmse_lasso &lt;- final_lasso %&gt;% predict_training() %&gt;% rmse(math_score, .pred) holdout_lasso &lt;- final_lasso %&gt;% predict_testing() %&gt;% rmse(math_score, .pred) train_rmse_lasso$type &lt;- &quot;training&quot; holdout_lasso$type &lt;- &quot;testing&quot; lasso &lt;- as.data.frame(rbind(train_rmse_lasso, holdout_lasso)) lasso$model &lt;- &quot;lasso&quot; lasso ## .metric .estimator .estimate type model ## 1 rmse standard 76.63928 training lasso ## 2 rmse standard 78.23457 testing lasso So far, we can check which model is performing better: model_comparison &lt;- rbind(ridge, lasso) model_comparison ## .metric .estimator .estimate type model ## 1 rmse standard 76.64458 training ridge ## 2 rmse standard 78.21517 testing ridge ## 3 rmse standard 76.63928 training lasso ## 4 rmse standard 78.23457 testing lasso Currently the ridge regression has a very minor advantaged over the lasso yet the difference is probably within the margin of error. Depending on your aim, you might want to choose either of the models. For example, if our models contained a lot of variables, lasso might be more interpretable as it reduces the number of variables. However, if you have reasons to believe that keeping all variables in the model is important, then ridge provides an advantage. 2.3 Elastic Net regularization If you’re aware of ridge and lasso, then elastic net regularization is a logical step. Elastic Net (the name sounds fancy, but it is also an adaptation of OLS) combines both penalties to form one single equation. Here we define our ridge penalty: \\[ridge = \\lambda \\sum_{k = 1}^n \\beta_j^2\\] And here we define our lasso penalty: \\[lasso = \\lambda \\sum_{k = 1}^n |\\beta_j|\\] Elastic net regularization is the addition of these two penalties in comparison to the RSS: \\[RSS + lasso + ridge\\] I think the best explanation for elastic net reguarlization comes from Boehmke and Greenwell (2019): Although lasso models perform feature selection, when two strongly correlated features are pushed towards zero, one may be pushed fully to zero while the other remains in the model. Furthermore, the process of one being in and one being out is not very systematic. In contrast, the ridge regression penalty is a little more effective in systematically handling correlated features together. Consequently, the advantage of the elastic net penalty is that it enables effective regularization via the ridge penalty with the feature selection characteristics of the lasso penalty. Essentially, you now have two tuning parameters. In the grid of values, instead of specifying a mixture of 0 (ridge) or 1 (lasso), tidyflow will slide through several values of mixture ranging from 0 to 1 and compare that to several values of lambda. This is formally called a grid search. We can recycle the same code from above: regularized_reg &lt;- update(regularized_reg, mixture = tune()) res &lt;- tflow %&gt;% plug_model(regularized_reg) %&gt;% fit() final_elnet &lt;- complete_tflow(res, metric = &quot;rmse&quot;) train_rmse_elnet &lt;- final_elnet %&gt;% predict_training() %&gt;% rmse(math_score, .pred) holdout_elnet &lt;- final_elnet %&gt;% predict_testing() %&gt;% rmse(math_score, .pred) train_rmse_elnet$type &lt;- &quot;training&quot; holdout_elnet$type &lt;- &quot;testing&quot; elnet &lt;- as.data.frame(rbind(train_rmse_elnet, holdout_elnet)) elnet$model &lt;- &quot;elnet&quot; elnet ## .metric .estimator .estimate type model ## 1 rmse standard 76.63928 training elnet ## 2 rmse standard 78.23457 testing elnet The RMSE of the elastic net is somewhat lower than then ridge and lasso but also probably within the margin of error. Let’s compare it visually: model_comparison &lt;- rbind(model_comparison, elnet) model_comparison %&gt;% ggplot(aes(model, .estimate, color = type, group = type)) + geom_point(position = &quot;dodge&quot;) + geom_line() + scale_y_continuous(name = &quot;RMSE&quot;) + scale_x_discrete(name = &quot;Models&quot;) + theme_minimal() 2.4 Exercises The Fragile Families Challenge is a study that aimed to predict a series of indicators of children at age 15 only using data from ages 0 to 9. With this challenge, the principal investigators wanted to test whether skills such as cognitive and non-cognitive abilities were correctly predicted. With that idea in mind, they were interested in following up children that beat the ‘predictions’: those children that exceeded the model’s prediction, for example given their initial conditions. Using a similarly constructed non-cognitive proxy, I’ve created a non-cognitive index using the PISA 2018 for the United States which is the average of the questions: ST182Q03HA - I find satisfaction in working as hard as I can. ST182Q04HA - Once I start a task, I persist until it is finished. ST182Q05HA - Part of the enjoyment I get from doing things is when I improve on my past performance. ST182Q06HA - If I am not good at something, I would rather keep struggling to master it than move on to something I may […] The scale of the index goes from 1 to 4, where in 4 the student strongly agrees and 1 is they completely disagree. In other words, this index shows that the higher the value, the higher the non cognitive skills. You can check out the complete PISA codebook here. In these series of exercises you will have to try different models that predict this index of non-cognitive skills, perform a grid search for the three models and compare the predictions of the three models. First, read in the data with: data_link &lt;- &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot; pisa &lt;- read.csv(data_link) 1. Create a tidyflow with a split Begin with the data pisa To plug a split, use initial_split Remember to set the seed to 2341 so that everyone can compare their results. &gt; Answer tflow &lt;- pisa %&gt;% tidyflow(seed = 2341) %&gt;% plug_split(initial_split) tflow 2. Run a ridge regression with non-cognitive as the dependent variable Plug in a formula (hint, look at ?plug_formula) and use as many variables as you want (you can reuse the previous variables from the examples or pick all of them). A formula of the like noncogn ~ . will regress noncogn on all variables. Plug in the ridge regression with penalty set to 0.001 (hint: remember to set mixture to the value corresponding to the ridge regression) Fit the ridge model (with fit) Predict on the training data with predict_training and explore the \\(R^2\\) (rsq) and \\(RMSE\\) (rmse). &gt; Answer ridge_mod &lt;- set_engine(linear_reg(penalty = 0.001, mixture = 0), &quot;glmnet&quot;) tflow &lt;- tflow %&gt;% plug_formula(noncogn ~ .) %&gt;% plug_model(ridge_mod) m1 &lt;- fit(tflow) m1_rsq &lt;- predict_training(m1) %&gt;% rsq(noncogn, .pred) m1_rmse &lt;- predict_training(m1) %&gt;% rmse(noncogn, .pred) 3. Add a recipe to sacle all of the predictors and rerun the previous model Drop the formula from the tidyflow with drop_formula Add a recipe with the same formula you had, but including the step_scale for all predictors Rerun the model and extract the \\(R^2\\) and \\(RMSE\\) How does the \\(R^2\\) and \\(RMSE\\) change? Was there an impact in change? &gt; Answer rcp &lt;- ~ recipe(noncogn ~ ., data = .) %&gt;% # We need to remove these two variables because they are # character variables. We can&#39;t scale and center a # character variable step_rm(STRATUM, VER_DAT) %&gt;% step_scale(all_predictors()) tflow &lt;- tflow %&gt;% drop_formula() %&gt;% plug_recipe(rcp) m2 &lt;- fit(tflow) m2_rsq &lt;- predict_training(m2) %&gt;% rsq(noncogn, .pred) m2_rmse &lt;- predict_training(m2) %&gt;% rmse(noncogn, .pred) 4. Adapt the previous model to do a grid search of penalty values Add a cross-validation resample (vfold_cv) Add a tuning grid (grid_regular) and specify levels = 10. This will create a tuning grid of 10 values Update the penalty parameter to be tuned Run the grid search (fit) Extract the tuning grid (pull_tflow_fit_tuning) and visualize autoplot Is there a pattern with the improvement/decrease of the metrics of fit with respect to the penalty? &gt; Answer ridge_mod &lt;- update(ridge_mod, penalty = tune()) tflow &lt;- tflow %&gt;% replace_model(ridge_mod) %&gt;% plug_resample(vfold_cv) %&gt;% plug_grid(grid_regular, levels = 10) m3 &lt;- fit(tflow) m3 %&gt;% pull_tflow_fit_tuning() %&gt;% autoplot() 5. Run a lasso regression with the same specification as above Update the model to have a mixture of 1 (this is specifying that we want a lasso) Run the grid search (fit) Extract the tuning grid (pull_tflow_fit_tuning) and visualize autoplot Which model is performing better? Ridge or Lasso? Can you comment on the pattern of the penalty between ridge and lasso? &gt; Answer lasso_mod &lt;- update(ridge_mod, mixture = 1) m4 &lt;- tflow %&gt;% replace_model(lasso_mod) %&gt;% fit() m4 %&gt;% pull_tflow_fit_tuning() %&gt;% autoplot() 6. Run an elastic net regression on non cognitive skills Update the model to have a tuned mixture Replace the model in the tidyflow with the elastic net model Run the grid search (fit) Extract the tuning grid (pull_tflow_fit_tuning) and visualize autoplot &gt; Answer elnet_mod &lt;- update(lasso_mod, mixture = tune()) m5 &lt;- tflow %&gt;% replace_model(elnet_mod) %&gt;% fit() m5 %&gt;% pull_tflow_fit_tuning() %&gt;% autoplot() # Additional plot with standard error m5 %&gt;% pull_tflow_fit_tuning() %&gt;% collect_metrics() %&gt;% pivot_longer(penalty:mixture) %&gt;% mutate(low = mean - (std_err * 2), high = mean + (std_err * 2)) %&gt;% ggplot(aes(value, mean)) + geom_point() + geom_errorbar(aes(ymin = low, ymax = high)) + facet_grid(.metric ~ name) 7. Compare the three models Finalize the three models (the ridge model, the lasso model and the elastic net model) with complete_tflow. Remember to set the metric! Use the three finalized models (the ones that were produced by complete_tflow) to predict_training and predict_testing on each one Calculate the rmse of the three models on both training and testing Visualize the three models and their error for training/testing Comment on which models is better in out-of-sample fit Is it better to keep the most accurate model or a model that includes relevant confounders (even if they’re relationship is somewhat weak)? &gt; Answer # Since we will be repeating the same process many times # let&#39;s write a function to predict on the training/testing # and combine them. This function will accept a single # model and produce a data frame with the RMSE error for # training and testing. This way, we can reuse the code # without having to copy everything many times calculate_err &lt;- function(final_model, type_model = NULL) { final_model &lt;- complete_tflow(final_model, metric = &quot;rmse&quot;) err_train &lt;- final_model %&gt;% predict_training() %&gt;% rmse(noncogn, .pred) err_test &lt;- final_model %&gt;% predict_testing() %&gt;% rmse(noncogn, .pred) err_train$type &lt;- &quot;train&quot; err_test$type &lt;- &quot;test&quot; res &lt;- as.data.frame(rbind(err_train, err_test)) res$model &lt;- type_model res } final_res &lt;- rbind( calculate_err(m3, &quot;ridge&quot;), calculate_err(m4, &quot;lasso&quot;), calculate_err(m5, &quot;elnet&quot;) ) final_res %&gt;% ggplot(aes(model, .estimate, color = type)) + geom_point() + theme_minimal() ## BONUS # Fit a linear regression and compare the four models References "],
["tree-based-methods.html", "Chapter 3 Tree-based methods 3.1 Decision trees 3.2 Bagging 3.3 Random Forests 3.4 Boosting 3.5 Exercises", " Chapter 3 Tree-based methods In this chapter we will touch upon the most popular tree-based methods used in machine learning. Haven’t heard of the term “tree-based methods”? Do not panic. The idea behind tree-based methods is very simple and we’ll explain how they work step by step through the basics. Most of the material on this chapter was built upon Boehmke and Greenwell (2019) and James et al. (2013). Before we begin, let’s load tidyflow and tidymodels and read the data that we’ll be using. library(tidymodels) library(tidyflow) library(rpart.plot) library(vip) library(baguette) data_link &lt;- &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot; pisa &lt;- read.csv(data_link) 3.1 Decision trees Decision trees are simple models. In fact, they are even simpler than linear models. They require little statistical background and are in fact among the simplest models to communicate to a general audience. In particular, the visualizations used for decision trees are very powerful in conveying information and can even serve as an exploratory avenue for social research. Throughout this chapter, we’ll be using the PISA data set from the regularization chapter. On this example we’ll be focusing on predicting the math_score of students in the United States, based on the socio economic status of the parents (named HISEI in the data; the higher the HISEI variable, the higher the socio economic status), the father’s education (named FISCED in the data; coded as several categories from 0 to 6 where 6 is high education) and whether the child repeated a grade (named REPEAT in the data). REPEAT is a dummy variable where 1 means the child repeated a grade and 0 no repetition. Decision trees, as their name conveys, are tree-like diagrams. They work by defining yes-or-no rules based on the data and assign the most common value for each respondent within their final branch. The best way to learn about decision trees is by looking at one. Let’s do that: In this example the top-most box which says HISEI &lt; 56 is the root node. This is the most important variable that predicts math_score. Inside the blue box you can see two numbers: \\(100\\%\\) which means that the entire sample is present in this node and the number 474, the average test score for mathematics for the entire sample: On both sides of the root node (HISEI &lt; 56) there is a yes and a no. Decision trees work by partitioning variables into yes-or-no branches. The yes branch satisfies the name of root (HISEI &lt; 56) and always branches out to the left: In contrast, the no branch always branches out to the right: The criteria for separating into yes-or-no branches is that respondents must be very similar within branches and very different between branches (later in this chapter I will explain in detail which criteria is used and how). The decision tree figures out that respondents that have an HISEI below \\(56\\) and above \\(56\\) are the most different with respect to the mathematics score. The left branch (where there is a yes in the root node) are those which have a HISEI below 56 and the right branch (where there is a no) are those which have a HISEI above \\(56\\). Let’s call these two groups the low and high SES respectively. If we look at the two boxes that come down from these branches, the low SES branch has an average math score of \\(446\\) while the high SES branch has an average test score of \\(501\\): For the sake of simplicity, let’s focus now on the branch of the low SES group (the left branch). The second node coming out of the low SES branch contains 50% of the sample and an average math score of \\(446\\). This is the node with the rule REPEAT &gt;= 0.5: This ‘intermediate’ node is called internal node. For calculating this internal node, the decision tree algorithm limits the entire data set to only those which have low SES (literally, the decision tree does something like pisa[pisa$HISEI &lt; 56, ]) and asks the same question that it did in the root node: of all the variables in the model which one separates two branches such that respondents are very similar within the branch but very different between the branches with respect to math_score? For those with low SES background, this variable is whether the child repeated a grade or not. In particular, those coming from low SES background which repeated a grade, had an average math score of \\(387\\) whereas those who didn’t have an average math score of \\(456\\): These two nodes at the bottom are called leaf nodes because they are like the ‘leafs of the tree’. Leaf nodes are of particular importance because they are the ones that dictate what the final value of math_score will be. Any new data that is predicted with this model will always give an average math_score of \\(456\\) for those of low SES background who didn’t repeat a grade: Similarly, any respondent from high SES background, with a highly educated father who didn’t repeat a grade, will get assigned a math_score of \\(527\\): That is it. That is a decision tree in it’s simplest form. It contains a root node and several internal and leaf nodes and it can be interpreted just as we just did. The right branch of the tree can be summarized with the same interpretation. For example, for high SES respondents, father’s education (FISCED) is more important than REPEAT to separate between math scores: This is the case because it comes first in the tree. Substantially, this might be due to the fact that there is higher variation in education credentials for parents of high SES background than for those of low SES background. We can see that those with the highest father’s education (FISCED above \\(5.5\\)), the average math score is \\(524\\) whereas those with father’s education below \\(5.5\\) have a math score of \\(478\\). Did you notice that we haven’t interpreted any coefficients? That’s right. Decision trees have no coefficients and many other machine learning algorithms also don’t produce coefficients. Although for the case of decision trees this is because the model produces information in another way (through the visualization of trees), lack of coefficients is common in machine learning models because they are too complex to generate coefficients for single predictors. These models are non-linear, non-parametric in nature, producing very complex relationships that are difficult to summarize as coefficients. Instead, they produce predictions. We’ll be delving into this topic in future sections in detail. These examples show that decision trees are a great tool for exploratory analysis and I strongly believe they have an inmense potential for exploring interactions in social science research. In case you didn’t notice it, we literally just interpreted an interaction term that social scientists would routinely use in linear models. Without having to worry about statistical significance or plotting marginal effects, social scientists can use decision trees as an exploratory medium to understand interactions in an intuitive way. You might be asking yourself, how do we fit these models and visualize them? tidyflow and tidymodels have got you covered. For example, for fitting the model from above, we can begin our tidyflow, add a split, a formula and define the decision tree: # Define the decision tree and tell it the the dependent # variable is continuous (&#39;mode&#39; = &#39;regression&#39;) mod1 &lt;- set_engine(decision_tree(mode = &quot;regression&quot;), &quot;rpart&quot;) tflow &lt;- # Plug the data pisa %&gt;% # Begin the tidyflow tidyflow(seed = 23151) %&gt;% # Separate the data into training/testing plug_split(initial_split) %&gt;% # Plug the formula plug_formula(math_score ~ FISCED + HISEI + REPEAT) %&gt;% # Plug the model plug_model(mod1) vanilla_fit &lt;- fit(tflow) tree &lt;- pull_tflow_fit(vanilla_fit)$fit rpart.plot(tree) All plug_* functions serve to build your machine learning workflow and the model decision_tree serves to define the decision tree and all of the arguments. rpart.plot on the other hand, is a function used specifically for plotting decision trees (that is why we loaded the package rpart.plot at the beginning). No need to delve much into this function. It just works if you pass it a decision tree model: that is why pull the model fit before calling it. I’ve told all the good things about decision trees but they have important disadvantages. There are two that we’ll discuss in this chapter. The first one is that decision trees tend to overfit a lot. Just for the sake of exemplifying this, let’s switch to another example. Let’s say we’re trying to understand which variables are related to whether teachers set goals in the classroom. Substantially, this example might not make a lot of sense, but but let’s follow along just to show how much trees can overfit the data. This variable is named ST102Q01TA. Let’s plug it into our tidyflow and visualize the tree: ## ST100Q01TA ## ST102Q01TA ## IC009Q07NA ## ST011Q03TA ## ST011Q05TA ## ST011Q10TA # We can recicle the entire `tflow` from above and just # replace the formula: tflow &lt;- tflow %&gt;% replace_formula(ST102Q01TA ~ .) fit_complex &lt;- fit(tflow) tree &lt;- pull_tflow_fit(fit_complex)$fit rpart.plot(tree) The tree is quite big compared to our previous example and makes the interpretation more difficult. However, equally important, some leaf nodes are very small. Decision trees can capture a lot of noise and mimic the data very closely. \\(6\\) leaf nodes have less than \\(3\\%\\) of the sample. These are leaf nodes with very weak statistical power: What would happen if a tiny \\(1\\%\\) of those leaf nodes respondend slightly different? It is possible we get a complete different tree. Decision trees are not well known for being robust. In fact, it is one of its main weaknesses. However, decision trees have an argument called min_n that force the tree to discard any node that has a number of observations below your specified minimum. Let’s run the model above and set the minimum number of observation per node to be \\(200\\): dectree &lt;- update(mod1, min_n = 200) tflow &lt;- tflow %&gt;% replace_model(dectree) fit_complex &lt;- fit(tflow) tree &lt;- pull_tflow_fit(fit_complex)$fit rpart.plot(tree) The tree was reduced considerably now. There are fewer leaf nodes and all nodes have a greater sample size than before. You might be wondering: what should the minimum sample size be? There is no easy answer for this. The rule of thumb should be relative to your data and research question. In particular, the identification of small nodes should be analyzed with care. Perhaps there is a group of outliers that consitute a node and it’s not a problem of statistical noise. By increasing the minimum sample size for each node you would be destroying that statistical finding. For example, suppose we are studying welfare social expenditure as the dependent variable and then we had other independent variables, among which are country names. Scandinavian countries might group pretty well into a solitary node because they are super powers in welfare spending (these are Denmark, Norway, Sweden and Finland). If we increased the minimum sample size to \\(10\\), we might group them with Germany and France, which are completely different in substantive terms. The best rule of thumb I can recommend is no other than to study your problem at hand with great care and make decisions accordingly. It might make sense to increase the sample or it might not depending on the research question, the sample size, whether you’re exploring the data or whether you’re interested in predicting on new data. Despite min_n helping to make the tree more robust, there are still several nodes with low sample sizes. Another way to approach this problem is through the depth of the tree. As can be seen from the previous plot, decision trees can create leaf nodes which are very small. In other more complicated scenarios, your tree might get huge. Yes, huge: More often that not, these huge trees are just overfitting the data. They are creating very small nodes that capture noise from the data and when you’re predicting on new data, they perform terribly bad. As well as the min_n argument, decision trees have another argument called tree_depth. This argument forces the tree to stop growing if it passes the maximum depth of the tree as measured in nodes. Let’s run our previous example with only a depth of three nodes: dectree &lt;- update(mod1, min_n = 200, tree_depth = 3) tflow &lt;- tflow %&gt;% replace_model(dectree) fit_complex &lt;- fit(tflow) tree &lt;- pull_tflow_fit(fit_complex)$fit rpart.plot(tree) The tree was reduced considerably now in combination with the minimun number of respondents within each node. In fact, there is only one node that has a sample size lower than \\(3\\%\\). The min_n and tree_depth can help you reduce the overfitting of your tree, but don’t think these are easy fixes. Decision trees are simply to easy to overfit the data and as we’ll see, there are more advanced tree methods that can help to fix this. Note that we’ve been interpreting decision trees in a ‘subjective’ fashion. That is, we’ve been cutting the nodes of the trees from subjective criteria that makes sense to our research problem. This is how we social scientists would analyze the data. The tree should model our theoretical problem and make substantive sense. However, for machine learning, we have other criteria: how well it predicts. Let’s check how our model predicts at this point: fit_complex %&gt;% predict_training() %&gt;% rmse(ST102Q01TA, .pred) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 0.514 Our predictions for each set goals is off by around \\(.5\\) in a scale of \\(1\\) through \\(4\\). This is not terribly bad. For example, it means that for every child that answered a \\(2\\), on average, we have an error of around \\(.5\\). This means that any prediction for a single number runs the risk of being wrongly predicting either the number from above or below (a child with a \\(2\\) might get a wrong prediction of \\(3\\) or a \\(1\\) but hardly a \\(4\\)). To improve prediction, we can allow tidyflow to search for the best combination of min_n and tree_depth that maximizes prediction. Let’s perform a grid search for these two tuning values. However, let’s set the range of tuning values ourselves: tune_mod &lt;- update(dectree, min_n = tune(), tree_depth = tune()) tflow &lt;- tflow %&gt;% plug_resample(vfold_cv, v = 5) %&gt;% plug_grid( expand.grid, tree_depth = c(1, 3, 9), min_n = c(50, 100) ) %&gt;% replace_model(tune_mod) fit_tuned &lt;- fit(tflow) fit_tuned %&gt;% pull_tflow_fit_tuning() %&gt;% show_best(metric = &quot;rmse&quot;) ## # A tibble: 5 x 7 ## tree_depth min_n .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 9 50 rmse standard 0.459 5 0.0126 ## 2 9 100 rmse standard 0.459 5 0.0126 ## 3 3 50 rmse standard 0.518 5 0.0116 ## 4 3 100 rmse standard 0.518 5 0.0116 ## 5 1 50 rmse standard 0.649 5 0.0102 It seems that our predictions on the training data were slightly overfitting the data, as the best error from the cross-validation search is centered around 0.459 with a standard error of 0.01. Let’s explore whether the error changes between the minimum sample size and the tree depth: tree_depth_lvl &lt;- paste0(&quot;Tree depth: &quot;, c(1, 3, 9)) fit_tuned %&gt;% pull_tflow_fit_tuning() %&gt;% collect_metrics() %&gt;% mutate(ci_low = mean - (1.96 * std_err), ci_high = mean + (1.96 * std_err), tree_depth = factor(paste0(&quot;Tree depth: &quot;, tree_depth), levels = tree_depth_lvl), min_n = factor(min_n, levels = c(&quot;50&quot;, &quot;100&quot;))) %&gt;% filter(.metric == &quot;rmse&quot;) %&gt;% ggplot(aes(min_n, mean)) + geom_point() + geom_errorbar(aes(ymin = ci_low, ymax = ci_high), width = .1) + scale_x_discrete(&quot;Minimum sample size per node&quot;) + scale_y_continuous(&quot;Average RMSE&quot;) + facet_wrap(~ tree_depth, nrow = 1) + theme_minimal() On the x axis we have the minimum sample size per node (these are the values for min_n) and on the y axis we have the error of the model through cross-validation (the \\(RMSE\\)). The lower each points is on the plot, the better, since it means that the error is lower. Let’s begin with the first plot on the left. The points represent the error of the model with different sample sizes for the nodes with a fixed tree depth of \\(1\\). For a tree depth of \\(1\\), the error of the model is around .65. However, as the number of trees increases (the additional plots to the right), the error comes down to nearly .47 when there is a tree_depth of 9. It seems that the simplest model with the lowest \\(RMSE\\) has a tree_depth of 9 and a minimum sample size of 50. We calculated this ourselves for this example, but complete_tflow can calculate this for you: final_model &lt;- fit_tuned %&gt;% complete_tflow(metric = &quot;rmse&quot;, tree_depth, method = &quot;select_by_one_std_err&quot;) train_err &lt;- final_model %&gt;% predict_training() %&gt;% rmse(ST102Q01TA, .pred) test_err &lt;- final_model %&gt;% predict_testing() %&gt;% rmse(ST102Q01TA, .pred) c(&quot;testing error&quot; = test_err$.estimate, &quot;training error&quot; = train_err$.estimate) ## testing error training error ## 0.4644939 0.4512248 Our testing error and our training error have a difference of only \\(0.01\\), not bad. The cross-validation tuning seemed to have helped avoid a great deal of overfitting. Before we go through the next section, I want to briefly mention an alternative to tree_depth and min_n. A technique called ‘tree pruning’ is also very common for modeling decision trees. It first grows a very large and complex tree and then starts pruning the leafs. This technique is also very useful but due to the lack of time, we won’t cover this in the course. You can check out the material on this technique from the resources outlined in the first paragraph of this section. 3.1.1 Advanced: how do trees choose where to split? Throughout most of the chapter we’ve seen that trees find optimal ‘splits’ that make the respondents very different between the splits and very similar within them. But how do decision trees make these splits? Let’s work out a simple example using the HISEI variable from the first model in this section. HISEI is an index for the socio-economic status of families. It’s continuous and has a distribution like this: As we saw in the first tree of this section, HISEI is the root node. To decide on the root node, the decision tree algorithm chooses a random location in the distribution of HISEI and draws a split: The two sides have an average math_score which serves as the baseline for how different these two groups are. At this point, the algorithm does something very simple: for each split, it calculates the Residual Sum of Squares (RSS). This is just the sum of the math_score of each respondent (\\(math_i\\)) minus the average math_score (\\(\\hat{math}\\)) for that split squared. In other words, it applies the \\(RSS\\) for each split: \\[\\begin{equation} RSS = \\sum_{k = 1}^n(math_i - \\hat{math})^2 \\end{equation}\\] Each side of the split then has a corresponding \\(RSS\\): After that, it calculates the total \\(RSS\\) of the split by adding the two \\(RSS\\): So far we should have a single random split with an associated \\(RSS\\) for \\(HISEI\\). The decision tree algorithm is called recursive binary splitting because it is recursive: it repeats itself again many times. It repeats the strategy of \\(Split\\) -&gt; \\(RSS_{split}\\) -&gt; \\(RSS_{total}\\) many times such that we get a distribution of splits and \\(RSS\\) for \\(HISEI\\): This produces a distribution of random splits with an associated metric of fit (\\(RSS\\)) for \\(HISEI\\). Recursive binary splitting applies this same logic to every single variable in the model such that you have a distribution of splits for every single variable: ## # A tibble: 8 x 3 ## variable random_split total_rss ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &quot;HISEI&quot; 41.22 &quot;Total RSS: 34423362&quot; ## 2 &quot;HISEI&quot; 53.77 &quot;Total RSS: 34400218&quot; ## 3 &quot;HISEI&quot; 56.57 &quot;Total RSS: 32523560&quot; ## 4 &quot;&quot; ... &quot;&quot; ## 5 &quot;FISCED&quot; 2 &quot;Total RSS: 35901660&quot; ## 6 &quot;FISCED&quot; 1 &quot;Total RSS: 36085201&quot; ## 7 &quot;FISCED&quot; 5 &quot;Total RSS: 34083264&quot; ## 8 &quot;&quot; ... &quot;&quot; With such a distribution, the algorithm can objectively ask: which random split best separates the data into two branches with the lowest \\(RSS\\)? And based on that answer, the first node is chosen. After this first node is chosen, two branches grow to both sides. The algorithm then applies exactly the same set of rules recursively for each branch until a maximum depth is reached. Although this explanation will be in nearly all cases invisible to you, this intuition can help you understand better which criteria is used for choosing a split. For example, understanding how this splitting is done gives you insight into how outliers do not affect the selection of splits because the splitting criteria is random and navigates the entire distribution. In addition, there might be cases where you might want to switch the \\(RSS\\) for another loss function because it makes sense for your problem. For example, using decision trees with binary dependent variables merits another type of loss function: Gini impurity. We won’t delve into this but it serves as an example that these are things which are not fixed. These are decision that depend on your research problem and it might make sense to experiment with them if needed. 3.2 Bagging The problem with decision trees is that even if you work really hard to avoid overfitting, they can be very susceptible to the exact composition of the data. For some extreme cases, you might even get completely different trees every time you run your model. Quite literally, running the same model might offer very different trees if some part of the sample changes. This small simulation predicts math_score on all variables in the pisa data set but in each iteration, makes a random sample from the total dataset: Figure 3.1: Visualization of many trees from the sample with varying compositions These drastic differences between each iteration is because decision trees have a lot of variance and very little bias. They learn the current data very well (little bias) but if you generalize them to new data, they can perform very badly (a lot of variance). This is where bagging, or Bootstrap Aggregation comes in. Before we explain what bagging is all about, let’s spend a minute explaining what bootstrapping is. Let’s work out a manual example and limit our pisa dataset to only five rows, keep a few selected columns and add a unique id for each row: sel_cols &lt;- c(&quot;math_score&quot;, &quot;HISEI&quot;, &quot;REPEAT&quot;, &quot;IMMIG&quot;, &quot;read_score&quot;) pisa_small &lt;- pisa[1:5, sel_cols] pisa_small$id &lt;- 1:5 pisa_small ## math_score HISEI REPEAT IMMIG read_score id ## 1 512.7125 28.60 0 1 544.2085 1 ## 2 427.3615 59.89 0 1 432.2518 2 ## 3 449.9545 39.02 0 1 503.9496 3 ## 4 474.5553 26.60 0 1 437.7777 4 ## 5 469.1545 76.65 0 1 535.9487 5 This is the same pisa dataset but only with the first five rows, and id column for each respondent and some columns. Bootstraping is a statistical technique that randomly picks observations from the sample. This means that some observations might get picked while others might no. In fact, some observations might even get picked many times! We can do this manually in R: # Sample from the number of rows in `pisa_small` # and allow certain numbers to be replaced. set.seed(23551) row_index &lt;- sample(nrow(pisa_small), replace = TRUE) pisa_small[row_index, ] ## math_score HISEI REPEAT IMMIG read_score id ## 1 512.7125 28.60 0 1 544.2085 1 ## 4 474.5553 26.60 0 1 437.7777 4 ## 4.1 474.5553 26.60 0 1 437.7777 4 ## 3 449.9545 39.02 0 1 503.9496 3 ## 5 469.1545 76.65 0 1 535.9487 5 We randomly sampled from the data and got the respondent number four repeated twice. We can run this many times and get many resamples of our data: lapply(1:2, function(x) { row_index &lt;- sample(nrow(pisa_small), replace = TRUE) pisa_small[row_index, ] }) ## [[1]] ## math_score HISEI REPEAT IMMIG read_score id ## 3 449.9545 39.02 0 1 503.9496 3 ## 5 469.1545 76.65 0 1 535.9487 5 ## 3.1 449.9545 39.02 0 1 503.9496 3 ## 1 512.7125 28.60 0 1 544.2085 1 ## 3.2 449.9545 39.02 0 1 503.9496 3 ## ## [[2]] ## math_score HISEI REPEAT IMMIG read_score id ## 1 512.7125 28.6 0 1 544.2085 1 ## 4 474.5553 26.6 0 1 437.7777 4 ## 4.1 474.5553 26.6 0 1 437.7777 4 ## 4.2 474.5553 26.6 0 1 437.7777 4 ## 4.3 474.5553 26.6 0 1 437.7777 4 Since the choosing of rows is random, in some instances we might randomly obtain the same row repeated ten times, in others only one time and others even zero times! This is what bootstrapping is all about. If we run \\(10\\) bootstraps, it just means we have \\(10\\) datasets where in each one some rows are repeated many times and others are randomly removed. Bootstrapping is mainly used to calculate statistics such as standard errors and standard deviations because it has very nice properties to estimate uncertainty in situations where its impossible to calculate it. However, it also has advantages for reducing the variance in models such as decision trees. Let’s get back to how bagging works. Bagging works by bootstraping your data \\(N\\) times and fitting \\(N\\) decision trees. Each of these decision trees has a lot of variance because we allow the tree to overfit the data. The trick with bagging is that we average over the predictions of all the \\(N\\) decision trees, improving the high variability of each single decision tree. In the same spirit as before, let’s work out a manual example just so you can truly grasp that intuition. However, don’t worry, there are functions inside tidymodels and tidyflow that will perform all of this for you. Let’s adapt the code from above to use the original pisa data, sample only 60% of the data in each bootstrap and generate 20 copies of our data with random picks of rows in each iteration: pisa$id &lt;- 1:nrow(pisa) bootstrap_pisa &lt;- lapply(1:20, function(x) { row_index &lt;- sample(nrow(pisa) * .6, replace = TRUE) pisa[row_index, ] }) The result is named bootstrap_pisa and is list with 20 data frames. You can inspect the first two with bootstrap_pisa[[1]] and bootstrap_pisa[[2]]. Inside each of these, there should be a data frame with 60% of the original number of rows of the pisa data where each row was randomly picked. Some of these might be repeated many times, others might just be there once and others might not even be there. Let’s now loop over these 20 datasets, fit a decision tree to each one and predict on the original pisa data. The result of this loop should be 20 data frames each with a prediction for every respondent: tflow &lt;- tidyflow() %&gt;% plug_formula(math_score ~ .) %&gt;% plug_model(decision_tree(mode = &quot;regression&quot;) %&gt;% set_engine(&quot;rpart&quot;)) all_pred_models &lt;- lapply(bootstrap_pisa, function(x) { small_model &lt;- tflow %&gt;% plug_data(x) %&gt;% fit() cbind( pisa[&quot;id&quot;], predict(small_model, new_data = pisa) ) }) The first slot contains predictions for all respondents. Let’s confirm that: head(all_pred_models[[1]]) ## id .pred ## 1 1 493.6071 ## 2 2 378.5172 ## 3 3 440.5835 ## 4 4 440.5835 ## 5 5 493.6071 ## 6 6 440.5835 Here we only show the first five rows, but you can check that it matches the same number of rows as the original pisa with nrow(all_pred_model[[1]]) and nrow(pisa). Let’s confirm the same thing for the second slot: head(all_pred_models[[2]]) ## id .pred ## 1 1 486.7747 ## 2 2 432.6909 ## 3 3 432.6909 ## 4 4 432.6909 ## 5 5 486.7747 ## 6 6 486.7747 The second slot also contains predictions for all respondents but they are different from the first slot because they are based on a random sample. This same logic is repeated 20 times such that every respondent has 20 predictions. The trick behind bagging is that it averages the prediction of each respondent over the 20 bootstraps. This averaging has two advantages. First, it allows each single tree to grow as much as possible, allowing it to have a lot of variance and little bias. This has a good property which is little bias but a negative aspect, which is a lot of variance. Bagging compensates this high level of variance by averaging the predictions of all the small trees: # Combine all the 20 predictions into one data frame all_combined &lt;- all_pred_models[[1]] for (i in seq_along(all_pred_models)[-1]) { all_combined &lt;- cbind(all_combined, all_pred_models[[i]][-1]) } # Average over the 20 predictions res &lt;- data.frame(id = all_combined[1], final_pred = rowMeans(all_combined[-1])) # Final prediction for each respondent head(res) ## id final_pred ## 1 1 494.1934 ## 2 2 403.6330 ## 3 3 436.1936 ## 4 4 443.5922 ## 5 5 491.6506 ## 6 6 457.9670 We get a final prediction for each respondent. If we wanted to, we could calculate the standard deviation of these 20 predictions for each respondent and generate uncertainty intervals around each respondent’s predictions. More often than not, this is a good idea. In the previous example we used 20 bootstraps for the sake of simplicity but generally speaking as the number of trees increases, the less variance we will have in the final prediction and thus a stronger model. We can see more clearly the power of combining many trees with the simulation below: The x axis shows the number of bootstraps (or fitted decision trees, it’s the same) and the y axis shows the average \\(RMSE\\) in math_score for each of these bagged trees. As we increase the number of decision trees (or bootstraps, it’s the same), there is a substantial reduction in the error rate of math_score. This is an impressive improvement relative to our initial single decision tree. Having seen the power of increasing the number of trees, how many trees should your model use? For models which exhibit reasonable levels of variability (like our math_score example), \\(100\\)-\\(200\\) bootstraps is often enough to stabilize the error in the predictions. However, very unstable models might require up to \\(500\\) bootstraps. Let’s fit the same model we just implemented manually above using tidymodels and tidyflow. Bagged trees can be implemented with the function bag_tree from the package baguette. With this package we can control the number of bootstraps with the argument times. We can define our model as usual using tidyflow: btree &lt;- bag_tree(mode = &quot;regression&quot;) %&gt;% set_engine(&quot;rpart&quot;, times = 50) tflow &lt;- pisa %&gt;% tidyflow(seed = 566521) %&gt;% plug_split(initial_split) %&gt;% plug_formula(math_score ~ .) %&gt;% plug_model(btree) tflow ## ══ Tidyflow ════════════════════════════════════════════════════════════════════ ## Data: 4.84K rows x 502 columns ## Split: initial_split w/ default args ## Formula: math_score ~ . ## Resample: None ## Grid: None ## Model: ## Bagged Decision Tree Model Specification (regression) ## ## Main Arguments: ## cost_complexity = 0 ## min_n = 2 ## ## Engine-Specific Arguments: ## times = 50 ## ## Computational engine: rpart You might be asking yourself, why don’t we define bootstraps inside plug_resample? After all,bootstraps is a resampling technique. We could do that but it doesn’t make sense in this context. plug_resample is aimed more towards doing grid search of tuning values together with plug_grid. Since bag_trees is not performing any type of grid search but rather fitting a model many times and making predictions, it automatically incorporates this procedure inside bag_trees. If instead we were doing a grid search of let’s say, min_n and tree_depth for bag_tree, using plug_resample with boostraps would be perfectly reasonable. Let’s fit both a simple decision tree and the bagged decision tree, predict on the training set and record the average \\(RMSE\\) for both: res_btree &lt;- tflow %&gt;% fit() res_dtree &lt;- tflow %&gt;% replace_model(decision_tree() %&gt;% set_engine(&quot;rpart&quot;)) %&gt;% fit() rmse_dtree &lt;- res_dtree %&gt;% predict_training() %&gt;% rmse(math_score, .pred) rmse_btree &lt;- res_btree %&gt;% predict_training() %&gt;% rmse(math_score, .pred) c(&quot;Decision tree&quot; = rmse_dtree$.estimate, &quot;Bagged decision tree&quot; = rmse_btree$.estimate) ## Decision tree Bagged decision tree ## 33.85131 11.33018 The bagged decision tree improves the error rate from \\(33\\) math test points to \\(11\\). That is a \\(66\\%\\) reduction in the error rate! That is an impressive improvement for such a simple extension of decision trees. We might decide to improve upon our model by tweaking min_n and tree_depth but you’ll be doing that in the exercises. Instead, let’s predict on the testing data to check whether how much our final model is overfitting the data. rmse_dtree_test &lt;- res_dtree %&gt;% predict_testing() %&gt;% rmse(math_score, .pred) rmse_btree_test &lt;- res_btree %&gt;% predict_testing() %&gt;% rmse(math_score, .pred) c(&quot;Decision tree&quot; = rmse_dtree_test$.estimate, &quot;Bagged decision tree&quot; = rmse_btree_test$.estimate) ## Decision tree Bagged decision tree ## 34.88733 28.87435 It looks like our model is better than the decision tree but it was overfitting the training data considerably. As all other models, bagging also has limitations. First, although bagged decision trees offer improved predictions over single decision trees, they do this at the expense of interpretability. Unfortunately, there is no equivalent of an ‘average’ tree that we can visualize. Remember, we have \\(100\\) predictions from \\(100\\) different trees. It is not possible nor advisable to visualize \\(100\\) trees. Instead, we can look at the average variable importance. Bagging offers the ‘contribution’ of each variable using loss functions. For continuous variables, it uses the \\(RSS\\) (which we have described and used throughout this chapter) and for binary variables it uses the Gini index. We can look at the importance of the variables to get a notion of which variables are contributing the most for predictions: res_btree %&gt;% pull_tflow_fit() %&gt;% .[[&#39;fit&#39;]] %&gt;% var_imp() ## # A tibble: 501 x 4 ## term value std.error used ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 scie_score 23363949. 75426. 50 ## 2 read_score 17033482. 69939. 50 ## 3 ST166Q03HA 5913918. 66479. 50 ## 4 METASPAM 5671665. 68871. 50 ## 5 IC152Q08HA 3850699. 304274. 49 ## 6 PISADIFF 3046729. 362250. 50 ## 7 IC010Q06NA 2691482. 355147. 50 ## 8 ST013Q01TA 433681. 142604. 50 ## 9 ESCS 329367. 16981. 50 ## 10 HOMEPOS 258437. 11440. 50 ## # … with 491 more rows Secondly, bagging might seem like a deal breaker for any type of model (you can apply it to any type of model such as logistic regression, regularized regression, etc..) but it works well only for models which are very unstable. For example, linear regression and logistic regression are models with very little variance. With enough sample size, running a bagged linear regression should return very similar estimates as a single fitted model. 3.3 Random Forests In section 3.1.1 we worked out a simple example on how decision trees choose where to split. If you remember correctly, decision trees evaluate several cutoff points for all the variables in the data. This is done recursively such that in each split, this iteration is performed again for all variables inside the split. This strategy is clever but fails spectacularly whenever some variables are very correlated with the outcome of the decision tree. I did not present the code for the simulation in figure 3.1 but I purposedly excluded the variables scie_score and read_score from the data. That’s why they’re never in any of the trees. Why did I do that? Because they are extremely correlated to math_score and dominate the entire tree. Here’s the same simulation including scie_score and read_score: Regardless of how much the data composition changes between every decision tree, scie_score and read_score are virtually the only two variables present. This is called correlated trees. They are correlated because the predictions between these different trees will be strongly correlated due to the same main variables dominating the trees. When performing bagged decision trees this can be a problem given that the whole idea behind bagging is to average predictions from very different trees. If we have a variable that is constantly repeated in every single tree, then the predictions will be very similar. Random Forests are an extension of bagged decision trees because they randomly sample \\(N\\) variables in each split. More specifically, instead of considering all variables in the data, for calculating a given split, random forests pick a random sample of \\(N\\) variables to be considered for that split. This intuition can be much more accesible through a manual example. Let’s refer back to the first plot of this chapter: For estimating the split of HISEI &lt; 56, decision trees evaluate many random splits in the distribution of HISEI and record the total \\(RSS\\) of each split such that you get a distribution of splits: This is repeated for every variable and then the best split out of all the variables is chosen as the split. Random Forests differ from decision trees in that instead of evaluating cutoff splits for every variable, they evaluate cutoff splits only for a random sample of \\(N\\) variables. The number of variables \\(N\\) used for each split can be chosen by the user and often it is chosen through a grid search. For this manual example, let’s just assume we’ll use half of all the variables in pisa. For the root node split, we randomly select half of the variables in pisa (pisa has 502 variables), so we randomly choose 251) variables and allow the decision tree to work as usual. It will calculate many random cutoff points for the 251 variables and choose the most optimal cutoff point among the 251 variables. Once the root node is determined, two branches are grown to both sides: Random Forests repeat exactly the same logic for every split. For example, to determine the best split for the left branch, it randomly samples 251 variables from the total of 502 but this time limiting the sample to those with HISEI &lt; 56. Similarly for the right branch, it will randomly select 251 variables for those with HISEI &gt; 56 and determine the best cutoff split from these limited number of variables. This is then repeated recursively for every split until the usual tree stopping criteria is reached (min_n per node or the tree_depth). The number of columns to be used in each split is called mtry and there are standard rules of thumb for the number of variables to be randomly picked. In particular, the two most common are \\(\\sqrt{Total\\text{ }number\\text{ }of\\text{ }variables}\\) and \\(\\frac{Total\\text{ }number\\text{ }of\\text{ }variables}{3}\\). For example, using the total number of columns from the pisa dataset (502) and the first equation, it would be \\(\\sqrt{502}\\) which is equal to 22.4053565, rounded down always to the lowest number which is 22. This means that at every split, the random forest will be picking 22 variables at random. As you might expect, if you set mtry to randomly pick the same number of columns in the data, you’re effectively performing a bagged decision tree rather than a random forest. This would be the equivalent of setting mtry to 502 for the pisa example. This approach of sampling random columns at each split was created to uncorrelate the distribution of decision trees. But, can we be sure that all variables will be used? Well, random sampling is by definition random. On average, all variables will be present in some splits and will allow other variables which are not that strongly correlated with the predictor to play a role in the tree construction. Random Forests have gained a lot of fame because they often achieve incredible accuracy with just standard values for the tuning parameters (for example, the default mtry of \\(\\sqrt{Total\\text{ }number\\text{ }of\\text{ }variables}\\) is often enough, without having to try multiple values in a grid search). Let’s try running the same math_score example but with a random forest, predict on the training set and calculate the \\(RMSE\\) of the model: # Define the random forest rf_mod &lt;- rand_forest(mode = &quot;regression&quot;) %&gt;% set_engine(&quot;ranger&quot;, importance = &quot;impurity&quot;) # Define the `tidyflow` with the random forest model # and include all variables (including scie_score and read_score) tflow &lt;- pisa %&gt;% tidyflow(seed = 23151) %&gt;% plug_formula(math_score ~ .) %&gt;% plug_split(initial_split) %&gt;% plug_model(rf_mod) rf_fitted &lt;- tflow %&gt;% fit() rf_fitted %&gt;% predict_training() %&gt;% rmse(math_score, .pred) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 16.3 It performs slightly worse than the bagged decision trees (the bagged decision tree had an error of 11.33 math test points). Why is that? Let’s look at which variables are important in the random forest: rf_fitted %&gt;% pull_tflow_fit() %&gt;% .[[&#39;fit&#39;]] %&gt;% vip() + theme_minimal() scie_score and read_score seem to be the most relevant variables. But not only are they the most relevant ones, they are disproportionately the strongest: they both are seven times more important than the next most strongest variable. When there are only a few very strong predictors (let’s say, two), then you might have a lot of trees which don’t offer a lot of good predictions. In the model we ran above, the total number of variables used at each split was 22 meaning that if scie_score and read_score are the only important variables that predict math_score, they might be excluded from a lot of the splits. This means that out of the 500 default trees that are repated, you might get trees which are not better at predicting math_score than random luck. Based on this intuition, if we increase the number of variables used at each split, we should see an increase in predictive error. Why? Because it means the scie_score and read_score will have greater probability of being included at each split. Let’s try something close to \\(1/3\\) of the number of variables (this is 150 variables): rf_mod &lt;- rand_forest(mode = &quot;regression&quot;, mtry = 150) %&gt;% set_engine(&quot;ranger&quot;) rf_fitted &lt;- tflow %&gt;% replace_model(rf_mod) %&gt;% fit() rf_fitted %&gt;% predict_training() %&gt;% rmse(math_score, .pred) The predictive error is reduced to be the same as the one from the bagged decision tree. However, time wise this model is superior than bag_tree because each decision tree uses less variables in total. You might be asking youself: if bagged decision trees have a lot of correlated trees and the random forest decorrelates the trees, why is it performing just as well and not better? It’s not entirely clear. Random Forests are considered to be a bit of a ‘black box’ and they might work well in certain cases and bad in others. However, having the intuition of how random forests work can help us to approximate a likely explanation. If scie_score and read_score are the most important variables for predicting math_score and no other variables have strong correlations with math_score, then excluding these two variables from a model, might produce trees which just don’t predict math_score well. The value of a random forest over bagged decision trees is that each individual tree must have some sort of predictive power despite excluding the strongest predictors. We can check whether our intuition is correct by running a bagged decision tree and a random forest without scie_score and read_score. If we exclude scie_score and read_score, the remaining variables are very weakly correlated with math_score and thus the random forest produces trees which have richer variability then the bagged trees in predictive performance: rf_mod &lt;- rand_forest(mode = &quot;regression&quot;, mtry = 150) %&gt;% set_engine(&quot;ranger&quot;, importance = &quot;impurity&quot;) bt_mod &lt;- bag_tree(mode = &quot;regression&quot;) %&gt;% set_engine(&quot;rpart&quot;) tflow &lt;- tflow %&gt;% replace_formula(math_score ~ . - scie_score - read_score) rf_res &lt;- tflow %&gt;% replace_model(rf_mod) %&gt;% fit() bt_res &lt;- tflow %&gt;% replace_model(bt_mod) %&gt;% fit() bt_rmse &lt;- bt_res %&gt;% predict_training() %&gt;% rmse(math_score, .pred) %&gt;% pull(.estimate) rf_rmse &lt;- rf_res %&gt;% predict_training() %&gt;% rmse(math_score, .pred) %&gt;% pull(.estimate) c(&quot;Bagged decision trees&quot; = bt_rmse, &quot;Random Forest&quot; = rf_rmse) ## Bagged decision trees Random Forest ## 24.32169 21.05124 The random forest is performing better than the bagged decision tree, as our reasoning predicted. Having shown this, since we exclude the two most important predictors, the overall error is higher than the previous models. This is expected and we have fitted the previous model just to understand why the random forest is not doing better than the bagged decision tree. Let’s predict on the testing data to check whether any of the two is overfitting more than the other: bt_rmse &lt;- bt_res %&gt;% predict_testing() %&gt;% rmse(math_score, .pred) %&gt;% pull(.estimate) rf_rmse &lt;- rf_res %&gt;% predict_testing() %&gt;% rmse(math_score, .pred) %&gt;% pull(.estimate) c(&quot;Bagged decision trees&quot; = bt_rmse, &quot;Random Forest&quot; = rf_rmse) ## Bagged decision trees Random Forest ## 59.64462 55.13338 Both models seem to have been overfitting the data considerably, but they were both overfitting to the same degree: the difference is of around 4 points in both the training and testing data. Random Forests have several advantages, among which are that they are considerably faster than bagged decision trees because they use only a fraction of the variables used in bagging. However, as we have just showed, their performance gain is not straight forward when the outcome variable only has a few predictors which are strongly correlated with the outcome and very few which are not. There has to be some type of gradient in the correlations between the predictors and the outcome for the random forest to have a considerable improvement. Our model also offers a good example of how tuning different values of the parameters can offer improved accuracy. If we would’ve stayed with the default mtry of \\(\\sqrt{Total\\text{ }number\\text{ }of\\text{ }variables}\\), the trees would have consistently included uncorrelated variables in each decision tree, making many of our trees obsolete in predictive accuracy. Instead, whenever we’re running random forests, we should try several values of mtry and empirically check which number of columns produce an improvement in the error rate. One important note which sometimes has aroused discussion in machine learning is whether increasing the number of trees increase the chance of overfitting. Increasing the number of trees in a random forest does not lead to increased overfitting because the random forest averages the predictions of all the different trees. Single trees can overfit the data, and that’s ok, because we take the average of all these hundred trees. Aside from the argument mtry, random forests also have other parameters which can be tuned for their optimal value in a grid search. min_n and trees are the most important ones to tune. Chapter 11 of Boehmke and Greenwell (2019) offers good examples on some of the default values you can explore in a random forest and some general intuitions on which parameters have the biggest impact on computational resources, including the time spent running models. This next example can serve as an introductory template if you want to tune some values for your random forest. Beware, searching for the most optimal value of a 10 x 10 grid through 10 cross-validated sets can take a lot of time (even a few hours). You are effectively running 1000 models: rf_mod &lt;- rand_forest(mode = &quot;regression&quot;, mtry = tune(), trees = tune(), min_n = tune()) %&gt;% set_engine(&quot;ranger&quot;) tflow &lt;- pisa %&gt;% tidyflow(seed = 2151) %&gt;% plug_split(initial_split) %&gt;% plug_resample(vfold_cv) %&gt;% plug_grid(grid_random, levels = 10) %&gt;% plug_formula(math_score ~ .) %&gt;% plug_model(rf_mode) res &lt;- rf_mod %&gt;% fit() res The problem with random forest is mostly about computing time. Although they often do really well with the default values, a user often has to perform a grid search that takes up a lot of time. This can be very time consuming and thus it’s always recommended to start with simple models and then elaborate from there. 3.4 Boosting So far, the tree based methods we’ve seen use decision trees as baseline models and an ensemble approach to calculate the average predictions of all decision trees. Boosting also uses decision trees as the baseline model but the ensemble strategy is fundamentally different. The name boosting comes from the notion that we fit a weak decision tree and we ‘boost’ it iteratively. This strategy is fundamentally different from bagging and random forests because instead of relying on hundreds of independent decision trees, boosting works by recursively boosting the the result of the same decision tree. How does it do that? Let’s work it out manually with our math_score example. Let’s fit a very weak decision tree of math_score regressed on scie_score. dt_tree &lt;- decision_tree(mode = &quot;regression&quot;, tree_depth = 1, min_n = 10) %&gt;% set_engine(&quot;rpart&quot;) pisa_tr &lt;- training(initial_split(pisa)) tflow &lt;- pisa_tr %&gt;% tidyflow(seed = 51231) %&gt;% plug_formula(math_score ~ scie_score) %&gt;% plug_model(dt_tree) mod1 &lt;- fit(tflow) mod1 %&gt;% pull_tflow_fit() %&gt;% .[[&#39;fit&#39;]] %&gt;% rpart.plot() This decision tree is very weak. It is only allowed to have a tree depth of 1 which probably means that it is underfitting the data. We can checkout the \\(RMSE\\) of the model: res_mod1 &lt;- pisa_tr %&gt;% cbind(., predict(mod1, new_data = .)) res_mod1 %&gt;% rmse(math_score, .pred) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 55.3 This is not a good model. The \\(RMSE\\) is much bigger than our previous models. In fact, if we look at the residuals, we should see a very strong pattern, something that signals that a model is not properly specified: res_mod1 &lt;- res_mod1 %&gt;% mutate(.resid = math_score - .pred) res_mod1 %&gt;% ggplot(aes(scie_score, .resid)) + geom_point(alpha = 1/3) + scale_x_continuous(name = &quot;Science scores&quot;) + scale_y_continuous(name = &quot;Residuals&quot;) + theme_minimal() Boosting works by predicting the residuals of previous decision tree. In our example, we just fitted our first model and calculated the residuals. Boosting works by fitting a second model but the dependent variable should now be the residuals of the first model rather than the math_score variable. Let’s do that: # Convert `math_score` to be the residuals of model 1 res_mod1 &lt;- mutate(res_mod1, math_score = .resid) # Replace the new data in our `tflow` # In the data `res_mod1`, `math_score` # is now the residuals of the first model mod2 &lt;- tflow %&gt;% replace_data(res_mod1) %&gt;% fit() mod2 %&gt;% pull_tflow_fit() %&gt;% .[[&#39;fit&#39;]] %&gt;% rpart.plot() This second model is exactly the same as the first model with the only difference that the dependent variable is the residuals (the unexplained test scores from the first model) of the first model. You can actually see a hint of this in the average score of the dependent variable in the two leaf nodes: they are both in the scale of residuals rather than the scale of math_score (the average math_score is between 300 and 600 points whereas the average of each leaf node is very far from this). Let’s visualize the residuals from the second model: res_mod2 &lt;- pisa_tr %&gt;% cbind(., predict(mod2, new_data = .)) %&gt;% mutate(.resid = math_score - .pred) res_mod2 %&gt;% ggplot(aes(scie_score, .resid)) + geom_point(alpha = 1/3) + scale_x_continuous(name = &quot;Science scores&quot;) + scale_y_continuous(name = &quot;Residuals&quot;) + theme_minimal() The pattern seems to have changed although it’s not clear that it’s closer to a random pattern. In the small-scale example we just did, we only fitted two models (or in machine learning jargon, two trees). If we fitted 20 trees, each using the previous model’s residuals as the dependent variable, we can start to see how the residuals are converging towards randonmness: As each model tries to improve on the previous model’s residuals, the pattern in the residual converges towards randomness. This means that most of the signal in the model has been captured and the remaining residuals are just random noise. Understanding this iterative process gives a more intuitive explanation to the term boosting. Boosting is a way for each model to boost the last model’s performance by focusing mostly on observations which had big residuals (and thus greater error). Suppose that you have now fitted 20 trees. This means that each of the respondents in the data will have 20 predictions. However, these predictions are not the same as in bagging and random forest: you cannot take the average of these predictions because they are not in the same metric as math_score. What? What do we mean that they’re not in the same metric? Let’s compare the predictions of our first model and the second model: mod1_pred &lt;- predict(mod1, new_data = pisa_tr) names(mod1_pred) &lt;- &quot;pred_mod1&quot; mod2_pred &lt;- predict(mod2, new_data = pisa_tr) names(mod2_pred) &lt;- &quot;pred_mod2&quot; resid_pred &lt;- cbind(mod1_pred, mod2_pred) head(resid_pred) ## pred_mod1 pred_mod2 ## 1 535.6033 -10.47568 ## 2 401.1973 -10.47568 ## 3 401.1973 -10.47568 ## 4 535.6033 -10.47568 ## 5 401.1973 -10.47568 ## 6 535.6033 -10.47568 The predictions from the first model are in the scale of math_score, mostly between 300 and 600 points. However, the predictions of the second model are very small compared to the first predictions. This is because the dependent variable from the second model is the residuals from the first model. All predictions will thus be in the scale of the residuals! If we ran 20 trees, the predictions from the first model will always be in the scale of the dependent variable while the predictions of the remaining 19 trees will be in the scale of the residuals. With these weird looking 20 predictive values, how can you arrive at the final prediction for each respondent? By adding up the predictions of the 20 trees for each respondent. For our small-scale example, we can do that with rowSums: resid_pred$final_pred &lt;- rowSums(resid_pred) head(resid_pred) ## pred_mod1 pred_mod2 final_pred ## 1 535.6033 -10.47568 525.1276 ## 2 401.1973 -10.47568 390.7216 ## 3 401.1973 -10.47568 390.7216 ## 4 535.6033 -10.47568 525.1276 ## 5 401.1973 -10.47568 390.7216 ## 6 535.6033 -10.47568 525.1276 We have a final prediction for each respondent. How can we use these models to predict on new data? We follow the same logic. We allow each tree to predict on this new data and then sum the predictions of all the trees. In case you wanted to build some code that applies our manual proceedure automatically, I have written the code below to do it for you. It fits \\(N\\) models on the same data but after the first model, the dependent variable of the decision tree becomes the residuals of the previous model: # This is the base decision tree used in each iteration. dt_tree &lt;- decision_tree(mode = &quot;regression&quot;, tree_depth = 1, min_n = 10) %&gt;% set_engine(&quot;rpart&quot;) # Let&#39;s extract the training data separately to fit the models pisa_tr &lt;- training(initial_split(pisa)) # Construct our tidyflow tflow &lt;- pisa_tr %&gt;% tidyflow(seed = 51231) %&gt;% plug_formula(math_score ~ scie_score) %&gt;% plug_model(dt_tree) # How many trees you want to run? n &lt;- 40 # This list will have the 40 models res &lt;- vector(&quot;list&quot;, n) # This dataframe will have the 40 predictions # for each respondent df_pred &lt;- data.frame(pred_1 = rep(0, nrow(pisa_tr))) # Fit the first model res[[1]] &lt;- fit(tflow) # Repeat this loop 40-1 times for (i in 1:(n-1)) { print(paste0(&quot;Model &quot;, i)) # Save the prediction from the last model in `df_pred` df_pred[[paste0(&quot;pred_&quot;, i)]] &lt;- predict(res[[i]], new_data = pull_tflow_rawdata(res[[i]]))[[1]] # Calculate the residuals from the previous model pred_data &lt;- cbind( pull_tflow_rawdata(res[[i]]), .pred = df_pred[[paste0(&quot;pred_&quot;, i)]] ) %&gt;% mutate(math_score = math_score - .pred) %&gt;% select(-.pred) # Run the actual model with the residuals from the previous # model res[[i + 1]] &lt;- tflow %&gt;% replace_data(pred_data) %&gt;% fit() } # Add up the 40 predictions for each respondent # What is the final RMSE? pisa_tr %&gt;% mutate(.pred = rowSums(df_pred)) %&gt;% as_tibble() %&gt;% rmse(math_score, .pred) # Does it match closely the more robust implementation # from the package gbm? mod &lt;- gbm::gbm(math_score ~ ., data = pisa_tr, n.trees = 40, interaction.depth = 1, n.minobsinnode = 10) pisa_tr %&gt;% mutate(.pred2 = predict(mod, newdata = pisa_tr, n.trees = 40)) %&gt;% rmse(math_score, .pred2) This raw implementation of the boosting algorithm is not something you should use for your own research or predictive tasks. Boosting has evolved to be more complex and other features have been introduced. For example, the gbm implementation includes a shrinkage parameter that penalizes each decision tree such that you have to increase the number of trees being fitted. This is a general approach called ‘slow learning’ which usually has better predictive accuracy than our manual approach. There are many other small tweaks that have improved the performance of boosting. We refer the reader to chapter 12 of Boehmke and Greenwell (2019) which discusses them in detail. Let’s fit our trademark model of math_score regressed on all variables with the more advanced implementation of boosting used in the package xgboost. The results from this model can be compared to our previous results of bagging and the random forest: boost_mod &lt;- boost_tree(mode = &quot;regression&quot;, trees = 500) %&gt;% set_engine(&quot;xgboost&quot;) tflow &lt;- pisa %&gt;% tidyflow(seed = 51231) %&gt;% plug_formula(math_score ~ .) %&gt;% plug_split(initial_split) %&gt;% plug_model(boost_mod) boot_res &lt;- fit(tflow) ## [17:32:51] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror. rmse_gb_train &lt;- boot_res %&gt;% predict_training() %&gt;% rmse(math_score, .pred) rmse_gb_train ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 0.000630 An \\(RMSE\\) of 0.0006! This an impressive improvement relative to the best random forest or bagging performance. It is even more remarkable that this improvement happens without performing any grid search for the most optimal values in the tuning values. Let’s check how it performs on the testing data and compare that to the same estimate from the bagged decision trees and the random forest: gb_rmse &lt;- boot_res %&gt;% predict_testing() %&gt;% rmse(math_score, .pred) %&gt;% pull(.estimate) c(&quot;Bagged decision trees&quot; = bt_rmse, &quot;Random Forest&quot; = rf_rmse, &quot;Extreme Gradient Boosting&quot; = gb_rmse) ## Bagged decision trees Random Forest Extreme Gradient Boosting ## 59.64462 55.13338 26.78929 The extreme gradient boosting model achieved a testing error of 26.7892924, considerably lower than the counterpart of the bagged decision trees and the random forest (59.6446197 for the bagged decision trees and 55.1333767 for the random forest, respectively). Boosting, and it’s more full featured cousin ‘Extreme Gradient Boosting’ (those are the initials of the package xgboost) are considered to be among the best predictive models currently developed. They can achieve impressive predictive power with little tuning and preprocessing of data. However, there is one pitfall that you should be aware of. In contrast to random forests, increasing the number of trees in a boosting algorithm can increase overfitting. For the random forest, increasing the number of trees has no impact on overfitting because what we actually get is the average of all these trees. However, for boosting, increasing the number trees means increasing the number of trees that iteratively try to explain residuals. You might reach a point that adding more trees will just try to explain residuals which are random, resulting in overfitting. boost_tree has implemented a tuning parameter called stop_iter (as of 15th of June 2020, this tuning parameter has now been implemented in the development version of parsnip which you can install with devtools::install_github(\"tidymodels/parsnip\")). stop_iter signals that after \\(N\\) number trees have passed without any improvement, the algorithm should stop. This approach often runs less trees than the user requested. There are other tuning parameters available in boost_tree which you can use to improve your model. Here I shortly describe each one. trees: the number of trees that will be ran. mtry: just as in random forests, mtry can be specified in boost_tree and as expected, it controls the number of variables that will be used at each split of a decision tree. min_n: controls the minimum number of observations that need to be present in each node of a decision tree. This can avoid having decision trees which overfit a lot. tree_depth: is the same tuning parameter we used in decision_tree and it controls how complex the tree is allowed to grow. This interacts very strongly with min_n. learn_rate: controls how much we regularize each tree such that more penalization warrants more decision trees. This is because the algorithm is learning ‘slowly’ and requires more iterations. loss_reduction: signals the amount of reduction in your loss function (for example, \\(RMSE\\)) that will allow each split in a decision tree to continue to grow. You can see this as cost-effective step: only if the tree improves it’s prediction by \\(X\\), we allow the tree to produce another split. sample_size: controls the percentage of the data used in each iteration of the decision tree. This is similar to the bagging approach where we perform bootstraps on each iteration. Boosting algorithms have often been considered black boxes in terms of how they work. In this section we manually implemented a very simple boosting algorithm and showed their predictive power with our trademark example. Note that throughout this entire chapter, all examples and explanations have focused on continuous variables. However, all of the techniques discussed above work just as well for dummy variables. We have omitted these discussions because it would be a lot of material for the course. 3.5 Exercises In these exercises we’ll continue try to predict the mathematics test score (math_score) from pisa. First, read in the data with: data_link &lt;- &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot; pisa &lt;- read.csv(data_link) 1. Fit a decision tree of math_score on all variables and visualize Assign a seed of 231151 to the tidyflow Separate the data into training/testing with initial_split Fit the decision_tree() with mode = 'regression' but without any arguments (don’t specify min_n or tree_depth) Extract the final model with pull_tflow_fit and .[['fit']] Visualize the final model with rpart.plot Which variables are the most important? &gt; Answer tflow &lt;- pisa %&gt;% tidyflow(seed = 23151) %&gt;% plug_split(initial_split) %&gt;% plug_formula(math_score ~ .) %&gt;% plug_model(decision_tree(mode = &quot;regression&quot;) %&gt;% set_engine(&quot;rpart&quot;)) dtree1 &lt;- tflow %&gt;% fit() dtree1 %&gt;% pull_tflow_fit() %&gt;% .[[&#39;fit&#39;]] %&gt;% rpart.plot() 2. Fit the same model from above but excluding the science and literacy test scores Replace the formula with math_score ~ . - scie_score - read_score Visualize the model with a tree plot Look up the PISA codebook here. Do these variables make sense? Is the ranking of most important variables making sense with respect to math_score? &gt; Answer tflow &lt;- tflow %&gt;% replace_formula(math_score ~ . - scie_score - read_score) dtree1 &lt;- tflow %&gt;% fit() dtree1 %&gt;% pull_tflow_fit() %&gt;% .[[&#39;fit&#39;]] %&gt;% rpart.plot() 3. Run a grid search of min_n and tree_depth for the single decision tree Extract the model from the tidyflow with pull_tflow_spec and update min_n and tree_depth to be tune Add a cross-validation step. Add a grid_regular tuning grid to the model. Be sure to specify the argument levels = 2 in plug_grid to force grid_regular to do a grid search of 2 x 2 of the tuning parameters. Fit the model (this can take a few minutes, that’s why we limited the number of grid values to 2 x 2) Extract the tuning values with pull_tflow_fit_tuning and summarize which metrics are better with collect_metrics Which tuning parameter seems to be more important for a lower \\(RMSE\\)? What is the most optimal combination of tuning parameters? &gt; Answer It seems that tree_depth is more important because there is no difference between min_n within each tree_depth Most optimal combination is tree_depth of 15 and min_n of 2, because it’s the simplest model with the lower mean \\(RMSE\\) Note that this is probably not the most optimal decision tree because we are trying very few tuning values (only a 2 x 2 grid). In case you want to experiment with even better models, you can run more tuning values by replacing the levels in plug_grid. If you supply a 10, the model will try a 10 x 10 grid of tree_depth and min_n. Trying out many tuning values is very important because you can visualize whether any particular parameter starts to increase the error rate after a certain threshold. dectree &lt;- tflow %&gt;% pull_tflow_spec() %&gt;% update(min_n = tune(), tree_depth = tune()) tflow &lt;- tflow %&gt;% plug_resample(vfold_cv) %&gt;% plug_grid(grid_regular, levels = 2) %&gt;% replace_model(dectree) tuned_mod &lt;- fit(tflow) tuned_mod %&gt;% pull_tflow_fit_tuning() %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) 4. Finalize the best model with complete_tflow and predict on the training set Run the best model with complete_tflow. Remember to set the metric from which you will define the best model! Here, we’re focusing on \\(RMSE\\). Use predict_training to predict on the training data and calculate the \\(RMSE\\) with rmse Compared to the examples in the main sections, is the \\(RMSE\\) acceptable? How could you improve it? &gt; Answer The \\(RMSE\\) of the model is quite low compared to the main examples. It’s almost twice as large as estimates from the main material. We could improve the error rate in many ways. First, we could search for the top 10 or 20 variables and run the model only using those. That way we can a get much simpler model and check whethere it gives very similar results. Additionally, we can try many more tuning values with the levels argument of plug_grid. Further stratigies could explore whether certain variables need to be recoded to capture meaningful variation or whether some variables need transformations. final_dtree &lt;- tuned_mod %&gt;% complete_tflow(metric = &quot;rmse&quot;) # You can check that it uses the best combination by pulling out # the model: final_dtree %&gt;% pull_tflow_spec() final_dtree %&gt;% predict_training() %&gt;% rmse(math_score, .pred) 5. Run the same model for bagged decision trees Remove the resample and grid with drop_resample and drop_grid Replace the model with a bag_tree model instead of the single decision_tree Fit the bagged decision tree Pull out the fitted model with pull_tflow_fit and explore the top ten variables Calculate the \\(RMSE\\) on the training data Which are the most important variables? Do they correspond with the variables used in the decision tree? Is the \\(RMSE\\) lower than the decision tree exercise? &gt; Answer If the top variables don’t make substantive sense, they might just be capturing noise in the data. It’s always important to keep a critical eye on your models. Check whether the variables chosen by the model make sense. If they don’t, explore through visualization and correlations whether they are indeed related to the test score and how. Often, a model with only the most important variables does as well as a model with 500 variables. tflow &lt;- tflow %&gt;% drop_resample() %&gt;% drop_grid() %&gt;% replace_model(bag_tree(mode = &quot;regression&quot;) %&gt;% set_engine(&quot;rpart&quot;)) btree_mod &lt;- fit(tflow) # Explore the top ten variables btree_mod %&gt;% pull_tflow_fit() # RMSE of the bagged decision tree btree_mod %&gt;% predict_training() %&gt;% rmse(math_score, .pred) 6. Run the same model from above but only on the ten top variables from the previous model Identify the top 10 variables from the model Replace the original formula with the summarized formula with only the top ten variables Extract the model from the tidyflow with pull_tflow_spec and set_engine with times = 100 to increase the bootstraps to 100. Remember, the higher the number of bootstraps, the more robust the model is. Replace the old bag_tree from the tidyflow with this new model with times = 100. Fit the model Calculate the \\(RMSE\\) of the model How far is the \\(RMSE\\) of ‘optimized’ model from the previous model? Is it worth chosing this model over the other one? Why didn’t we run times = 100 for the previous model? &gt; Answer This model’s \\(RMSE\\) is 26.3 while the one with 500 variables is 24. Depending on your context, you might prefer either one. For example, in a business setting, an improvement of 2 points might increase revenue by a big margin. In other settings, the difference of 2 points might not be that important in substantive terms, and thus a simple model would be better. For this case, a difference of 2 points in a math test score is very small and probably within the margin of error of the models. The best decision here is to keep the simplest model. Running 100 models with 500 variables is very time consuming. We decided to use times = 100 on the simplified model because the time spent on fitting is really fast and allows us to rapidly prototype different models. # Which are the top ten variables? btree_mod %&gt;% pull_tflow_fit() optim_btree &lt;- tflow %&gt;% pull_tflow_spec() %&gt;% set_engine(&quot;rpart&quot;, times = 100) # Replace the formula with 500 variables with only the top 10 tflow &lt;- tflow %&gt;% replace_formula(math_score ~ ST166Q03HA + METASPAM + PISADIFF + ST166Q01HA + ESCS + ST163Q03HA + ST166Q04HA + ST163Q02HA + ST163Q04HA + HISEI) %&gt;% replace_model(optim_btree) topten_mod &lt;- fit(tflow) topten_mod %&gt;% predict_training() %&gt;% rmse(math_score, .pred) 7. Perform a grid search on the simplified model of 10 variables Extract the model from the tidyflow and update min_n and tree_depth to be tune. If you want the models to run faster, change times = 20 with set_engine to this model. Replace the model the old model with the tuned one in the tidyflow Add a cross-validation step to the tidyflow with vfold_cv Plug in a manual grid with expand.grid. Set min_n to explore the values c(5, 10) and tree_depth to explore the values c(3, 9). Fit the model to perform the grid search (this might take a few minutes since we’re running 200 models, 20 bootstraps by 10 cross-validation datasets) Pull the tuning results with pull_tflow_fit_tuning and pass it to collect_metrics. Explore the \\(RMSE\\) of the best tuning values Train the best model with complete_tflow, predict_training and calculate the best \\(RMSE\\) with rmse. Is the value of the \\(RMSE\\) lower than the previous model? Why? Does it make sense to do a grid search? Are the results improving? &gt; Answer For certain scenarios, performing a grid search of the most optimal tuning values might not a good idea with bagged decision trees. Why? Because the purpose of bagged trees is to run small trees that overfit the data as much as possible. That is, they allow the tree depth to grow as much as possible and require as little as 2 observations with each node. This means that there is really no improvement for choosing more ‘adequate’ tuning values. In the end, each small tree should be as complex as possible because what we’re really after is the average prediction of all these small trees. tune_btree_mod &lt;- tflow %&gt;% pull_tflow_spec() %&gt;% update(min_n = tune(), tree_depth = tune()) %&gt;% set_engine(times = 20, &quot;rpart&quot;) tflow &lt;- tflow %&gt;% plug_resample(vfold_cv) %&gt;% plug_grid(expand.grid, min_n = c(5, 10), tree_depth = c(3, 9)) %&gt;% replace_model(tune_btree_mod) tuned_btree &lt;- fit(tflow) tuned_btree %&gt;% pull_tflow_fit_tuning() %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) final_btree &lt;- tuned_btree %&gt;% complete_tflow(metric = &quot;rmse&quot;) final_btree %&gt;% predict_training() %&gt;% rmse(math_score, .pred) 8. Fit an untuned random forest to check whether the default values are enough to beat the other models Random Forests are well known for achieving greater predictive performance than bagging with simple off-the-shelf tuning values. That is without any grid search, they can often perform well with the default values. Run two models, one with the same specification as before (with the top ten variables) and one regressed on all the variables in the data sets and compare the \\(RMSE\\) of the models. Define a rand_forest model with 500 trees and replace the previous model form the tflow. Remember to set importance = \"impurity\" in the engine to check for the most important variables. Since we won’t be doing any grid search, remove the resample and grid from the tflow Create another tidyflow (with the same specification) but replacing the formula with math_score ~ . - scie_score - read_score Fit both tflow Calculate the \\(RMSE\\) in the training set for both models Do the most important variables from the more complex models match the top 10 variables from the simple model? Does the random forest beat the previous model in accuracy? Does the simple model perform better than the more complex one? &gt; Answer Without any grid search, the random forest beats the previous models by a range of nearly 20 math points. The more complex model seems to outperform the simple model by around 14 points. Nearly all variables from the top 10 are also present in the random forest, however the performance increase is notable so the remaining variables must be contributing to a certain extent. rand_mod &lt;- rand_forest(mode = &quot;regression&quot;, trees = 500) %&gt;% set_engine(&quot;ranger&quot;, importance = &quot;impurity&quot;) tflow &lt;- tflow %&gt;% replace_model(rand_mod) %&gt;% drop_resample() %&gt;% drop_grid() tflow_all &lt;- tflow %&gt;% replace_formula(math_score ~ . - scie_score - read_score) res_randf &lt;- tflow %&gt;% fit() res_randf_all &lt;- tflow_all %&gt;% fit() # RMSE of simple model res_randf %&gt;% predict_training() %&gt;% rmse(math_score, .pred) # RMSE of complex model res_randf_all %&gt;% predict_training() %&gt;% rmse(math_score, .pred) # Visualize the top 10 variables res_randf_all %&gt;% pull_tflow_fit() %&gt;% .[[&#39;fit&#39;]] %&gt;% vip() + theme_minimal() 9. Perform a grid search using a boosting algorithm From the example in the main text, we saw that boosting had the highest accuracy out of all models. However, can we improve it? This exercise is exploratory. Try running a boost_tree varying levels of trees and learn_rate. Keep in mind that: Increasing the number of trees can results in greater overfitting If the learning rate is decreased, it means that the model is learning slower and thus greater trees are needed The testing error for the default boosting algorithm was 54. Can you improve that? One strategy would be to perform a grid search. Another strategy is to focus on variables which are important for predicting math_score. &gt; Answer Here we settled for a solution which increased the number of trees from 500 to 1000 but decreased the default learning rate of 0.3 to 0.1. This difference decreased the testing error to 48 points, 7 points below the benchmark. xg_mod &lt;- boost_tree(mode = &quot;regression&quot;, trees = 1000, learn_rate = 0.1) %&gt;% set_engine(&quot;xgboost&quot;, importance = &quot;impurity&quot;) tflow &lt;- tflow %&gt;% replace_model(xg_mod) %&gt;% replace_formula(math_score ~ . - scie_score - read_score) res_boosting &lt;- tflow %&gt;% fit() res_boosting %&gt;% predict_testing() %&gt;% rmse(math_score, .pred) References "],
["loss-functions.html", "Chapter 4 Loss functions 4.1 Continuous loss functions 4.2 Binary loss functions", " Chapter 4 Loss functions To evaluate a model’s fit, social scientists use metrics such as the \\(R^2\\), \\(AIC\\), \\(Log\\text{ }likelihood\\) or \\(BIC\\). We almost always use these metrics and their purpose is to inform some of our modeling choices. However, they are rarely determinant for us in publishing results, excluding variables or making important modeling decisions. Machine Learning practitioners use these exact metrics but they are at the core of their modeling workflow. In machine learning, metrics such as the \\(R^2\\) and the \\(AIC\\) are called ‘loss functions’. Remember that, because you’ll see that term popping in everywhere in machine learning. Despite this fancy name, loss functions evaluate the same thing we social scientists evaluate: a model’s fit. In this chapter we won’t focus on loss functions that social scientists are used to such as \\(R^2\\), \\(BIC\\) and \\(AIC\\). Instead, we’ll discuss loss functions which are targetted towards prediction. Let’s load the packages we’ll use in the chapter before we begin: library(tidymodels) library(tidyflow) library(plotly) library(ggplot2) data_link &lt;- &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot; pisa &lt;- read.csv(data_link) 4.1 Continuous loss functions Continuous variables require completely different loss functions than binary variables. In this section we’ll discuss the \\(RMSE\\) and the \\(MAE\\). 4.1.1 Root Mean Square Error (RMSE) The most common loss function used for continous variables is the Root Mean Squared Error, or \\(RMSE\\). The \\(RMSE\\) works by evaluating how far our predictions are from the actual data. More concretely, suppose we have a linear relationship between X and Y: We fit a model and plot the predicted values: How can we evaluate whether this line is accurate? The most common approach is to subtract the actual \\(Y_{i}\\) score of each respondent from the predicted \\(\\hat{Y_{i}}\\) for each respondent: After the subtraction occurs, we square these differences (because squaring removes the negative and positive sign of subtracting over and under predicted values), calculate the average error and take the square root of that. Social scientists are aware of this. Here’s the formula, in case this sounds familiar: \\[ RMSE = \\sqrt{\\sum_{i = 1}^n{\\frac{(\\hat{y_{i}} - y_{i})^2}{N}}} \\] In machine learning, this metric is used to identify models which have the lowest predictive error. For example, let’s fit two linear models using the pisa data set. One will regress math_score (mathematics test score) on scie_score and read_score (they each measure test scores in science and literacy respectively) and the other will regress math_score on all variables excluding science and literacy scores. # Model with only scie_score and read_score mod1 &lt;- pisa %&gt;% tidyflow(seed = 4113) %&gt;% plug_formula(math_score ~ scie_score + read_score) %&gt;% plug_split(initial_split) %&gt;% plug_model(set_engine(linear_reg(), &quot;lm&quot;)) # Model with all varibales except scie_score and read_score mod2 &lt;- mod1 %&gt;% replace_formula(math_score ~ . - scie_score - read_score) # Fit the models res1 &lt;- fit(mod1) res2 &lt;- fit(mod2) # Calculate the RMSE of model 1 rmse_1 &lt;- res1 %&gt;% predict_training() %&gt;% rmse(math_score, .pred) %&gt;% pull(.estimate) # Calculate the RMSE of model 2 rmse_2 &lt;- res2 %&gt;% predict_training() %&gt;% rmse(math_score, .pred) %&gt;% pull(.estimate) c(&quot;RMSE first model&quot; = rmse_1, &quot;RMSE second model&quot; = rmse_2) ## RMSE first model RMSE second model ## 30.73546 40.72052 The first model has the lowest error. The \\(RMSE\\) can be interpreted as the average error in the metric of the dependent variable. For example, the first \\(RMSE\\) can be interpreted as having an average error of 30.74 points in mathematics. The \\(RMSE\\) has the benefit of penalizing large errors more than small errors. In other words, taking the square root of a big number penalizes this large number more than taking the same square root of a small number. Let’s show an example. The result of sqrt(1000) is 31.62 whereas the result for sqrt(100) is 10. The difference in the decrease is not proportional to the difference between 1000 and 100. The difference between 1000 and 100 is much larger than the difference between 31.62 and 10 because sqrt penalizes more the number 1000. This property makes the \\(RMSE\\) attractive to work with when you don’t care about the proportion of the errors. In the case of our math_score variable, this is not desirable since having a \\(RMSE\\) of 30 is twice as worse as having a \\(RMSE\\) of 15. It doesn’t make sense to penalize large errors more in test scores. Whenever the scale of the dependent variable does reflects this, it might be worth exploring other loss functions, such as the \\(MAE\\). 4.1.2 Mean Absolute Error The Mean Absolute Error or \\(MAE\\) is very similar to the \\(RMSE\\). It calculates the average error in the same metric as the dependent variable. However, instead of squaring the difference and taking the square root, it takes the absolute value of the difference: \\[ MAE = \\sum_{i = 1}^n{\\frac{|\\hat{y_{i}} - y_{i}|}{N}} \\] This approach doesn’t penalize any values and just takes the absolute error of the predictions. Let’s calculate the \\(MAE\\) for our previous two models: mae1 &lt;- res1 %&gt;% predict_training() %&gt;% mae(math_score, .pred) %&gt;% pull(.estimate) mae2 &lt;- res2 %&gt;% predict_training() %&gt;% mae(math_score, .pred) %&gt;% pull(.estimate) c(&quot;MAE first model&quot; = mae1, &quot;MAE second model&quot; = mae2) ## MAE first model MAE second model ## 24.42091 32.21999 The \\(MAE\\) is fundamentally simpler to interpret than the \\(RMSE\\) since it’s just the average absolute error. The main difference with respect to the \\(RMSE\\) is that it does not penalize larger errors but rather allows each error to have it’s absolute value. 4.2 Binary loss functions Binary loss functions are more elaborate than the ones discussed in the continuous loss function section. In particular, it introduces many terms which social scientists are rarely aware of. The pillar of these new terms is the confusion matrix, the topic we’ll be discussing next. 4.2.1 Confusion Matrices A confusion matrix is a very simple concept. It is the table comparing the predicted category of a model versus the actual category. Let’s work out an example. The city of Berlin is working on developing an ‘early warning’ system that is aimed at predicting whether a family is in need of childcare support. They are trying to build this system on a dataset they’ve been collecting for the past 10 years where they have all families in the city. Families which received childcare support are flagged with a 1 and families which didn’t received childcare support are flagged with a 0. Below is a figure showing how this fake data would look like: The column DV is whether a family has received childcare support and X1 and X2 are independent variables that are assumed to be correlated with receiving childcare support. Suppose we fit a logistic regression that returns a predicted probability for each family: For simplicity, we could assign a 1 to every respondent who has a probability above 0.5 and a 0 to every respondent with a probability below 0.5: Confusion matrices are two-way tables between the real category (this is the column DV in the data) and predicted category (this is the column Pred class in the data): On the left axis, we have the predicted values of whether a family received childcare support (a 1 or a 0) and on the top axis of the table the true value of whether a family received childcare. This template is easy enough to calculate with our own data. Let’s do that using the labels of each cell (lable (A), (B), etc…): Let’s focus on the top row. In cell (A), three respondents received childcare support that were predicted correctly by the logistic regression. These are rows 1, 5 and 7 from the data. Cell (A) corresponds to those that had a 1 and got predicted a 1. To the right side of this cell, cell (B) has those that didn’t receive childcare support but the model predicted they did. That is, they had a 0 in the dependent variable but the model predicted a 1. These were rows 2 and 6 from the data. For the bottom row, cell (C) contains those which received childcare support but the model predicted they didn’t. This is only one respondent from our data, namely row 4. Finally, cell (D) contains those which didn’t receive childcare support and the model predicted accurately that they didn’t receive childcare support. This corresponds only to row 3. These cells in the confusion matrix are used to generate several loss functions. The most standard is the accuracy of the model. The accuracy is the sum of all correctly predicted rows divided by the total number of predictions: In our example, the correct predictions are in cell (A) and cell (B). Cell (A) contains the number of families with childcare support that were assigned childcare support successfully. Cell (B) contains the number of families without childcare support that were correctly predicted to not have childcare support. We can calculate the accuracy as the sum of these two cells divided by all cells: Accuracy: \\((3 + 1) / (3 + 1 + 2 + 2) = 50\\%\\) If you randomly picked a person from this dataset, you can be 50% confident that you can predict whether they received childcare support. This is not a good accuracy. By chance alone, we could also achieve a predicttion accuracy of 50% if we made repeated predictions. That is why in certain scenarios, the sensitivity and specificty of a model is often a more important loss function. The sensitivity of a model is a fancy name for the true positive rate. For the accuracy calculations we compared those that were correctly predicted (regardless of whether they received childcare support or whether they didn’t). Sensitivity measures the same thing but only focused on the true positive rate: That is, we take those which were predicted childcare support correctly and divide them by the total number of respondets with childcare support. In substantive terms, it can be interpreted as: out of all the people who received childcare support, how many were classified correctly? With our small confusion matrix we can calculate it manually: Sensitivity: \\(3 / (3 + 1) = 75\\%\\) This true positive rate of 75% is substantially higher than the 50% of the accuracy. However, we’re measuring different things. The sensitivity calculation completely ignores the accuracy of those which didn’t receive childcare support. For that, the opposite calculation is called specificity. The specificity of a model measures the true false rate. That is, out of all the respondents who didn’t receive childcare, how many were classified correctly? Our confusion matrix highlights the two corresponding cells: We know that the cell (D) contains those which were correctly assigned no childcare support and cell (B) contains those which were incorrectly assigned childcare support when in fact they didn’t have it. We can calculate specificity the same way we did sensitivity: Specificity: \\(1 / (1 + 2) = 33\\%\\) The model has a 33% accuracy for those who didn’t receive childcare support. In other words, if you randomly picked someone who didn’t receive childcare support, you can be 33% confident that you can predict their childcare support status. The model is biased towards predicting better childcare support and negatively biased towards those that didn’t receive childcare support. But this is ok, as long as your research question is aligned with your loss function. For example, it might be more costly in terms of time and money for the city of Berlin to fail to give childcare support to a family that needs it and thus we would prefer to improve the sensitivity of this model. Giving childcare support to a family that doesn’t need it is better than not giving childcare support to a family that needs it. This discussion is just to exemplify that having poor accuracy or poor measures of other types of loss functions depends on your research question rather than the actual value. Although the previous exercises seemed easy enough, they are contingent on one assumption: that the cutoff value in the probabilities is 0.5. Remember that we assumed that everyone who had a probability above 50% would be assigned a 1 and otherwise a 0. That’s a strong assumption. Perhaps the real cutoff should be 0.65. Or it should be something more conservative, such as 0.40. To address this problem, we have to look at the sensitivity and specificity for different thresholds. This is where the ROC curve comes in. 4.2.2 ROC Curves and Area Under the Curve The ROC curve (ROC means Receiver Operating Characteristic Curve) is just another fancy name for something the is just a representation of sensitivity and specificity. Let’s work out a manual example. Suppose we have the same example of trying to predict whether a family will need childcare support. In that example, we calculated the sensitivity and specificity but assuming that the thresholdfor being 1 in the probability of each respondent is 0.5. We could assess how much the sensitivity and specificity of the model changes by changing this cutoff to 0.3 and 0.7: # Create some very fake data for childcare_support # Do not interpret this as real!! childcare_support &lt;- USArrests %&gt;% mutate(dv = factor(ifelse(Rape &gt;= mean(Rape), 1, 0))) %&gt;% as_tibble() %&gt;% select(dv, everything(), -Rape) names(childcare_support) &lt;- c(&quot;dv&quot;, paste0(&quot;X&quot;, 1:3)) # Define the tidyflow with the logistic regression tflow &lt;- childcare_support %&gt;% tidyflow(seed = 23151) %&gt;% plug_split(initial_split) %&gt;% plug_formula(dv ~ .) %&gt;% plug_model(set_engine(logistic_reg(), &quot;glm&quot;)) # Run the model res &lt;- tflow %&gt;% fit() # Get the probabilities res1 &lt;- res %&gt;% predict_training(type = &quot;prob&quot;) # Calculate the sensitivity and specificty # of different thresholds all_loss &lt;- lapply(c(0.3, 0.5, 0.7), function(x) { res &lt;- res1 %&gt;% mutate(pred = factor(as.numeric(.pred_1 &gt;= x))) sens &lt;- sensitivity(res, dv, pred) speci &lt;- specificity(res, dv, pred) data.frame(cutoff = x, sensitivity = round(sens$.estimate, 2), specificity = round(speci$.estimate, 2)) }) res_loss &lt;- do.call(rbind, all_loss) res_loss ## cutoff sensitivity specificity ## 1 0.3 0.74 0.87 ## 2 0.5 0.87 0.80 ## 3 0.7 0.96 0.53 The result contains the corresponding sensitivity and specificity when assigning a 1 or a 0 based on different cutoffs. For example, assigning a 1 if the probability was above 0.3 is associated with a true positive rate (sensitivity) of 0.74. In contrast, switching the cutoff to 0.7, increases the true positive rate to 0.95, quite an impressive benchmark. This means that if we randomly picked someone who received childcare support, our model can accurately predict 95% of the time whether they did receive childcare support. However, at the expense of increasing sensitivity, the true false rate decreases from 0.87 to 0.53. It seems that we need to find the most optimal value between these two accuracy measures. We want a cutoff that maximizes both the true positive rate and true false rate. For that, we need to try many different values from 0 to 1 and calculate the sensitivity and specificity: all_loss &lt;- lapply(seq(0.01, 0.99, by = 0.01), function(x) { res &lt;- res1 %&gt;% mutate(pred = factor(as.numeric(.pred_1 &gt;= x))) sens &lt;- sensitivity(res, dv, pred) speci &lt;- specificity(res, dv, pred) data.frame(cutoff = x, sensitivity = sens$.estimate, specificity = speci$.estimate) }) res_loss &lt;- do.call(rbind, all_loss) res_loss ## cutoff sensitivity specificity ## 1 0.01 0.2608696 1.00000000 ## 2 0.02 0.3478261 1.00000000 ## 3 0.03 0.3913043 1.00000000 ## 4 0.04 0.3913043 1.00000000 ## 5 0.05 0.3913043 1.00000000 ## 6 0.06 0.3913043 1.00000000 ## 7 0.07 0.3913043 1.00000000 ## 8 0.08 0.3913043 1.00000000 ## 9 0.09 0.4347826 1.00000000 ## 10 0.10 0.5217391 0.93333333 ## 11 0.11 0.5652174 0.93333333 ## 12 0.12 0.5652174 0.93333333 ## 13 0.13 0.6086957 0.93333333 ## 14 0.14 0.6086957 0.93333333 ## 15 0.15 0.6086957 0.93333333 ## 16 0.16 0.6086957 0.93333333 ## 17 0.17 0.6086957 0.93333333 ## 18 0.18 0.6086957 0.93333333 ## 19 0.19 0.6521739 0.93333333 ## 20 0.20 0.6521739 0.86666667 ## 21 0.21 0.6521739 0.86666667 ## 22 0.22 0.6521739 0.86666667 ## 23 0.23 0.6956522 0.86666667 ## 24 0.24 0.6956522 0.86666667 ## 25 0.25 0.7391304 0.86666667 ## 26 0.26 0.7391304 0.86666667 ## 27 0.27 0.7391304 0.86666667 ## 28 0.28 0.7391304 0.86666667 ## 29 0.29 0.7391304 0.86666667 ## 30 0.30 0.7391304 0.86666667 ## 31 0.31 0.7391304 0.86666667 ## 32 0.32 0.7391304 0.86666667 ## 33 0.33 0.7391304 0.86666667 ## 34 0.34 0.7391304 0.86666667 ## 35 0.35 0.7391304 0.86666667 ## 36 0.36 0.7391304 0.80000000 ## 37 0.37 0.7391304 0.80000000 ## 38 0.38 0.7391304 0.80000000 ## 39 0.39 0.7391304 0.80000000 ## 40 0.40 0.7826087 0.80000000 ## 41 0.41 0.7826087 0.80000000 ## 42 0.42 0.7826087 0.80000000 ## 43 0.43 0.8260870 0.80000000 ## 44 0.44 0.8260870 0.80000000 ## 45 0.45 0.8695652 0.80000000 ## 46 0.46 0.8695652 0.80000000 ## 47 0.47 0.8695652 0.80000000 ## 48 0.48 0.8695652 0.80000000 ## 49 0.49 0.8695652 0.80000000 ## 50 0.50 0.8695652 0.80000000 ## 51 0.51 0.8695652 0.80000000 ## 52 0.52 0.8695652 0.73333333 ## 53 0.53 0.8695652 0.73333333 ## 54 0.54 0.9130435 0.73333333 ## 55 0.55 0.9130435 0.73333333 ## 56 0.56 0.9130435 0.73333333 ## 57 0.57 0.9130435 0.73333333 ## 58 0.58 0.9130435 0.73333333 ## 59 0.59 0.9130435 0.73333333 ## 60 0.60 0.9130435 0.73333333 ## 61 0.61 0.9130435 0.73333333 ## 62 0.62 0.9130435 0.73333333 ## 63 0.63 0.9130435 0.73333333 ## 64 0.64 0.9130435 0.60000000 ## 65 0.65 0.9565217 0.60000000 ## 66 0.66 0.9565217 0.60000000 ## 67 0.67 0.9565217 0.60000000 ## 68 0.68 0.9565217 0.60000000 ## 69 0.69 0.9565217 0.60000000 ## 70 0.70 0.9565217 0.53333333 ## 71 0.71 1.0000000 0.53333333 ## 72 0.72 1.0000000 0.53333333 ## 73 0.73 1.0000000 0.53333333 ## 74 0.74 1.0000000 0.53333333 ## 75 0.75 1.0000000 0.53333333 ## 76 0.76 1.0000000 0.53333333 ## 77 0.77 1.0000000 0.53333333 ## 78 0.78 1.0000000 0.53333333 ## 79 0.79 1.0000000 0.53333333 ## 80 0.80 1.0000000 0.53333333 ## 81 0.81 1.0000000 0.53333333 ## 82 0.82 1.0000000 0.53333333 ## 83 0.83 1.0000000 0.53333333 ## 84 0.84 1.0000000 0.53333333 ## 85 0.85 1.0000000 0.53333333 ## 86 0.86 1.0000000 0.46666667 ## 87 0.87 1.0000000 0.46666667 ## 88 0.88 1.0000000 0.46666667 ## 89 0.89 1.0000000 0.40000000 ## 90 0.90 1.0000000 0.40000000 ## 91 0.91 1.0000000 0.40000000 ## 92 0.92 1.0000000 0.40000000 ## 93 0.93 1.0000000 0.33333333 ## 94 0.94 1.0000000 0.26666667 ## 95 0.95 1.0000000 0.20000000 ## 96 0.96 1.0000000 0.13333333 ## 97 0.97 1.0000000 0.13333333 ## 98 0.98 1.0000000 0.06666667 ## 99 0.99 1.0000000 0.06666667 This result contains the sensitivity and specificity for many different cutoff points. These results are most easy to understand by visualizing them: res_loss %&gt;% ggplot(aes(specificity, sensitivity)) + geom_point() + theme_minimal() On the X axis we have the true false rate (specificity) and on the Y axis we have the true positive rate (sensitivity). As expected, there is a trade off between the two. That is, cutoffs that improve the specificity does so at the expense of sensitivity. This is evident from the negative correlation in the scatterplot. Instead of visualizing the specificity as the true negative rate, let’s subtract 1 such that as the X axis increases, it means that the error is increasing: res_loss %&gt;% ggplot(aes(1 - specificity, sensitivity)) + geom_point() + theme_minimal() The ideal result from this plot is that most points cluster on the top left quadrant. This would mean that the sensitivity is high (the true positive rate) and the specificity is high (because \\(1 - specificity\\) will switch the direction of the accuracy to the lower values of the X axis). Let’s add two lines to the plot. One that joins all the dots, such that it’s clearer where the imprevements are and a diagonal line: res_loss %&gt;% ggplot(aes(1 - specificity, sensitivity)) + geom_point() + geom_line() + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + scale_y_continuous(limits = c(0, 1)) + theme_minimal() Most statistical software will calculate this plot for you, but you just calculated it manually! These plots and statistics are sometimes treated as mystified but by writing them down step by step you can truly understand the intuition behind them. Let’s allow R to calculate this for us automatically: p1 &lt;- res1 %&gt;% roc_curve(dv, .pred_1) %&gt;% mutate(.threshold = round(.threshold, 2)) %&gt;% ggplot(aes(1 - specificity, sensitivity)) + geom_line() + geom_point(aes(text = paste0(&quot;Cutoff: &quot;, .threshold)), alpha = 0) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + theme_minimal() p1 Although we built this plot from scratch, there is one thing we’re missing: the actual cutoff points! Here we’re visualizing the sensitivity and specificity of each cutoff point but it’s not entirely intuitive because we cannot see these cutoff values. Let’s make that plot interactive: ggplotly(p1) You can now hover over the line and check the actual cutoff point associated with each sensitivity and specificity. Let’s suppose that for us, it’s more important to have a high sensitivity than a higher specificity. However, we want a specificity rate of at least 80%. The cutoff 0.44 offers both a sensitivity of 86% and a \\(1 - specificity\\) of 20%, which translates to a specificity of 80%. Although we’ve chosen this threshold from a very manual approach, there are formulas that already choose the most optimal combination for you. Due to the lack of time in the course, we won’t be covering them but feel free to look them up. One example is the Youden Index. The last loss function we’ll discuss is a very small extension of the ROC curve. It’s called the Area Under the Curve or \\(AUC\\). As the name describes it, the \\(AUC\\) is the percentage of the plot that is under the curve. For example: p1 + geom_ribbon(aes(ymax = sensitivity, ymin = 0), fill = &quot;red&quot;, alpha = 1/6) As we described earlier, as the more points are located in the top left quadrant, the higher the overall accuracy of our model. If the points are very close to the top left quadrant, the area under the curve will be higher. As expected, higher \\(AUC\\) values reflect increasing accuracy. We can let R workout the details with our predictions: res1 %&gt;% roc_auc(dv, .pred_1) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.904 It seems that 90% of the space of the plot is under the curve. "],
["unsupervised-methods.html", "Chapter 5 Unsupervised methods 5.1 Principal Component Analysis (PCA) 5.2 K-Means Clustering 5.3 Hierarchical Clustering 5.4 Exercises", " Chapter 5 Unsupervised methods Unsupervised learning is a very popular concept in machine learning. Although we social scientists are aware of some of these methods, we do not take advantage of them as much as machine learning practitioners. What is unsupervised learning? Let’s get that out of the way: this just means that a particular statistical method does not have a dependent variable. These are models that look to find relationships between independent variables without the need of a dependent variable. One common unsupervised method that social scientists are aware of is the Principal Component Analysis or \\(PCA\\). \\(PCA\\) aims to summarize many variables into a small subset of variables that can capture the greatest variance out of all the main variables. We really never thought about this as an ‘unsupervised’ method, but it is used widely for predictive tasks. Before we begin, let’s load the packages and data we’ll be using. library(tidymodels) library(tidyflow) library(ggfortify) data_link &lt;- &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot; pisa &lt;- read.csv(data_link) Most of the material on this chapter was built upon Boehmke and Greenwell (2019) and James et al. (2013). In particular, the section on K-Means clustering contains images directly copied from James et al. (2013). 5.1 Principal Component Analysis (PCA) Principal Component Analysis or \\(PCA\\) is a method that tries to summarize many columns into a very small subset that captures the greatest variability of the original columns. Social Scientists often use this method to create more ‘parsimonious’ models and summarize many variables into a few ‘strong’ variables. \\(PCA\\) works by creating several components which are the normalized linear combination of the variables in the model. In the pisa data there are a six variables which asks the student whether they’ve suffered negative behavior from their friends in the past 12 months. In particular, it asks whether Other students left them out of things on purpose Other students made fun of them They were threatened by other students Other students took away or destroyed things that belonged to them They got hit or pushed around by other students Other students spread nasty rumours about them For each of these variables, the scale ranges from 1 to 4 where 4 is ‘Once a week or more’ and 1 is ‘Never or almost never’. In other words, the higher the number, the more negative their response. Let’s rename these variables into more interpretable names and look at their correlation: pisa &lt;- pisa %&gt;% rename( past12_left_out = ST038Q03NA, past12_madefun_of_me = ST038Q04NA, past12_threatened = ST038Q05NA, past12_destroyed_personal = ST038Q06NA, past12_got_hit = ST038Q07NA, past12_spread_rumours = ST038Q08NA ) pisa_selected &lt;- pisa %&gt;% select(starts_with(&quot;past12&quot;)) cor(pisa_selected) ## past12_left_out past12_madefun_of_me ## past12_left_out 1.0000000 0.6073982 ## past12_madefun_of_me 0.6073982 1.0000000 ## past12_threatened 0.4454125 0.4712083 ## past12_destroyed_personal 0.4037351 0.4165931 ## past12_got_hit 0.3918129 0.4480862 ## past12_spread_rumours 0.4746302 0.5069299 ## past12_threatened past12_destroyed_personal ## past12_left_out 0.4454125 0.4037351 ## past12_madefun_of_me 0.4712083 0.4165931 ## past12_threatened 1.0000000 0.5685773 ## past12_destroyed_personal 0.5685773 1.0000000 ## past12_got_hit 0.5807617 0.6206485 ## past12_spread_rumours 0.5513099 0.4543380 ## past12_got_hit past12_spread_rumours ## past12_left_out 0.3918129 0.4746302 ## past12_madefun_of_me 0.4480862 0.5069299 ## past12_threatened 0.5807617 0.5513099 ## past12_destroyed_personal 0.6206485 0.4543380 ## past12_got_hit 1.0000000 0.4451408 ## past12_spread_rumours 0.4451408 1.0000000 Most correlations lie between 0.4 and 0.6, a somewhat acceptable threshold for assesing whether they can be reduced into fewer variables. \\(PCA\\) works by receiving as input \\(P\\) variables (in this case six) and calculating the normalized linear combination of the \\(P\\) variables. This new variable is the linear combination of the six variables that captures the greatest variance out of all of them. \\(PCA\\) continues to calculate other normalized linear combinations but with the constraint that they need to be completely uncorrelated to all the other normalized linear combinations. This approach has the advantage that it constructs as many principal components (new variables) as it can, as long as they all capture 100% of the variability of the original \\(P\\) variables, and each of these new variables are completely uncorrelated between each other. Each variable is assessed by how much variance it explains of the original \\(P\\) variables and each new variable is completely independent of the others. Depending on the correlation of the \\(P\\) input variables, you might get three principal components that capture all of the variability of the original \\(P\\) variables. In other cases, you can get an you might many more. This discussion is getting too theoretical. Let’s get get some hands-on experience of how this works. Let’s pass in our six variables to the function prcomp, which estimates these principal components based on our six variables. However, for \\(PCA\\) to work well, we need to center and scale the independent variables. This means that the independent variables will have a mean of zero and a standard deviation of one. prcomp does this for you, but you should be aware of this for future discussion: pc &lt;- prcomp(pisa_selected) all_pcs &lt;- as.data.frame(pc$x) head(all_pcs) ## PC1 PC2 PC3 PC4 PC5 PC6 ## 1 -2.836172297 -0.7549602 -1.91065434 -0.232647114 -0.368981283 -1.885607656 ## 2 -1.478020766 0.6622561 0.94113153 0.181451711 0.149387648 0.678384471 ## 3 1.025953306 0.1602906 -0.03806864 -0.008994148 0.009439987 -0.002391996 ## 4 -0.002173173 -0.7902197 -0.10112894 -0.197389118 0.013521080 -0.002718289 ## 5 -4.832075955 0.1996595 0.39221922 -0.256660522 -1.178883084 0.150399629 ## 6 -1.132036976 -1.8534154 -0.68913950 0.914561923 0.065907346 0.087208533 Let’s explain what just happened. Our dataset pisa_selected contains the six variables of interest. We passed that to prcomp which calculated the principal components. With this model object, we extracted the dataframe with the new principal components. The result of all of this is a dataframe with six new columns. These six new columns are not the initial six variables from pisa_selected. Instead, they are variables that summarize the relationship of these six variables. You might ask yourself, how come six variables summarize six variables? That doesn’t make much sense. The whole idea is that fewer variables can summarize the original six. Let’s look at how much variance of the original \\(P\\) variables these ‘index’ variables explain: tidy(pc, &quot;pcs&quot;) ## # A tibble: 6 x 4 ## PC std.dev percent cumulative ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.34 0.591 0.591 ## 2 2 0.640 0.135 0.726 ## 3 3 0.530 0.0929 0.819 ## 4 4 0.522 0.0899 0.909 ## 5 5 0.394 0.0513 0.960 ## 6 6 0.347 0.0397 1 This output shows how well each principal component is explaining the original six variables. For example, the first principal component (1st row) explains about 59% of the variance of the six variables. The second principal component explains an additional 13.5%, for a total of 72.6% between the two. This is certainly better. It means that the first two variables seem to have some power in summarizing the original six variables. Let’s focus on the first two principal components. They are supposed to be completely uncorrelated, so let’s check that ourselves: cor(all_pcs[c(&quot;PC1&quot;, &quot;PC2&quot;)]) ## PC1 PC2 ## PC1 1.00000000000000000000 -0.00000000000001545012 ## PC2 -0.00000000000001545012 1.00000000000000000000 As expected, the correlation between these two variables is 0. How do we use these two variables? Well, a typical social scientist would make sure that their expected explanatory power of the two components is high enough for their research problem. If it is, they would include these two columns in their statistical models instead of the six variables. However, \\(PCA\\) is all about exploratory data analysis. We might want to go further and explore how the original six variables are related to these principal components. These two principal components are a bit of a black box at this point. Which variables do they represent? We can check that with the initial output of prcomp: pc$rotation[, 1:2] ## PC1 PC2 ## past12_left_out -0.4631946 -0.4189125 ## past12_madefun_of_me -0.5649319 -0.5315979 ## past12_threatened -0.3446963 0.4025682 ## past12_destroyed_personal -0.2694606 0.3405411 ## past12_got_hit -0.2987481 0.3715999 ## past12_spread_rumours -0.4308453 0.3546832 These two columns show the correlations between the six original variables and the first two principal components. Let’s focus on the first column. The first thing that stands out is that for all the six variables, the correlation is negative. This means that as the respondents answered negatively to the six questions, the first principal component decreases. Informally, we could call this variable a ‘negative-peer index’. Moving to the second column, four of these six variables correlate positively with the second principal component. At least for these four variables, the principal components tend capture the exact opposite relationship. In other words, at least for these four variables, this is a ‘positive-peer index’. This type of decomposition is precisely where the usefulness of this type of method comes in. It allows us to summarize many variables into a small set of components that capture meaningful variation. The package ggfortifty contains the function autoplot which can help us visualize these correlations in a more meaningful way: set.seed(6652) pc %&gt;% autoplot(loadings = TRUE, loadings.label = TRUE, loadings.label.repel = TRUE, alpha = 1/6) + theme_minimal() Let’s distill this plot. On the X axis we have the actual column of the first principal component (PC1) (this is literaly the same column we saw in the object all_pcs; if it serves to refresh your memory, check it out with head(all_pcs)). As you can see, the label of the X axis already tells us that this component explains nearly 60% of the variance of these six variables. On the Y axis we have the actual column of the second principal component (PC2) (same as before, you can see this with head(all_pcs)). This principal component explains an additional 13.5% of the variance of the six variables. What this plot is trying to show is where these six variables are clustered between these two principal components. Since these two variables were centered and scaled to have a mean of zero, the red lines always begin at the intersection of the zero in PC1 and PC2. In other words, we can see more clearly the correlations we saw earlier. For example, remember that the first two variables were both negatively correlated with both PC1 and PC2. These two variables are located in the bottom left of the plot, showing that for both principal components both variables are associated with lower values of PC1 and PC2: There is nothing new here. This is the same thing we interpreted from the correlation but from a more intuitive visualization. If you remember the other four variables from the correlation, they showed negative correlations with PC1 and positive correlations with PC2. This means that these variables should cluster below the average of PC1 and higher than the average of PC2. We can see that more clearly if we first add a line showing the zero values for both variables: Any values to the left of the the vertical line are low values of PC1 while all values above the horizontal line are high values for PC2. Building on this intuition, we should find that the remaining four variables cluster at lower values of PC1 and at higher values of PC1: Depending on these correlations, you might reject to focus on the first two principal components and explore this same plot for PC1 and PC3 or PC2 and PC4. There’s no clear cut rule for the number of principal components to use. The user should instead explore these plots to understand whether there are interesting findings from clustering many variables into fewer variables. Depending on this, you might reject the idea entirely of using principal components. Or you might use these principal components to represent some interesting findings for your theoretical model. In any case, this method is inherently exploratory. It serves as way to understand whether we can reduce correlated variables into a small subset of variables that represent them. For a social science point of view, this method is often used for reducing the number of variables. However, there is still room for using it as a clustering method to understand whether certain variables can help us summarize our understanding into simpler concepts. Having said this, for predictive tasks there is an objective measure on how many principal components to use: the ones that improve predictions the most. Using our previous example, we could perform a grid search on a number of components to see which one maximizes predictive accuracy. Let’s run a random forest by regressing the variable math_score on all variables in the dataset. While we do that, let’s try models with different number of principal components: # Set the number of components `num_comp` # to be tuned rcp &lt;- ~ recipe(.x, math_score ~ .) %&gt;% step_pca(starts_with(&quot;past12_&quot;), num_comp = tune()) tflow &lt;- pisa %&gt;% tidyflow(seed = 25131) %&gt;% plug_split(initial_split) %&gt;% plug_recipe(rcp) %&gt;% plug_model(set_engine(rand_forest(mode = &quot;regression&quot;), &quot;ranger&quot;)) %&gt;% plug_resample(vfold_cv) %&gt;% # Set `num_comp`in the grid to 1:3 # meaning that we&#39;ll try the models with # number of components 1, 2 and 3 plug_grid(expand.grid, num_comp = 1:3) res_rf &lt;- fit(tflow) res_rf %&gt;% pull_tflow_fit_tuning() %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) These are the average results of running a 10-fold cross-validation trying out models with one, two and three principal components respectively. As we can see from the mean column, there is little difference between the average \\(RMSE\\) of these different models. If there are important reasons to include these variables in the model and we want to reduce the number of variables in the model for simplicity, we could just keep the model with one principal component. However, there’s also an alternative approach. step_pca allows you to specify the minimum explanatory power of the principal components. As discussed in the documentation of step_pca, you specify the fraction of the total variance that should be covered by the components. For example, threshold = .75 means that step_pca should generate enough components to capture 75% of the variance. We can try our previous models with a 90% threshold. Since we will not perform a grid search, we will drop the grid and only keep the cross-validation to get uncertain estimates of our loss function \\(RMSE\\): # Define a new recipe # where threshold is .90 rcp &lt;- ~ recipe(.x, math_score ~ .) %&gt;% step_pca(starts_with(&quot;past12_&quot;), threshold = .90) # Replace the previous recipe # and drop the grid tflow &lt;- tflow %&gt;% replace_recipe(rcp) %&gt;% drop_grid() res_rf &lt;- fit(tflow) res_cv &lt;- res_rf %&gt;% pull_tflow_fit_tuning() %&gt;% collect_metrics() round(res_cv$mean[1], 2) res_cv This approach offers a very similar \\(RMSE\\) of MISSING CALCULATION. Althought not possible at this moment, tidymodels is expected to allow the threshold parameter to be tune such that you can perform a grid search of this value as well (for those interested, see here). Although \\(PCA\\) is a very useful method for summarizing information, it is based on the notion that the variables to be summarized are best summarized through a linear combination. In other instances, non-linear methods can also prove useful as exploratory means. 5.2 K-Means Clustering K-Means is a method for finding clusters in a dataset of \\(P\\) variables. It is somewhat different from \\(PCA\\) because it attemps to find non-overlapping clusters of respondents using \\(P\\) variables. In terms of interpretation and transparency, K-Means clustering is particularly useful for exploration in the social sciences. Suppose we have a scatterplot of two variables: The distribution of this scatterplot shows that there are at least two visible clusters, one in the top part of the plot and one in the bottom part of plot. How does K-Means identify clusters appropriately? First, it begins by randomly assigning each point a cluster. Let’s suppose we want to identify three clusters: Each point has now an associated color. However, these colors were randomly assigned. There’s no evident pattern from the colors and the position of each point. K-Means clustering works by creating something called ‘centroids’ which represent the center of the different clusters. The centroid is usually the mean of the \\(P\\) variables. For our simplified case, we calculate the average of X and Y for the color orange, then the average of X and Y for the color purple and then the average of X and Y for the color green. The end result of this is an average mean of X and Y for the three colors. For example, for the orange points, we can plot a ‘average big point’ that is located at the average of X and the average of Y only for the orange points. We repeat this for every color and plot it: Remember that we assigned the cluster colors to the points randomly? Then the mean of X and Y for the three different colors should be more or less the same. We find that’s the case, as the three ‘average big points’ are located at the center of the plot overlapping among each other. So far, everything that has happened is random. There’s no capturing of real knowledge here. The next step is where the substantive part comes in. We need to calculate something called the Euclidian distance between each point and the three centroids. This is not so difficult. Let’s work it out manually. Suppose that the three centroids are located with these X and Y values: centroids_df &lt;- data.frame(type = factor(c(&quot;orange&quot;, &quot;purple&quot;, &quot;green&quot;), levels = c(&quot;orange&quot;, &quot;purple&quot;, &quot;green&quot;)), x = c(.54, .56, .52), y = c(.553, .55, .56)) We can visualize them: centroids_df %&gt;% ggplot(aes(x, y, color = type)) + geom_point(size = 4) + scale_color_manual(values = c(&quot;orange&quot;, &quot;purple&quot;, &quot;green&quot;)) + lims(x = c(0, 1), y = c(0, 1)) + theme_minimal() Now suppose we add an additional random point in the plot: centroids_df %&gt;% ggplot(aes(x, y)) + geom_point(aes(color = type), size = 4) + geom_point(data = data.frame(x = 0.25, y = 0.75)) + scale_color_manual(values = c(&quot;orange&quot;, &quot;purple&quot;, &quot;green&quot;)) + lims(x = c(0, 1), y = c(0, 1)) + theme_minimal() How do we calculate the Euclidean distance between this point and the three clusters? We use this formula: \\[\\sqrt{(x_2 - x_1) + (y_2 - y_1)}\\] where \\(x_2\\) and \\(y_2\\) are the values for a particular centroid and \\(x_1\\) and \\(y_1\\) are the values for the random point. Here is the manual calculation for each centroid: Orange \\[\\sqrt{(0.54 - 0.25) + (0.553 - 0.75)} = 0.304959 \\] Purple \\[\\sqrt{(0.56 - 0.25) + (0.550 - 0.75)} = 0.3316625 \\] Green \\[\\sqrt{(0.52 - 0.25) + (0.560 - 0.75)} = 0.2828427 \\] Since each of these calculations is done on the random point and the three centroids, the only numbers that change between each formula is the location of the centroids. From these results, the random point is closest to the green centroid, as the distance is the smallest (0.28). Let’s assign it to that cluster: centroids_df %&gt;% ggplot(aes(x, y, color = type)) + geom_point(size = 4) + geom_point(data = data.frame(type = factor(&quot;green&quot;), x = 0.25, y = 0.75)) + scale_color_manual(values = c(&quot;orange&quot;, &quot;purple&quot;, &quot;green&quot;)) + lims(x = c(0, 1), y = c(0, 1)) + theme_minimal() The K-Means clustering algorithm applies this calculation for each point: Each point is now assigned the color of the closest centroid. The centroids are still positioned in the center, reflecting the random allocation of the initial points. However, we just reallocated the cluster of every single point. At this stage the K-Means clustering algorithm repeats the same process we just performed manually. It calculates new centroids based on the average of the X and Y of the newly new assigned points: The next step is to repeat exactly the same strategy again. That is: Calculate the distance between each point and all corresponding clusters Reassign all points to the cluster of the closest centroid Recalculate the centroid Mathematically, it can proved that after \\(N\\) iterations, each point will be allocated to a particular centroid and it will stop being reassigned: This approach looks to minimize within-cluster variance and maximize between-cluster variance. That is, respondents are very similar within each cluster with respect to the \\(P\\) variables and very different between clusters. Althought K-Means clustering is a very fast, interpretable and flexible method, it has important drawbacks. First, K-Means will always calculate the number of supplied clusters. That is, if the user supplies three clusters, it will calculate three clusters. If the user supplies, ten clusters, it will also calculate ten clusters. In other words, the clusters calculate by the K-Means algorithm should be interpreted as exploratory and be contrasted with a theoretical description of the problem at hand. The clusters need to make substantive sense rather than statistical sense. K-Means also has a stability problem. That is, it is completely dependent on the initial random assignment of the clusters. Remember how we assigned each point a random cluster in our manual example? If we repeated the same random assignment again, it is possible we get completely different clusters. For example, here’s a simulation using the same example as before: These six plots show the exact same model showed above but repeated six times with different random allocation for each one. In some instances, the result is very similar to our initial clusters (for example, the middle top panel) but for others it’s slightly different (for example, the top left panel and the bottom right panel). This does not mean that the method is useless. It can be very useful to determine clusters whenever the latent distribution of the variables really reflect a clustering. However, instead of taking the clusters at face value, the result of K-Means should be used as an exploratory tool that needs to be compared and complemented with other types of analysis. These methods need to offer evidence of the validity of the clusters as well the robustness of the clusters. Clustering methods have in general some problems which are important to address. For example, in some instances, centering and scaling variables might be more appropriate, and this can have important implications for the resulting clusters. In addition, outliers can have a big impact on the cluster assignment in general. In addition, small changes in the data can have big impacts on the final clusters. You might ask yourelf, how can we fit this in R? Let’s suppose that we have reasons to believe that there are different clusters between the socio-economic status of a family and a student’s expected socio-economic status. For example, we might argue that students from low socio-economic status might not have great aspirations, students from middle socio-economic status have average aspirations while students from high socio-economic status might have great aspirations. The function kmeans is used to calculate the clusters. It accepts two arguments: the dataframe with the \\(P\\) columns and how many clusters the user wants. Let’s pass that to kmeans and visualize the clusters res &lt;- pisa %&gt;% select(ESCS, BSMJ) %&gt;% kmeans(centers = 3) pisa$clust &lt;- factor(res$cluster, levels = 1:3, ordered = TRUE) pisa %&gt;% ggplot(aes(ESCS, BSMJ, color = clust)) + geom_point(alpha = 1/3) + scale_x_continuous(&quot;Index of economic, social and cultural status of family&quot;) + scale_y_continuous(&quot;Students expected occupational status&quot;) + theme_minimal() Substantively, this example might not make a lot of sense, but it serves to exemplify that the K-Means can find clusters even when there aren’t any clusters. From this particular plot, it doesn’t seem to be a clear cut distinction between the three clusters. We can inspect the actual values of the centrois, for example: res$centers ## ESCS BSMJ ## 1 0.01219057 55.53109 ## 2 0.12278091 75.46021 ## 3 -0.29322474 28.82504 Althought some people might think that these methods have no value in the social sciences, I like to think this is because they’ve been trained with a particular hammer, and thus every problem seems like a nail. We don’t seem problems in a way that can be answered with these techniques because we don’t think about problems with these type of approaches. For example, social scientists working on labor market and technology might want to try to understand why companies cluster into certain cities. K-Means might be a first step towards understanding the variables that discriminate where different types of companies cluster. A traditional social scientist might think of answering these questions by feeding all variables into a linear model but this defies the whole purpose of clustering: there’s no need for a dependent variable. We can understand the relationship between the variables as a first step towards understanding clustering patterns. These are questions that require creativity from the social science discipline because we’ve been trained in a particular paradigm that is difficult to break. 5.3 Hierarchical Clustering dt &lt;- USArrests dt %&gt;% ggplot(aes(Assault, Rape)) + geom_point() plot(hclust(dist(dt[c(&quot;Assault&quot;, &quot;Rape&quot;)]))) 5.4 Exercises In 2016, the Data Science website Kaggle published a dataset which combines the rankings of more than 2000 universities. This data (which comes from https://www.kaggle.com/mylesoneill/world-university-rankings/data) contains the name of each university, together with their world ranking, the country where they’re located and several other variables that show the respective ranking in each area. The respective columns are: institution: the name of the university world_rank: the overall ranking of the university country: the country where the university is located national_rank: the ranking of the university within each country quality_of_education: the ranking in quality of education alumni_employment: the ranking in employment for their students quality_of_faculty: the ranking of quality of the faculty publications: the ranking in the number of publications influence: the ranking in the overall influence citations: the ranking in the overall number of citations patents: the ranking in the number of patents year: the year of each ranking We can read in the data with the code below: dt_all &lt;- read.csv(&quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/university_ranking_final.csv&quot;) # Have a look with: head(dt_all) ## institution world_rank country national_rank ## 1 Harvard University 1 USA 1 ## 2 Massachusetts Institute of Technology 2 USA 2 ## 3 Stanford University 3 USA 3 ## 4 University of Cambridge 4 United Kingdom 1 ## 5 California Institute of Technology 5 USA 4 ## 6 Princeton University 6 USA 5 ## quality_of_education alumni_employment quality_of_faculty publications ## 1 7 9 1 1 ## 2 9 17 3 12 ## 3 17 11 5 4 ## 4 10 24 4 16 ## 5 2 29 7 37 ## 6 8 14 2 53 ## influence citations patents year ## 1 1 1 5 2012 ## 2 4 4 1 2012 ## 3 2 2 15 2012 ## 4 16 11 50 2012 ## 5 22 22 18 2012 ## 6 33 26 101 2012 1. Explore whether there is room for reducing the number of variables Many of these ranking variables (except world_rank) should be very correlated. It could be that universities that score well in quality of faculty, they also score high on the ranking of publications. Explore these ranking variables by using a Principal Component Approach: Limit the data to year 2015 Explore the correlation of the variables quality_of_education, alumni_employment, quality_of_faculty, publications, influence, citations and patents with the function cor. What is the overall conclusion of the correlation? Are these variables weakly or strongly correlated? &gt; Answer The correlations are quite high for some variables (for example a .84 correlation between publication and influence and a .80 between influence and citation) while lower for others (correlation of .39 between alumni employment and quality of faculty). All in all, these ranking variables seem to correlated to a reasonable degree together. dt &lt;- dt_all %&gt;% filter(year == 2015) dt %&gt;% select(-world_rank, -institution, -country, -national_rank, -year) %&gt;% cor() ## quality_of_education alumni_employment quality_of_faculty ## quality_of_education 1.0000000 0.4736529 0.6859886 ## alumni_employment 0.4736529 1.0000000 0.3906046 ## quality_of_faculty 0.6859886 0.3906046 1.0000000 ## publications 0.5213777 0.4767965 0.5371659 ## influence 0.5495743 0.4182182 0.5704868 ## citations 0.5210670 0.4491194 0.5599886 ## patents 0.3866523 0.3980353 0.4180885 ## publications influence citations patents ## quality_of_education 0.5213777 0.5495743 0.5210670 0.3866523 ## alumni_employment 0.4767965 0.4182182 0.4491194 0.3980353 ## quality_of_faculty 0.5371659 0.5704868 0.5599886 0.4180885 ## publications 1.0000000 0.8451104 0.7888855 0.6169113 ## influence 0.8451104 1.0000000 0.8089169 0.5486543 ## citations 0.7888855 0.8089169 1.0000000 0.5181444 ## patents 0.6169113 0.5486543 0.5181444 1.0000000 2. Perform a PCA on the ranking variables Exclude the columns world_rank, institution, country, national_rank, year from the data Pass the ranking columns to prcomp and set scale = TRUE and center = TRUE to normalize the data Use tidy with \"pcs\" to explore the contribution of explained variance by each principal component Explore what each principal component measures with respect to the original seven variables. This can be done by accessing the object rotation with the $ of the result of the prcomp With the package ggfortify, use the function autoplot to plot the principal components How many principal componenets were created? How much variance do the first two and three components explain? What does PC1 measure? What does PC2 measure? &gt; Answer Seven principal components were created by prcomp. This is quite a high number, considering that provided seven variables. The first two components, as expected, explain the majority of the variance of the original six variables with a total of 73%. As PC1 increases, so does the rankings of each institution. This means that higher values of PC1 will reflect university with poorer rankings (because 1st is the best university and 2000 is the worst university within the ranking). As PC2 increases, publications, influence, citations and patents decrease (meaning better university rankings) while the remaining four increase. Overall, there doesn’t seem to be great consistency between the principal components and the variables. res_pc &lt;- dt %&gt;% select(-world_rank, -institution, -country, -national_rank, -year) %&gt;% prcomp(scale = TRUE, center = TRUE) tidy(res_pc, &quot;pcs&quot;) ## # A tibble: 7 x 4 ## PC std.dev percent cumulative ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2.08 0.619 0.619 ## 2 2 0.897 0.115 0.734 ## 3 3 0.823 0.0967 0.830 ## 4 4 0.728 0.0758 0.906 ## 5 5 0.551 0.0434 0.950 ## 6 6 0.457 0.0299 0.979 ## 7 7 0.380 0.0206 1 res_pc$rotation ## PC1 PC2 PC3 PC4 PC5 ## quality_of_education 0.3587429 0.5760826 0.16579182 -0.1095131 0.69777431 ## alumni_employment 0.3040826 0.3183726 -0.82359376 0.3051580 -0.16374091 ## quality_of_faculty 0.3629945 0.4588274 0.35704392 -0.2484045 -0.67321210 ## publications 0.4264696 -0.3208572 0.03655215 0.1558644 0.08695217 ## influence 0.4240042 -0.2722786 0.20768186 0.2411395 0.09517359 ## citations 0.4146066 -0.2442876 0.15335882 0.3319024 -0.11934581 ## patents 0.3369240 -0.3456641 -0.31422808 -0.8003621 0.04715163 ## PC6 PC7 ## quality_of_education -0.10366608 0.04813140 ## alumni_employment 0.04386293 -0.07752131 ## quality_of_faculty 0.12067837 0.02737664 ## publications 0.48290226 0.66991734 ## influence 0.33664464 -0.72278433 ## citations -0.77984564 0.11110425 ## patents -0.13447189 -0.08587055 autoplot(res_pc, loadings = TRUE, loadings.label = TRUE, loadings.label.repel = TRUE, alpha = 1/6) + theme_minimal() 3. Apply the kmeans to the set of ranking variables Exclude the columns world_rank, institution, country, national_rank, year from the data Calculate the average quality_of_education and alumni_employment by country Pass these country average ranking variables to kmeans Visualize the clusters in a scatterplot of quality_of_education and alumni_employment Try several centers. Is there a substantive cluster among these countries? &gt; Answer Some of these clusters don’t seem to captury substantial differences. However, the top-left group of countries seems to cluster continually with different centroids. This cluster is continually composed of countries such as France, Spain, Egypt, Argentina, etc… Do we have reasons to believe that they are very similar in quality_of_education and alumno_employment? An extension of this could hypothesize that we should collect information in terms of growth of labor market and population of students in order to check whether they also cluster on these variables. ############################# Three clusters ################################## ############################################################################### sum_dt &lt;- dt %&gt;% select(-world_rank, -institution, -national_rank, -year) %&gt;% group_by(country) %&gt;% summarize_all(mean) set.seed(523131) res_km &lt;- sum_dt %&gt;% select(quality_of_education, alumni_employment) %&gt;% kmeans(centers = 3, nstart = 50) sum_dt$.cluster_three &lt;- factor(res_km$cluster, levels = 1:3) sum_dt %&gt;% ggplot(aes(quality_of_education, alumni_employment, color = .cluster_three)) + geom_text(aes(label = country)) + theme_minimal() ############################# Two clusters #################################### ############################################################################### set.seed(523131) res_km &lt;- sum_dt %&gt;% select(quality_of_education, alumni_employment) %&gt;% kmeans(centers = 2, nstart = 50) sum_dt$.cluster_two &lt;- factor(res_km$cluster, levels = 1:2) sum_dt %&gt;% ggplot(aes(quality_of_education, alumni_employment, color = .cluster_two)) + geom_text(aes(label = country)) + theme_minimal() References "],
["no-free-lunch.html", "Chapter 6 No free lunch 6.1 Causal Inference 6.2 Explanaining complex models 6.3 Inference 6.4 Prediction 6.5 Prediction challenge", " Chapter 6 No free lunch Throughout this course we’ve explained several different methods that are used in machine learning for predictive problems. Although we presented the benefits and pitfalls of each one when possible, there’s no clear cut rule on which one to use. The ‘No free lunch’ theorem is a simple axiom that states that since every predictive algorithm has different assumptions, no single model is known to perform better than all others a priori. In other words, machine learning practitioners need to try different models to check which one predicts better for their task. However, since social scientists are not only interested in predictive accuracy, for different scenarios this might be different. Let’s discuss some of these scenarios. 6.1 Causal Inference There is growing interest from the social science literature on achieving causal inference using tree-based methods (Athey and Imbens 2016). By definition, this type of analysis is not interested in predictive accuracy alone. This means that we would not try several different models and test each one against their predictive accuracy. Instead, we need to carefully understand how tree-based methods work and how they can help us estimate a causal effect. For this type of area, machine learning servers as a tool for exploration and for estimation of point estimates of causal effects rather than for predictive accuracy. 6.2 Explanaining complex models In business settings, there are scenarios where interpretability is often needed more than accuracy. This is also the case for the social sciences. For example, explaining a complex model to key stakeholders can be challenging. It is sometimes better to have a simple model that performs worse than more complex model at the expense of interpretability. I’ve experienced situations like this one where we used simple decision trees that performed worse than other tree-based methods simply because it was much more important that the stakeholder understand how we achieved at a final prediction and which variables were the most important ones. 6.3 Inference For social scientists, we can use machine learning methods for exploring hypothesis in the data. In particular, tree-based methods and regularized regressions can help us understand variables which are very good for prediction but that we weren’t aware of. Moreover, it can help us understand the role of interactions from a more intuitive point of view through exploration. This includes unsupervised methods such as \\(PCA\\) and K-Means clustering. 6.4 Prediction If you’re aim is to achieve the best predictive accuracy out there, then there’s also evidence that some models seem to perform better than others. Tree based methods such as random forests and gradient boosting seem to continually perform the best in predictive competitions, together with more advanced models such as neural networks and support vector machines. For raw accuracy, there’s no rule on which model to use. You might have a hunch depending on the distribution and exploration of your data but since these methods are quite complex, there’s no single rule that states that one will perform better. We simply need to try several of them. Having said this, we need to explore our data and understand it. This can help a lot in figuring out why some models work more than others. 6.5 Prediction challenge As part of the end of the course, we will have a prediction competition. This means you’ll get to use all the methods we’ve discussed so far and compare your predictions to your fellow class mates. At the 2019 Summer Institute In Computational Social Science (SICSS), Mark Verhagen, Christopher Barrie, Arun Frey, Pablo Beytía, Arran Davis and me collected data on the number of people that visit the Wikipedia website of all counties in the United States. This data can be used to understand whether counties with different poverty levels get more edits from the Wikipedia community. This can help assess whether there is a fundamental bias in Wikipedia contribution to richer counties. We will use this data to predict the total number of edits of each county in Wikipedia. We’ve matched this Wikipedia data with census-level indicators for each county, including indicators on poverty level, racial composition and general metadata on the counties Wikipedia page. This amouns to a total of 150 columns. The variable you will be trying to predict is revisions. This is the total number of edits for each countie all along it’s history since 2001. Below is the complete codebook: county_fips: the county code longitude/latitude: the location of the county population: total population of county density: density of population in the county watchers: number of wikipedia users who ‘watch’ the page pageviews: number of pageviews pageviews_offset: minimum number of pageviews which is visible to the user (censored) revisions: total number of edits (from the creation of the website) editors: total number of editors (from the creation of the website) secs_since_last_edit: seconds since last edit characters: number of characters in the website words: number of words in the website references: number of references in the article unique_references: number of unique references in the article sections: number of sections in the wikipedia article external_links: number of external links links_from_this_page: number of hyperlinks used in this page links_to_this_page: number of hyperlinks that point to this page (from other wikipedia websites) male_*_*: these are the number of males within different age groups female_*_*: these are the number of females within different age groups total_*: This is the total population for different demographic groups latino: total count of latinos latino_*: total count of latinos from different races no_schooling_completed: total respondents with no schooling nursery_school: total respondents with only nursery school kindergarten: total respondents with kindergarten grade_*: These are the number of people the completed certain high school education hs_diploma: total respondents with high school diploma ged: total respondents with a GED diploma less_than_1_year_college: total respondents with less than one year of college more_than_1_year_college: total respondents with more than one year of college associates_degree: total respondents with associates degree bachelors_degree: total respondents with bachelors degree masters_degree: total respondents with masters degree professional_degree: total respondents with professional degree doctorate_degree: total respondents with doctorate degree total_with_poverty_status: total respondents with poverty status income_below_poverty: total respondents with income below poverty levels born_in_usa: total respondents born in USA foreign_born: total respondents foreing born speak_only_english: total respondents who speak only english speak_other_languages: total respondents who speak other languages count_*: total number of respondents within age groups percent_age_*: percentage of people within different age groups percent_*: percentage of people form different demographic groups. For example, whites, blacks, less than highschool, born in USA, etc.. internet_usage: percentage of internet usage in county For all of your analysis, use the rmse loss function, so that we can compare results across participants. Here are some ideas you can try in your analysis: Does it make sense to reduce the number of correlated variables into a few principal components? Do some counties cluster on very correlated variables? Is it fesiable to summarize some of these variables through predicting the cluster membership? Do we really need to use all variables? Does regularized regression or tree-based methods do better? You can read the data with: wiki_dt &lt;- read.csv(&quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/wikipedia_final.csv&quot;) You have 45 minutes, start! References "],
["syllabus.html", "Chapter 7 Syllabus 7.1 Course description 7.2 Schedule 7.3 Software 7.4 Prerequisites 7.5 About the author", " Chapter 7 Syllabus 7.1 Course description With the increasing amounts of data being collected on a daily basis, the field of machine learning has gained mainstream attention. By shifting away from focusing on inference, machine learning is a field at the intersection of statistics and computer science that is focused on maximizing predictive performance by learning patterns from data. That is, the goal of machine learning is to predict something – and predict it very well, regardless of whether you understand it. These techniques are common in business settings where, for example, stakeholders are interested in knowing the probability of a client leaving a company or the propensity of a client for buying a particular product. The field can be intimidating as it is vast and growing every year. However, scholars in the social sciences are beginning to understand the importance of the machine learning framework and how it can unlock new knowledge in fields such as sociology, political science, economics and psychology. On this course we will introduce students to the basic ideas of the machine learning framework and touch upon the basic algorithms used for prediction and discussing the potential it can have in the social sciences. In particular, we will introduce predictive algorithms such as regularized regressions, classification trees and clustering techniques through basic examples. We will discuss their advantages and disadvantages while paying great attention to how it’s been used in research. Although many social scientists do not see how predictive models can help explain social phenomena, we will also focus on how machine learning can play a role as a tool for discovery, improving causal inference and generalizing our classical models through cross validation. We will end the course with a prediction challenge that will put to test all of your acquired knowledge. Starting with a discussion on the role of predictive challenges such as the Fragile Families Challenge in the social sciences, our predictive challenge will require the student to run machine learning algorithms, test their out-of-sample error rate and discuss strategies on how the results are useful. This will give the class a real hands-on example of how to incorporate machine learning into their research right away. Below is a detailed description of the syllabus. 7.2 Schedule Session 1 July 6th 09:00h-10:45h Introduction to the Machine Learning Framework Inference vs Prediction Can inference and prediction complement each other? Bias-variance / Interpretability-prediction tradeoffs Resampling for understtanding your model “The Fragile Families Challenge” Readings: Sections 2.1 and 2.2 from James, Gareth, et al. An Introduction To Statistical Learning. Vol. 112. New York: springer, 2013 Molina, M., &amp; Garip, F. (2019). Machine Learning for Sociology. Annual Review of Sociology, 45. Hofman, J. M., Sharma, A., &amp; Watts, D. J. (2017). Prediction and explanation in social systems. Science, 355(6324), 486-488. Mullainathan, S., &amp; Spiess, J. (2017). Machine learning: an applied econometric approach. Journal of Economic Perspectives, 31(2), 87-106. Watts, D. J. (2014). Common sense and sociological explanations. American Journal of Sociology, 120(2), 313-351. Breiman, L. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199-231. Break 10:45h-11:15h Session 2 July 6th 11:15h-13:00h Linear regression and regularization Continuous predictions and loss functions Lasso Advantages/Disadvantages R example Ridge regression Advantages/Disadvantages R example Elastic Net Advantages/Disadvantages R example Exercises Readings: For a theoretical introduction to Lasso/Ridge, sections 6.1, 6.2 and 6.6 from James, Gareth, et al. (2013) An Introduction To Statistical Learning. Vol. 112. New York: springer For hands-on examples, Chapter 6 of Boehmke &amp; Greenwell (2019) Hands-On Machine Learning with R, 1st Edition, Chapman &amp; Hall/CRC The R Series. Accessible at: https://bradleyboehmke.github.io/HOML/ Session 3 July 7th 09:00h-10:45h Supervised Regression Introduction to supervised regression Classification Confusion matrices ROC Curves Classification Trees Advantages/Disadvantages R example Exercises Readings: For an introduction to classification trees, Section 8.1, 8.3.1 and 8.3.2 from James, Gareth, et al. An Introduction To Statistical Learning. Vol. 112. New York: springer, 2013 For hands-on examples, chapter 9 from Boehmke &amp; Greenwell (2019) Hands-On Machine Learning with R, 1st Edition, Chapman &amp; Hall/CRC The R Series. Accessible at: https://bradleyboehmke.github.io/HOML/ For real-world applications of Classification Trees: Billari, F. C., Fürnkranz, J., &amp; Prskawetz, A. (2006). Timing, sequencing, and quantum of life course events: A machine learning approach. European Journal of Population/Revue Européenne de Démographie, 22(1), 37-65. Chapter 3 of Nolan, D., &amp; Lang, D. T. (2015). Data science in R: a case studies approach to computational reasoning and problem solving. CRC Press. Break 10:45h-11:15h Session 4 July 7th 11:15h-13:00h Supervised Regression Bagging Advantages/Disadvantages R example Random Forest Advantages/Disadvantages R example Gradient Boosting Advantages/Disadvantages R example Exercises Readings: For an introduction to bagging/random forests/boosting, Chapter 8 from James, Gareth, et al. An Introduction To Statistical Learning. Vol. 112. New York: springer, 2013 For hands-on examples, chapter 10, 11 and 12 from Boehmke &amp; Greenwell (2019) Hands-On Machine Learning with R, 1st Edition, Chapman &amp; Hall/CRC The R Series. Accessible at: https://bradleyboehmke.github.io/HOML/ For real-world applications of Random Forests: Perry, C. (2013). Machine learning and conflict prediction: a use case. Stability: International Journal of Security and Development, 2(3), 56. Berk, R. A., Sorenson, S. B., &amp; Barnes, G. (2016). Forecasting domestic violence: A machine learning approach to help inform arraignment decisions. Journal of Empirical Legal Studies, 13(1), 94-115. Arpino, B., Le Moglie, M., &amp; Mencarini, L. (2018, April). Machine-learning techniques for family demography: an application of random forests to the analysis of divorce determinants in Germany. In ANNUAL MEETING PAA (Vol. 83). Session 5 July 8th 09h-10:45h Unsupervised Regression Introduction to unsupervised learning Principal Component Analysis (PCA) Advantages/Disadvantages R example K-Means clustering Advantages/Disadvantages R example Exercises Readings: For an introduction to unsupervised learning, Section 10.1 from James, Gareth, et al. An Introduction To Statistical Learning. Vol. 112. New York: springer, 2013 For an introduction to PCA Section 10.2 and 10.4 from James, Gareth, et al. An Introduction To Statistical Learning. 112. New York: springer, 2013 For hands-on examples, chapter 17 from Boehmke &amp; Greenwell (2019) Hands-On Machine Learning with R, 1st Edition, Chapman &amp; Hall/CRC The R Series. Accessible at: https://bradleyboehmke.github.io/HOML/ For an introduction to K-Means clustering Section 10.5 from James, Gareth, et al. An Introduction To Statistical Learning. Vol. 112. New York: springer, 2013 For hands-on examples, chapter 20 from Boehmke &amp; Greenwell (2019) Hands-On Machine Learning with R, 1st Edition, Chapman &amp; Hall/CRC The R Series. Accessible at: https://bradleyboehmke.github.io/HOML/ For real-world applications of K-means clustering: Garip, F. (2012). Discovering diverse mechanisms of migration: The Mexico–US Stream 1970–2000. Population and Development Review, 38(3), 393-433. Bail, C. A. (2008). The configuration of symbolic boundaries against immigrants in Europe. American Sociological Review, 73(1), 37-59. Break 10:45h-11:15h Session 6 July 8th 11:15h-13:00h Unsupervised Regression Hierarchical clustering Advantages/Disadvantages R example Final challenge: Prediction competition Explanation of strategies No free lunch theorem Presentation of results Readings: For an introduction to hierarchical clustering, sections 10.3.2, 10.3.3, 10.5.2 from James, Gareth, et al. An Introduction To Statistical Learning. Vol. 112. New York: springer, 2013 For hands-on examples, chapter 21 from Boehmke &amp; Greenwell (2019) Hands-On Machine Learning with R, 1st Edition, Chapman &amp; Hall/CRC The R Series. Accessible at: https://bradleyboehmke.github.io/HOML/ For examples on prediction competitions: Glaeser, E. L., Hillis, A., Kominers, S. D., &amp; Luca, M. (2016). Crowdsourcing city government: Using tournaments to improve inspection accuracy. American Economic Review, 106(5), 114-18. Salganik, M. J., Lundberg, I., Kindel, A. T., &amp; McLanahan, S. (2019). Introduction to the Special Collection on the Fragile Families Challenge. Socius, 5, 2378023119871580. Accessible at https://www.researchgate.net/publication/335733962_Introduction_to_the_Special_Collection_on_the_Fragile_Families_Challenge 7.3 Software We will be using the R software together with the Rstudio interface. No laptop is required as the seminars will take place in the RECSM facilities. Any packages we plan to use will be already downloaded previous to the session. 7.4 Prerequisites The course assumes that the student is familiar with R and should be familiar with reading, manipulating and cleaning data frames. Ideally, the student has conducted some type of research using the software. Students should have solid knowledge of basic statistics such as linear and logistic regression, ideally with more advanced concepts such as multilevel modelling. 7.5 About the author Jorge Cimentada has a PhD in Sociology from Pompeu Fabra University and is currently a Research Scientist at the Laboratory of Digital and Computational Demography at the Max Planck Institute for Demographic Research. His research is mainly focused on the study of educational inequality, inequality in spatial mobility and computational social science. He has worked on data science projects both in the private sector and in academic research and is interested in merging cutting edge machine learning techniques with classical social statistics. You can check out his blog at cimentadaj.github.io or contact him through twitter at @cimentadaj. "]
]
