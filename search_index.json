[
["index.html", "Machine Learning for Social Scientists Preface", " Machine Learning for Social Scientists Jorge Cimentada 2020-06-10 Preface Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists. These are intended to introduce social scientists to concepts in machine learning using traditional social science examples and datasets. Currently, it is not intended to be a book but rather supporting material for the course. Perhaps it evolves enough to be a book some day. "],
["machine-learning-for-social-scientists.html", "Chapter 1 Machine Learning for Social Scientists 1.1 A different way of thinking 1.2 Split your data into training/testing 1.3 Cross-validation 1.4 Bias-Variance Tradeoff 1.5 An example", " Chapter 1 Machine Learning for Social Scientists Machine Learning practitioners and social scientists share many things in common. These shared traits are mostly related to the transformation, analysis and evaluation of statistical models. In fact, when many of my fellow social scientists take any introductory course on machine learning, I often hear that many of the things they get taught are very common in their social statistics classes. This is good news! This means that you already have a foot inside the field without even knowing it. Machine Learning practitioners use many of the same statistical model we use and also many of transformation techniques that we use. However, there are important differences on how we analyze data and how we answer our questions. In this chapter I will elaborate on how machine learning practitioners have developed strategies different from social scientists for analyzing their data, how their analysis workflow compares to ours and finally, a tour around their way of thinking, which has evolved to be very different from ours. I hope that by understanding the strategies and techniques that machine learning practitioners use, social scientists would expand their analysis toolbox, allowing us to complement their way of thinking with our strong research design skills and modelling techniques. 1.1 A different way of thinking The first question we want to ask ourselves is, what is machine learning? Machine Learning bears indeed a fancy name which brings to mind thoughts related to artificial intelligence and robots. However, as you’ll see throughout the course, most terms and models used in machine learning are actually what we know as statistical models. The overaching difference in the definition of machine learning and social statistics is not the models or new strategies for analyzing data. It is the main objective of the analysis. What is machine learning after all? Using statistical methods to learn the data enough to be able to predict it accurately on new data That sounds somewhat familiar to us social scientists. Perhaps our goal is not to predict our data but it is certainly to learn it and understand it. In particular, social scientists are interested in figuring out if our theoretical description of a problem fits the data we have collected or have at hand. We do that by carefully building a model that explains the problem really well such that we can extrapolate an explanation for the problem from the data. Our gold standard to check whether we did a good job is to collect the exact same data again and see if our final models replicates. How does this differ from the way of thinking of machine learning practitioners? The main objective in a machine learning problem is accurate predictions; that is, regardless of how well they understand a problem, they want to learn the data enough to predict it well. Prediction problems are usually concerned with building and tweaking a model that predicts a dependent variable accurately on your data, such that when new data arrives, the model can predict it just as accurately. This does not mean that machine learning practitioners don’t have domain-specific knowledge of what they’re trying to predict (they have to select variables to include in a model just as we do). However, ‘parsimonious models’ (that is, simple and interpretable models) are not something they’re limited to (in contrast, social scientists hardly experiment with non-interpretable models). They might use models which contain up to hundreds of variables if that increases predictive accuracy. Although that might sound counter-intuitive to social scientists, more and more ground is being gained by this type of thinking in the social sciences (Watts 2014; Yarkoni and Westfall 2017). The difference between how we both approach research questions is the problem of inference versus prediction (Breiman and others 2001). That is the fundamental difference between the approach used by social scientists and practitioners of machine learning. However, for having such drastic differences in our objective, we share a lot of common strategies. For example, here’s the typical workflow of a social scientist: This is our safe zone: we understand these steps and we’ve exercised them many times. We begin by importing our data and inmediately start to clean it. This involves, for example, collapsing fine grained groups into bigger categories, transforming variables using logarithms and creating new variables which reflect important concepts from our theoretical model. Once we’re confident with our set of variables, we begin the iterative process of visualizing our data, fitting statistical models and evaluating the fit of the model. This is an iterative process because the results of our model might give us ideas on new variables or how to recode an existing variable. This prompts us to repeat the same process again with the aim of carefully building a model that fits the data well. Well, let me break it to you but this same process is very familiar to the machine learning process: They import their data, they wrangle their data, they fit statistical models, and they evaluate the fit of their models. They might have different names for the same things but in essence, they are more or less the same. For example, here are some common terms in the machine learning literature which have exact equivalents in social statistics: Features –&gt; Variables Feature Engineering –&gt; Creating Variables Learning Algorithms –&gt; Statistical Models Supervised Learning –&gt; Models that have a dependent variable Unsupervised Learning –&gt; Models that don’t have a dependent variable, such as clustering Classifiers –&gt; Models for predicting categorical variables, such as logistic regression and you’ll find more around. These are the common steps which you’ll find between both fields. However, machine learning practioners have developed extra steps which help them achieve their goal of predicting new data well: Training/Testing data –&gt; Unknown to us Cross-validation –&gt; Unknown to us Grid search –&gt; Unknown to us Loss functions –&gt; Model fit –&gt; Known to us but are not predominant (\\(RMSE\\), \\(R^2\\), etc…) These are very useful concepts and we’ll focus on those in this introduction. In this introduction I won’t delve into the statistical models (learning algorithms) used in machine learning as these will be discussed in later chapters but I wanted to highlight that although they share similarities with the models used in social statistics, there are many models used in the machine learning literature which are unknown to us. Let’s delve into each of these three new concepts. Before we beginning explaining these concepts and using R, let’s load the packages we’ll use in this chapter: # All these packages can be installed with `install.packages` library(ggplot2) library(patchwork) library(scales) library(tidymodels) 1.2 Split your data into training/testing Since the main objective in machine learning is to predict data accurately, all of their strategies are geared towards avoiding overfitting/underfitting the data. In other words, they want to capture all the signal and ignore the noise: set.seed(2313) n &lt;- 500 x &lt;- rnorm(n) y &lt;- x^3 + rnorm(n, sd = 3) age &lt;- rescale(x, to = c(0, 100)) income &lt;- rescale(y, to = c(0, 5000)) age_inc &lt;- data.frame(age = age, income = income) y_axis &lt;- scale_y_continuous(labels = dollar_format(suffix = &quot;€&quot;, prefix = &quot;&quot;), limits = c(0, 5000), name = &quot;Income&quot;) x_axis &lt;- scale_x_continuous(name = &quot;Age&quot;) bad_fit &lt;- ggplot(age_inc, aes(age, income)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + y_axis + x_axis + ggtitle(&quot;Underfit&quot;) + theme_linedraw() overfit &lt;- ggplot(age_inc, aes(age, income)) + geom_point() + geom_smooth(method = &quot;loess&quot;, span = 0.015) + y_axis + x_axis + ggtitle(&quot;Overfit&quot;) + theme_linedraw() goodfit &lt;- ggplot(age_inc, aes(age, income)) + geom_point() + geom_smooth(method = &quot;loess&quot;, span = 0.9) + y_axis + x_axis + ggtitle(&quot;Ideal fit&quot;) + theme_linedraw() bad_fit + overfit + goodfit Figure 1.1: Different ways of fitting your data The first panel of figure 1.1 shows a model which is not flexible, as it fits a straight line without capturing the subtle non-linearities of the data. The middle panel is too flexible as it captures much of the random noise of the non-linear relationship. Finally, the third panel shows the ideal fit, where the fitted line is flexible enough to capture the non-linear relationship in the data yet it it is mainly unaffected by the random noise in the data. Although social scientists are aware of these concepts, we really don’t think about them a lot. When we perform statistical modelling we don’t really think about whether we’re overfitting or underfitting: we’re mostly paying attention to whether the statistical relationships make sense. For example, how would social scientists fit a model? They would take the entire data and fit the model on it. How do you know you’re overfitting? Well, one very easy and naive approach is to randomly divide your data into two chunks called training and testing: The training data usually consists of a random sample of around ~70% of the initial data and the testing data the remaining ~30% of the initial data. If a particular row is in the training data, it must not be on the testing data. In contrast, if a particular row is in the testing data, it shouldn’t be in the training data either. Why should splitting the data into two chunks help us fix the problem of overfitting? Because you can elaborate your model in the training set as much as you want, and when you’re confident enough, the testing data can serve as an unseen, pristine source of data on which you can evaluate your model. If fitting your model on the testing data shows that your model was too optimistic, you were probably overfitting the data. Let’s go through the steps one by one. Fit your model in the training data (remember, that’s a random sample of about 70% of the initial data) evaluate the model fit and make the same changes you would do on your complete data: create new variables, recode variables, etc. You can think of this chunk as the complete data to perform your analysis. It is the equivalent of the initial data where social scientists fit their models. Once you’re very comfortable with your model, the best recipe for checking whether your model was overfitting is to use this fitted model to predict on the other chunk of data (the testing data): If you tweaked your model in such a way that it learned the noise of your training data, it will perform poorly on the testing data, since your the model didn’t capture the overall trend in the training data but rather the noise. It’s time to introduce how we perform these steps in R. For this, we’ll use the package tidyflow, a package created for this book. It aims to have a simple and intuitive workflow for machine learning which you’ll learn through this book. You can install the package with the code below: install.packages(&quot;devtools&quot;) devtools::install_github(&quot;cimentadaj/tidyflow&quot;) In R we can build a machine learning ‘workflow’ with the function tidyflow. To this workflow, we can plug in steps that you can execute. In our example, to plug in the step of partitioning the data into training and testing, you can use plug_split with the function initial_split: library(tidyflow) ml_flow &lt;- age_inc %&gt;% tidyflow(seed = 2313) %&gt;% plug_split(initial_split) ml_flow ## ══ Tidyflow ════════════════════════════════════════════════════════════════════ ## Data: 500 rows x 2 columns ## Split: initial_split w/ default args ## Recipe/Formula: None ## Resample: None ## Grid: None ## Model: None tidyflow already knows that age_inc is the main data source and that we need to apply the training/testing split with initial_split. You can think of this as plan that will be executed once you tell it to. Let’s get back to our example and suppose that you fit your model several times on the training data, tweaking it to improve performance (when I say tweaking I mean applying transformations, including new variables, recoding old variables, including polynomials, etc..). When you think you’re ready, you use this model to predict on the testing data and find out that the model was indeed overfitting the data because you cannot predict the testing data as well as the training data. You then go back to the training data, tweak some more, run some models again and when you think the model is ready again, you predict on your testing data again and find that it improved. Then you repeate the process again, \\(3\\), \\(4\\), \\(5\\) \\(N\\) times. If you do that, you will, in very subtle ways, start to overfit your model on the testing data! In other words, you’re fitting a model \\(N\\) times on your training data, evaluating its fit on the testing data and then tweaking again to improve the prediction on the testing data. The testing data should serve as the final dataset to compare your model: you should not tweak the model again after seeing how your model fits the unseen testing data. That doesn’t sound right. It seems we have too few “degrees of freedom” to test the accuracy of our model. We can tweak the model in the training data as much as we want but we only have one attempt at testing our model against the testing data. How can we evaluate, then, whether we’re overfitting with the training data alone, then? Enter cross-validation 1.3 Cross-validation The idea behind cross-validation is to allow the user to check whether they’re overfitting the data without predicting on the testing data. How does it work? First, we only work with our training data and replicate the training data 10 times The 10 rectangular red rows below the training data, contain an exact replica of the initial training data. That is, if the initial training data has 500 rows and 10 columns, then each of these red rectangled rows also has 500 rows and 10 columns. The idea behind this approach is that for each rectangled row, you can use 70% of the data to fit your model and then predict on the remaining 30%. For example, for the first rectangled row, you would fit your initial model model with some tweak (let’s say, adding a squared term to the age variable to check if that improves fit) on the training data and then predict on the testing data to evaluate the fit: Since we fit a model to the trainingdata of each rectangled row and then predict on the testing data of each rectangled row, we can record how well our model is doing for each of our replicate data sets. For example, for the first row we record the \\(RMSE\\) of the prediction on the testing data. For the second rectangled row, fit the exact same model (that is, including the age squared term) on 70% of the training data, predict on the testing data and record the \\(RMSE\\). And then repeat the same iteration for every rectangled row: After you’ve fitted the model and evaluated the model 10 times, you have 10 values of the \\(RMSE\\). With these 10 values you can calculate the average \\(RMSE\\) and standard error of your model’s performance. Note that with this approach, the testing data changes in each rectangled row, making sure that each ~30% chunk of the data passes through the testing dataset at some point during the predictions. This is done to ensure the predictions are as balanced as possible. This approach offers a way to iterate as many times as you want on tweaking your model and predicting on the cross-validated testing data without actually predicting on the initial testing dataset. This is the least bad approach that is currently accepted in the literature. Why is it the least bad approach? Because if we tweak the model on these 10 replicas one time, then a second time, then a third time, etc…, we’ll also start overfitting on each of these 10 slots! The superiority of this approach over tweaking on the training data is that since we have 10 replicas, we can take the average of model fit metrics and also obtain standard errors. This allows to have a somewhat balanced account of how our model fit is doing and the uncertainty around it. That said, since we will always overfit in someway using a cross-validation approach, the final error of your model fit on the training data will always be over optimistic (lower error than what you will actually have, if you predicted on the pristine testing data. Based on our previous tidyflow, we can plug in a cross-validation step with plug_resample. There are many different cross-validation techniques but let’s focus on the one from our example (replicating the data 10 times). For that, we use the function vfold_cv: ml_flow &lt;- ml_flow %&gt;% plug_resample(vfold_cv) ml_flow ## ══ Tidyflow ════════════════════════════════════════════════════════════════════ ## Data: 500 rows x 2 columns ## Split: initial_split w/ default args ## Recipe/Formula: None ## Resample: vfold_cv w/ default args ## Grid: None ## Model: None 1.4 Bias-Variance Tradeoff Before we elaborate a complete coded example, it’s important to talk about the concept of bias-variance tradeoff used in machine learning problems. As was shown in figure 1.1, we want the ideal fit without overfitting or underfitting the data. In some instances, fitting the data that well is very difficult because we don’t have variables that reflect the data generating process or because the relationship is too complex. In that case, for machine learning problems, you might want to either underfit or overfit slightly, depending on your problem. Overfitting your data has some value, which is that we learn the data very well. This is often called a model with a lot of flexibility. A model that can learn all the small intricacies of the data is often called a flexible model. There is very little bias in a model like this one, since we learn the data very very well. However, at the expense of bias, overfitting has a lot of variance. If we predict on a new dataset using the overfitted model, we’ll find a completely different result from the initial model. If we repeat the same on another dataset, we’ll find another different result. That is why models which can be very flexible are considered to have very little bias and a lot of variance: The model above fits the criteria: On the other hand, models which are not flexible, have more bias and less variance. One familiar example of this is the linear model. By fitting a straight line through the data, the variance is very small: if we run the same exact model on a new data, the fitted line is robust to slight changes in the data (outliers, small changes in the tails of the distribution, etc..). However, the fitted line doesn’t really capture the subtle trends in the data (assuming the relationship is non-linear, which is in most cases). That is why non-flexible models are often called to have high bias and low variance: or in other words: In reality, what we usually want is something located in the middle of these two extremes: we want a model that is neither too flexible that overfits the data nor too unflexible that misses the signal. There is really no magical recipe to achieving the perfect model and our best approach is to understand our model’s performance using techniques such as cross-validation to assess how much our model is overfitting/underfitting the data. Even experiencied machine learning practitioners can build models that overfit the data (one notable example is the results from the Fragile Families Challenge, see HEREEE put the plot of the paper where overfitting is huge). 1.5 An example Let’s combine all the new steps into a complete pipeline of machine learning in R. We can do that by finishing the tidyflow we’ve been developing so far. Let’s use the data age_inc which has the age of a person and their income. We want to predict their income based on their age. The rectangular data looks like this: The relationship between these two variables is non-linear, showing a variant of the Mincer equation (Mincer 1958) where income is a non-linear function of age: # age_inc was defined above, and it is reused here age_inc %&gt;% ggplot(aes(age, income)) + geom_point() + theme_linedraw() Since ml_flow is a series of steps, it allows you to remove any of them. Let’s remove the cross-validation step with drop_resample: ml_flow &lt;- ml_flow %&gt;% drop_resample() ml_flow ## ══ Tidyflow ════════════════════════════════════════════════════════════════════ ## Data: 500 rows x 2 columns ## Split: initial_split w/ default args ## Recipe/Formula: None ## Resample: None ## Grid: None ## Model: None Let’s begin running some models. The first model we’d like run is a simple regression income ~ age on the training data and plot the fitted values. # Run the model m1 &lt;- ml_flow %&gt;% plug_recipe(~ recipe(income ~ age, data = .)) %&gt;% # Add the formula plug_model(set_engine(linear_reg(), &quot;lm&quot;)) %&gt;% # Define the linear regression fit() # Fit model # Predict on the training data m1_res &lt;- m1 %&gt;% predict_training() m1_res ## # A tibble: 375 x 3 ## age income .pred ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 56.9 2574. 2650. ## 2 38.9 2091. 2298. ## 3 62.8 2328. 2767. ## 4 84.9 3548. 3200. ## 5 65.3 2440. 2815. ## 6 93.8 3866. 3374. ## 7 7.92 1511. 1692. ## 8 78.3 3134. 3070. ## 9 55.7 2777. 2628. ## 10 42.2 2918. 2362. ## # … with 365 more rows The result of predict_training is the training data from age_inc with one new column: the predicted values of the model. Let’s visualize the predictions: # Visualize the result m1_res %&gt;% ggplot(aes(age, income)) + geom_line(aes(y = .pred), color = &quot;red&quot;, size = 2) + geom_point() + scale_x_continuous(name = &quot;Age&quot;) + scale_y_continuous(name = &quot;Income&quot;, label = dollar_format(suffix = &quot;€&quot;, prefix = &quot;&quot;)) + theme_linedraw() It seems we’re underfitting the relationship. To measure the fit of the model, we’ll use the Root Mean Square Error (RMSE). Remember it? \\[ RMSE = \\sqrt{\\sum_{i = 1}^n{\\frac{(\\hat{y} - y)^2}{N}}} \\] Without going into too many details, it is the average difference between each dot from the plot from the value same value in the fitted line. The current \\(RMSE\\) of our model is 379.59. This means that on average our predictions are off by around 379.59 euros. The fitted line is underfitting the relationship because it cannot capture the non-linear trend in the data. How do we increase the fit? We could add non-linear terms to the model, for example \\(age^2\\), \\(age^3\\), …, \\(age^{10}\\). However, remember, by fitting very high non-linear terms to the data, we might get lower error from the model on the training data but that’s because the model is learning the training data so much that it starts to capture noise rather than the signal. This means that when we predict on the unseen testing data, our model would not know how to identify the signal in the data and have a higher \\(RMSE\\) error. How can we be sure we’re picking the best model specification? This is where cross-validation comes in! We can use the function vfold_cv to separate the training data into 10 cross-validation sets, where each one has a training and testing data. ## # 10-fold cross-validation ## # A tibble: 10 x 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [337/38]&gt; Fold01 ## 2 &lt;split [337/38]&gt; Fold02 ## 3 &lt;split [337/38]&gt; Fold03 ## 4 &lt;split [337/38]&gt; Fold04 ## 5 &lt;split [337/38]&gt; Fold05 ## 6 &lt;split [338/37]&gt; Fold06 ## 7 &lt;split [338/37]&gt; Fold07 ## 8 &lt;split [338/37]&gt; Fold08 ## 9 &lt;split [338/37]&gt; Fold09 ## 10 &lt;split [338/37]&gt; Fold10 Each of those split objects (there are 10) contains a training and testing set. This is the equivalent of the image we saw before: The next thing we have to do is train the same model on the training data of each of these cross-validated sets, use these trained models to predict on the 10 testing sets and record the error rate using our \\(RMSE\\) metric. But don’t worry, you don’t have to do that all of that manually, tidyflow can leverage many packages to do that for you: # Define the formula of your model and specify that the polynomial # value will be &#39;tuned&#39;. That is, we will try several values # instead of only one. rcp &lt;- ~ recipe(income ~ age, data = .) %&gt;% step_poly(age, degree = tune()) m2 &lt;- m1 %&gt;% # Add the cross-validation step plug_resample(vfold_cv) %&gt;% # Replace the initial recipe with the one with several polynomials replace_recipe(rcp) %&gt;% # Here we define the values we will try, from 2 to 10 plug_grid(expand.grid, degree = 2:10) %&gt;% # Fit the model fit() # Visualize the result m2 %&gt;% pull_tflow_fit_tuning() %&gt;% # Extract all models with different &#39;degree&#39; values autoplot() + theme_linedraw() Figure 1.2: Average evaluation metrics of predicting on the testing data through the 10 cross-validation sets Figure 1.2 shows the error rate for the \\(RMSE\\) and the \\(R^2\\). For the \\(RMSE\\) (left panel), the resulting error terms show that any polynomial above 2 has very similar error rates. However, there is a point in which adding \\(age^9\\) and \\(age^{10}\\) increases the error rate. This decrease in fit as complexity increases can also been see with the \\(R^2\\) (right panel), as it decreases with higher polynomials. This is a good example where a lot of flexibility (fitting the non-linear trend very well), increases accuracy on the training set but shows a lot variability on the testing set. The \\(RMSE\\) that we see in figure 1.2 is the average \\(RMSE\\) from predicting on the testing set using the model fitted on the training data in the 10 cross validated sets. Given that most of the polynomial terms have similar error terms, we usually would go for the simplest model, that is, the model with \\(age^3\\). We can run the model on the entire training data with 3 non-linear terms and check the fit: # Fit the final model with degrees = 3 res_m2 &lt;- complete_tflow(m2, best_params = data.frame(degree = 3)) res_m2 %&gt;% predict_training() %&gt;% ggplot(aes(age, income)) + geom_line(aes(y = .pred), color = &quot;red&quot;, size = 2) + geom_point() + scale_x_continuous(name = &quot;Age&quot;) + scale_y_continuous(name = &quot;Income&quot;, label = dollar_format(suffix = &quot;€&quot;, prefix = &quot;&quot;)) + theme_linedraw() The \\(RMSE\\) on the training data for the three polynomial model is 279.87. We need to compare that to our testing \\(RMSE\\). res_m2 %&gt;% predict_testing() %&gt;% ggplot(aes(age, income)) + geom_line(aes(y = .pred), color = &quot;red&quot;, size = 2) + geom_point() + scale_x_continuous(name = &quot;Age&quot;) + scale_y_continuous(name = &quot;Income&quot;, label = dollar_format(suffix = &quot;€&quot;, prefix = &quot;&quot;)) + theme_linedraw() training \\(RMSE\\) is 279.87 testing \\(RMSE\\) is 311.03 testing \\(RMSE\\) will almost always be higher, since we always overfit the data in some way through cross-validation. References "],
["regularization.html", "Chapter 2 Regularization 2.1 Ridge regularization 2.2 Lasso regularization 2.3 Elastic Net regularization 2.4 Exercises", " Chapter 2 Regularization Regularization is a common topic in machine learning and bayesian statistics. In this chapter, we will describe the three most common regularized linear models in the machine learning literature and introduce them in the context of the PISA data set. At the end of the document you’ll find exercises that will put your knowledge to the test. Most of the material is built upon Boehmke and Greenwell (2019) and James et al. (2013), which can be used as reference. 2.1 Ridge regularization Do no let others fool you into thinking that ridge regression is a fancy artificial intelligence algorithm. Are you familiar with linear regression? If you are, then ridge regression is just an adaptation of linear regression. The whole aim of linear regression, or Ordinary Least Squares (OLS), is to minimize the sum of the squared residuals. In other words, fit N number of regression lines to the data and keep only the one that has the lowest sum of squared residuals. In simple formula jargon, OLS tries to minimize this: \\[\\begin{equation} RSS = \\sum_{k = 1}^n(actual_i - predicted_i)^2 \\end{equation}\\] For each fitted regression line, you compare the predicted value (\\(predicted_i\\)) versus the actual value (\\(actual_i\\)), square it, and add it all up. Each fitted regression line then has an associated Residual Sum of Squares (RSS) and the linear model chooses the line with the lowest RSS. Note: Social scientists are familiar with the RSS and call it just by it’s name. However, be aware that in machine learning jargon, the RSS belongs to a general family called loss functions. Loss functions are metrics that evaluate the fit of your model and there are many around (such as AIC, BIC or R2). Ridge regression takes the previous RSS loss function and adds one term: \\[\\begin{equation} RSS + \\lambda \\sum_{k = 1}^n \\beta^2_j \\end{equation}\\] The term on the right is called a shrinkage penalty because it forces each coefficient \\(\\beta_j\\) closer to zero by squaring it. The shrinkage part is clearer once you think of this term as forcing each coefficient to be as small as possible without compromising the Residual Sum of Squares (RSS). In other words, we want the smallest coefficients that don’t affect the fit of the line (RSS). An intuitive example is to think of RSS and \\(\\sum_{k = 1}^n \\beta^2_j\\) as two separate things. RSS estimates how the model fits the data and \\(\\sum_{k = 1}^n \\beta^2_j\\) limits how much you overfit the data. Finally, the \\(\\lambda\\) between these two terms (called lambda) can be interpreted as a “weight”. The higher the lambda, the higher the weight that will be given to the shrinkage term of the equation. If \\(\\lambda\\) is 0, then multiplying 0 by \\(\\sum_{k = 1}^n \\beta^2_j\\) will always return zero, forcing our previous equation to simply be reduced to the single term \\(RSS\\). Why is there a need to “limit” how well the model fits the data? Because we, social scientists and data scientists, very commonly overfit the data. The plot below shows a simulation from Simon Jackson where we can see that when tested on a training set, OLS and Ridge tend to overfit the data. However, when tested on the test data, ridge regression has lower out of sample error as the \\(R2\\) is higher for models with different observations. The strength of the ridge regression comes from the fact that it compromises fitting the training data really well for improved generalization. In other words, we increase bias (because we force the coefficients to be smaller) for lower variance (making our predictions more robust). In other words, the whole gist behind ridge regression is penalizing very large coefficients for better generalization on new data. Having that intuition in mind, there is one important thing to keep in mind: the predictors of the ridge regression need to be standardized. Why is this the case? Because due to the scale of a predictor, its coefficient can be more penalized than other predictors. Suppose that you have the income of a particular person (measured in thousands per months) and time spent with their families (measured in seconds) and you’re trying to predict happiness. A one unit increase in salary could be penalized much more than a one unit increase in time spent with their families just because a one unit increase in salary can be much bigger due to it’s metric. In R, you can fit a ridge regression using tidymodels and tidyflow. Let’s load the packages that we will work with and read the data: library(tidymodels) library(tidyflow) data_link &lt;- &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot; pisa &lt;- read.csv(data_link) We will construct our tidyflow step by step. We begin with the data and then separate the training and test data. All of our modelling will be performed on the training data and the test data is saved for later (the test data must be completely ignored until you have your final tuned model). The second step is specifying the variables in the model and scaling all of them, as I have explained, we want to normalize all variables such that no variable gets more penalized than other due to their metric. # Specify all variables and scale rcp &lt;- # Define dependent (math_score) and independent variables ~ recipe(math_score ~ MISCED + FISCED + HISEI + REPEAT + IMMIG + DURECEC + BSMJ, data = .) %&gt;% # Scale all predictors (already knows it&#39;s the independent variables) step_scale(all_predictors()) tflow &lt;- tidyflow(seed = 231141) %&gt;% plug_data(pisa) %&gt;% plug_split(initial_split, prop = .7) %&gt;% # Add the recipe with all variables and scale plug_recipe(rcp) tflow ## ══ Tidyflow ════════════════════════════════════════════════════════════════════ ## Data: 4.84K rows x 501 columns ## Split: initial_split w/ prop = ~0.7 ## Recipe: available ## Resample: None ## Grid: None ## Model: None The argument prop controls the proportion of the sample that will be in the training data. Here we specify it to be .7, 70% of the data. The third step is specifying the tuning parameters. The ridge regression has a parameter called penalty which needs to be set by us. penalty is the “weight” term in the ridge equation, which controls how much weight do we want to give to the “shrinkage penalty” (this is the \\(\\lambda\\) from the equation). If this penalty is set to 0, it means we attach no weight to the penalty term and we will get the same result over OLS. Let’s try that: ############################# Ridge regression ################################ ############################################################################### regularized_reg &lt;- set_engine( # mixture specifies the type of penalized regression: 0 is ridge regression linear_reg(penalty = 0, mixture = 0), &quot;glmnet&quot; ) model1 &lt;- tflow %&gt;% plug_model(regularized_reg) %&gt;% fit() # Get ridge coefficients mod &lt;- model1 %&gt;% pull_tflow_fit() %&gt;% .[[&quot;fit&quot;]] ridge_coef &lt;- predict(mod, s = 0, type = &quot;coefficients&quot;) ############################# Linear model #################################### ############################################################################### model2 &lt;- tflow %&gt;% plug_model(set_engine(linear_reg(), &quot;lm&quot;)) %&gt;% fit() lm_coef &lt;- model2 %&gt;% pull_tflow_fit() %&gt;% .[[&quot;fit&quot;]] %&gt;% coef() ############################# Comparing model ################################# ############################################################################### comparison &lt;- data.frame(coefs = names(lm_coef), `Linear coefficients` = unname(round(lm_coef, 2)), `Ridge coefficients` = round(as.vector(ridge_coef), 2)) knitr::kable(comparison) coefs Linear.coefficients Ridge.coefficients (Intercept) 329.37 331.55 MISCED 3.88 4.17 FISCED 11.93 11.61 HISEI 17.85 17.36 REPEAT -22.03 -21.41 IMMIG 6.66 6.41 DURECEC -0.33 -0.27 BSMJ 9.10 8.96 Coming from a social science background, it might seem counterintuitive that the researcher has to specify tuning parameters for the model. In traditional social science statistics, models usually estimate similar values internally and the user doesn’t have to think about them. However, there are strategies already implemented to explore the combination of many possible values. With our previous example, we have to add tune() to the penalty argument and add a grid for the model to search for the best one: # Here we add the cross-validation and grid tflow &lt;- tflow %&gt;% # Cross-validation plug_resample(vfold_cv, v = 5) %&gt;% # Grid plug_grid(grid_regular) regularized_reg &lt;- update(regularized_reg, penalty = tune()) res &lt;- tflow %&gt;% # Update the model to specify that `penalty` will be tuned plug_model(regularized_reg) %&gt;% fit() final_ridge &lt;- complete_tflow(res, metric = &quot;rmse&quot;) final_ridge %&gt;% pull_tflow_fit() %&gt;% .[[&quot;fit&quot;]] %&gt;% plot(xvar = &quot;lambda&quot;, label = TRUE) Here we can see how our coefficients are affected by increasing the weight of the penalty parameter. Each of those lines are the coefficients for the variables. The x axis contains the penalty values and we can see how as the penalty increases, the size of the coefficients is shrinking to be close to zero. By around the log of penalty around 8 nearly all coefficients are shrinked very close to zero. This plot is just an exercise to understand how the ridge regression works. In other words, we can figure out the best lambda automatically: best_tune &lt;- res %&gt;% pull_tflow_fit_tuning() %&gt;% select_best(metric = &quot;rmse&quot;) best_tune ## # A tibble: 1 x 1 ## penalty ## &lt;dbl&gt; ## 1 0.0000000001 However, there’s no need to calculate this, as complete_tflow figures it out for you (as you can see in the code chunk above, complete_tflow extracts this automatically and fits the best model). We can calculate the \\(RMSE\\) of the training data from the best model and compare it to the predictions on the testing data: train_rmse_ridge &lt;- final_ridge %&gt;% predict_training() %&gt;% rmse(math_score, .pred) holdout_ridge &lt;- final_ridge %&gt;% predict_testing() %&gt;% rmse(math_score, .pred) train_rmse_ridge$type &lt;- &quot;training&quot; holdout_ridge$type &lt;- &quot;testing&quot; ridge &lt;- as.data.frame(rbind(train_rmse_ridge, holdout_ridge)) ridge$model &lt;- &quot;ridge&quot; ridge ## .metric .estimator .estimate type model ## 1 rmse standard 76.64458 training ridge ## 2 rmse standard 78.21517 testing ridge The testing error (RMSE) is higher than the training error, as expected, as the training set nearly always memorizes the data better for the training. 2.2 Lasso regularization The Lasso regularization is very similar to the ridge regularization where only one thing changes: the penalty term. Instead of squaring the coefficients in the penalty term, the lasso regularization takes the absolute value of the coefficient. \\[\\begin{equation} RSS + \\lambda \\sum_{k = 1}^n |\\beta_j| \\end{equation}\\] Althought it might not be self-evident from this, the lasso reguralization has an important distinction: it can force a coefficient to be exactly zero. This means that lasso does a selection of variables which have big coefficients while not compromising the RSS of the model. The problem with ridge regression is that as the number of variables increases, the training error will almost always improve but the test error will not. For example, if we define the same model from above using a lasso, you’ll see that it forces coefficients to be exactly zero if they don’t add anything relative to the RSS of the model. This means that variables which do not add anything to the model will be excluded unless they add explanatory power that compensates the size of their coefficient. Here’s the same lasso example: regularized_reg &lt;- update(regularized_reg, mixture = 1) res &lt;- tflow %&gt;% plug_model(regularized_reg) %&gt;% fit() final_lasso &lt;- complete_tflow(res, metric = &quot;rmse&quot;) final_lasso %&gt;% pull_tflow_fit() %&gt;% .[[&quot;fit&quot;]] %&gt;% plot(xvar = &quot;lambda&quot;, label = TRUE) In contrast to the ridge regression, where coefficients are forced to be close to zero, the lasso penalty actually forces some coefficients to be zero. This property means that the lasso makes a selection of the variables with the higher coefficients and eliminates those which do not have a strong relationship. Lasso is usually better at model interpretation because it removes redundant variables while ridge can be useful if you want to keep a number of variables in the model, despite them being weak predictors (as controls, for example). To check the final model and it’s error, we can recicle the code from above and adapt it to the lasso: train_rmse_lasso &lt;- final_lasso %&gt;% predict_training() %&gt;% rmse(math_score, .pred) holdout_lasso &lt;- final_lasso %&gt;% predict_testing() %&gt;% rmse(math_score, .pred) train_rmse_lasso$type &lt;- &quot;training&quot; holdout_lasso$type &lt;- &quot;testing&quot; lasso &lt;- as.data.frame(rbind(train_rmse_lasso, holdout_lasso)) lasso$model &lt;- &quot;lasso&quot; lasso ## .metric .estimator .estimate type model ## 1 rmse standard 76.63928 training lasso ## 2 rmse standard 78.23457 testing lasso So far, we can check which model is performing better: model_comparison &lt;- rbind(ridge, lasso) model_comparison ## .metric .estimator .estimate type model ## 1 rmse standard 76.64458 training ridge ## 2 rmse standard 78.21517 testing ridge ## 3 rmse standard 76.63928 training lasso ## 4 rmse standard 78.23457 testing lasso Currently the ridge regression has a very minor advantaged over the lasso yet the difference is probably within the margin of error. Depending on your aim, you might want to choose either of the models. For example, if our models contained a lot of variables, lasso might be more interpretable as it reduces the number of variables. However, if you have reasons to believe that keeping all variables in the model is important, then ridge provides an advantage. 2.3 Elastic Net regularization If you’re aware of ridge and lasso, then elastic net regularization is a logical step. Elastic Net (the name sounds fancy, but it is also an adaptation of OLS) combines both penalties to form one single equation. Here we define our ridge penalty: \\[ridge = \\lambda \\sum_{k = 1}^n \\beta_j^2\\] And here we define our lasso penalty: \\[lasso = \\lambda \\sum_{k = 1}^n |\\beta_j|\\] Elastic net regularization is the addition of these two penalties in comparison to the RSS: \\[RSS + lasso + ridge\\] I think the best explanation for elastic net reguarlization comes from Boehmke and Greenwell (2019): Although lasso models perform feature selection, when two strongly correlated features are pushed towards zero, one may be pushed fully to zero while the other remains in the model. Furthermore, the process of one being in and one being out is not very systematic. In contrast, the ridge regression penalty is a little more effective in systematically handling correlated features together. Consequently, the advantage of the elastic net penalty is that it enables effective regularization via the ridge penalty with the feature selection characteristics of the lasso penalty. Essentially, you now have two tuning parameters. In the grid of values, instead of specifying a mixture of 0 (ridge) or 1 (lasso), tidyflow will slide through several values of mixture ranging from 0 to 1 and compare that to several values of lambda. This is formally called a grid search. We can recycle the same code from above: regularized_reg &lt;- update(regularized_reg, mixture = tune()) res &lt;- tflow %&gt;% plug_model(regularized_reg) %&gt;% fit() final_elnet &lt;- complete_tflow(res, metric = &quot;rmse&quot;) train_rmse_elnet &lt;- final_elnet %&gt;% predict_training() %&gt;% rmse(math_score, .pred) holdout_elnet &lt;- final_elnet %&gt;% predict_testing() %&gt;% rmse(math_score, .pred) train_rmse_elnet$type &lt;- &quot;training&quot; holdout_elnet$type &lt;- &quot;testing&quot; elnet &lt;- as.data.frame(rbind(train_rmse_elnet, holdout_elnet)) elnet$model &lt;- &quot;elnet&quot; elnet ## .metric .estimator .estimate type model ## 1 rmse standard 76.63928 training elnet ## 2 rmse standard 78.23457 testing elnet The RMSE of the elastic net is somewhat lower than then ridge and lasso but also probably within the margin of error. Let’s compare it visually: model_comparison &lt;- rbind(model_comparison, elnet) model_comparison %&gt;% ggplot(aes(model, .estimate, color = type, group = type)) + geom_point(position = &quot;dodge&quot;) + geom_line() + scale_y_continuous(name = &quot;RMSE&quot;) + scale_x_discrete(name = &quot;Models&quot;) + theme_minimal() 2.4 Exercises The Fragile Families Challenge is a study that aimed to predict a series of indicators of children at age 15 only using data from ages 0 to 9. With this challenge, the principal investigators wanted to test whether skills such as cognitive and non-cognitive abilities were correctly predicted. With that idea in mind, they were interested in following up children that beat the ‘predictions’: those children that exceeded the model’s prediction, for example given their initial conditions. Using a similarly constructed non-cognitive proxy, I’ve created a non-cognitive index using the PISA 2018 for the United States which is the average of the questions: ST182Q03HA - I find satisfaction in working as hard as I can. ST182Q04HA - Once I start a task, I persist until it is finished. ST182Q05HA - Part of the enjoyment I get from doing things is when I improve on my past performance. ST182Q06HA - If I am not good at something, I would rather keep struggling to master it than move on to something I may […] The scale of the index goes from 1 to 4, where in 4 the student strongly agrees and 1 is they completely disagree. In other words, this index shows that the higher the value, the higher the non cognitive skills. You can check out the complete PISA codebook here. In these series of exercises you will have to try different models that predict this index of non-cognitive skills, perform a grid search for the three models and compare the predictions of the three models. First, read in the data with: data_link &lt;- &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot; pisa &lt;- read.csv(data_link) 1. Create a tidyflow with a split Begin with the data pisa To plug a split, use initial_split Remember to set the seed to 2341 so that everyone can compare their results. &gt; Answer tflow &lt;- pisa %&gt;% tidyflow(seed = 2341) %&gt;% plug_split(initial_split) tflow 2. Run a ridge regression with non-cognitive as the dependent variable Plug in a formula (hint, look at ?plug_formula) and use as many variables as you want (you can reuse the previous variables from the examples or pick all of them). A formula of the like noncogn ~ . will regress noncogn on all variables. Plug in the ridge regression with penalty set to 0.001 (hint: remember to set mixture to the value corresponding to the ridge regression) Fit the ridge model (with fit) Predict on the training data with predict_training and explore the \\(R^2\\) (rsq) and \\(RMSE\\) (rmse). &gt; Answer ridge_mod &lt;- set_engine(linear_reg(penalty = 0.001, mixture = 0), &quot;glmnet&quot;) tflow &lt;- tflow %&gt;% plug_formula(noncogn ~ .) %&gt;% plug_model(ridge_mod) m1 &lt;- fit(tflow) m1_rsq &lt;- predict_training(m1) %&gt;% rsq(noncogn, .pred) m1_rmse &lt;- predict_training(m1) %&gt;% rmse(noncogn, .pred) 3. Add a recipe to sacle all of the predictors and rerun the previous model Drop the formula from the tidyflow with drop_formula Add a recipe with the same formula you had, but including the step_scale for all predictors Rerun the model and extract the \\(R^2\\) and \\(RMSE\\) How does the \\(R^2\\) and \\(RMSE\\) change? Was there an impact in change? &gt; Answer rcp &lt;- ~ recipe(noncogn ~ ., data = .) %&gt;% # We need to remove these two variables because they are # character variables. We can&#39;t scale and center a # character variable step_rm(STRATUM, VER_DAT) %&gt;% step_scale(all_predictors()) tflow &lt;- tflow %&gt;% drop_formula() %&gt;% plug_recipe(rcp) m2 &lt;- fit(tflow) m2_rsq &lt;- predict_training(m2) %&gt;% rsq(noncogn, .pred) m2_rmse &lt;- predict_training(m2) %&gt;% rmse(noncogn, .pred) 4. Adapt the previous model to do a grid search of penalty values Add a cross-validation resample (vfold_cv) Add a tuning grid (grid_regular) and specify levels = 10. This will create a tuning grid of 10 values Update the penalty parameter to be tuned Run the grid search (fit) Extract the tuning grid (pull_tflow_fit_tuning) and visualize autoplot Is there a pattern with the improvement/decrease of the metrics of fit with respect to the penalty? &gt; Answer ridge_mod &lt;- update(ridge_mod, penalty = tune()) tflow &lt;- tflow %&gt;% replace_model(ridge_mod) %&gt;% plug_resample(vfold_cv) %&gt;% plug_grid(grid_regular, levels = 10) m3 &lt;- fit(tflow) m3 %&gt;% pull_tflow_fit_tuning() %&gt;% autoplot() 5. Run a lasso regression with the same specification as above Update the model to have a mixture of 1 (this is specifying that we want a lasso) Run the grid search (fit) Extract the tuning grid (pull_tflow_fit_tuning) and visualize autoplot Which model is performing better? Ridge or Lasso? Can you comment on the pattern of the penalty between ridge and lasso? &gt; Answer lasso_mod &lt;- update(ridge_mod, mixture = 1) m4 &lt;- tflow %&gt;% replace_model(lasso_mod) %&gt;% fit() m4 %&gt;% pull_tflow_fit_tuning() %&gt;% autoplot() 6. Run an elastic net regression on non cognitive skills Update the model to have a tuned mixture Replace the model in the tidyflow with the elastic net model Run the grid search (fit) Extract the tuning grid (pull_tflow_fit_tuning) and visualize autoplot &gt; Answer elnet_mod &lt;- update(lasso_mod, mixture = tune()) m5 &lt;- tflow %&gt;% replace_model(elnet_mod) %&gt;% fit() m5 %&gt;% pull_tflow_fit_tuning() %&gt;% autoplot() # Additional plot with standard error m5 %&gt;% pull_tflow_fit_tuning() %&gt;% collect_metrics() %&gt;% pivot_longer(penalty:mixture) %&gt;% mutate(low = mean - (std_err * 2), high = mean + (std_err * 2)) %&gt;% ggplot(aes(value, mean)) + geom_point() + geom_errorbar(aes(ymin = low, ymax = high)) + facet_grid(.metric ~ name) 7. Compare the three models Finalize the three models (the ridge model, the lasso model and the elastic net model) with complete_tflow. Remember to set the metric! Use the three finalized models (the ones that were produced by complete_tflow) to predict_training and predict_testing on each one Calculate the rmse of the three models on both training and testing Visualize the three models and their error for training/testing Comment on which models is better in out-of-sample fit Is it better to keep the most accurate model or a model that includes relevant confounders (even if they’re relationship is somewhat weak)? &gt; Answer # Since we will be repeating the same process many times # let&#39;s write a function to predict on the training/testing # and combine them. This function will accept a single # model and produce a data frame with the RMSE error for # training and testing. This way, we can reuse the code # without having to copy everything many times calculate_err &lt;- function(final_model, type_model = NULL) { final_model &lt;- complete_tflow(final_model, metric = &quot;rmse&quot;) err_train &lt;- final_model %&gt;% predict_training() %&gt;% rmse(noncogn, .pred) err_test &lt;- final_model %&gt;% predict_testing() %&gt;% rmse(noncogn, .pred) err_train$type &lt;- &quot;train&quot; err_test$type &lt;- &quot;test&quot; res &lt;- as.data.frame(rbind(err_train, err_test)) res$model &lt;- type_model res } final_res &lt;- rbind( calculate_err(m3, &quot;ridge&quot;), calculate_err(m4, &quot;lasso&quot;), calculate_err(m5, &quot;elnet&quot;) ) final_res %&gt;% ggplot(aes(model, .estimate, color = type)) + geom_point() + theme_minimal() ## BONUS # Fit a linear regression and compare the four models References "],
["tree-based-methods.html", "Chapter 3 Tree-based methods 3.1 Decision trees 3.2 Bagging 3.3 Exercises", " Chapter 3 Tree-based methods In this chapter we will touch upon the most popular tree-based methods used in machine learning. Haven’t heard of the term “tree-based methods”? Do not panic. The idea behind tree-based methods is very simple and we’ll explain how they work step by step through the basics. Most of the material on this chapter was built upon Boehmke and Greenwell (2019) and James et al. (2013). Before we begin, let’s load tidyflow and tidymodels and read the data that we’ll be using. library(tidymodels) library(tidyflow) library(rpart.plot) library(baguette) data_link &lt;- &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot; pisa &lt;- read.csv(data_link) 3.1 Decision trees Decision trees are simple models. In fact, they are even simpler than linear models. They require little statistical background and are in fact among the simplest models to communicate to a general audience. In particular, the visualizations used for decision trees are very powerful in conveying information and can even serve as an exploratory avenue for social research. Throughout this chapter, we’ll be using the PISA data set from the regularization chapter. On this example we’ll be focusing on predicting the math_score of students in the United States, based on the socio economic status of the parents (named HISEI in the data; the higher the HISEI variable, the higher the socio economic status), the father’s education (named FISCED in the data; coded as several categories from 0 to 6 where 6 is high education) and whether the child repeated a grade (named REPEAT in the data). REPEAT is a dummy variable where 1 means the child repeated a grade and 0 no repetition. Decision trees, as their name conveys, are tree-like diagrams. They work by defining yes-or-no rules based on the data and assign the most common value for each respondent within their final branch. The best way to learn about decision trees is by looking at one. Let’s do that: In this example the top-most box which says HISEI &lt; 56 is the root node. This is the most important variable that predicts math_score. Inside the blue box you can see two numbers: \\(100\\%\\) which means that the entire sample is present in this node and the number 474, the average test score for mathematics for the entire sample: On both sides of the root node (HISEI &lt; 56) there is a yes and a no. Decision trees work by partitioning variables into yes-or-no branches. The yes branch satisfies the name of root (HISEI &lt; 56) and always branches out to the left: In contrast, the no branch always branches out to the right: The criteria for separating into yes-or-no branches is that respondents must be very similar within branches and very different between branches (later in this chapter I will explain in detail which criteria is used and how). The decision tree figures out that respondents that have an HISEI below \\(56\\) and above \\(56\\) are the most different with respect to the mathematics score. The left branch (where there is a yes in the root node) are those which have a HISEI below 56 and the right branch (where there is a no) are those which have a HISEI above \\(56\\). Let’s call these two groups the low and high SES respectively. If we look at the two boxes that come down from these branches, the low SES branch has an average math score of \\(446\\) while the high SES branch has an average test score of \\(501\\): For the sake of simplicity, let’s focus now on the branch of the low SES group (the left branch). The second node coming out of the low SES branch contains 50% of the sample and an average math score of \\(446\\). This is the node with the rule REPEAT &gt;= 0.5: This ‘intermediate’ node is called internal node. For calculating this internal node, the decision tree algorithm limits the entire data set to only those which have low SES (literally, the decision tree does something like pisa[pisa$HISEI &lt; 56, ]) and asks the same question that it did in the root node: of all the variables in the model which one separates two branches such that respondents are very similar within the branch but very different between the branches with respect to math_score? For those with low SES background, this variable is whether the child repeated a grade or not. In particular, those coming from low SES background which repeated a grade, had an average math score of \\(387\\) whereas those who didn’t have an average math score of \\(456\\): These two nodes at the bottom are called leaf nodes because they are like the ‘leafs of the tree’. Leaf nodes are of particular importance because they are the ones that dictate what the final value of math_score will be. Any new data that is predicted with this model will always give an average math_score of \\(456\\) for those of low SES background who didn’t repeat a grade: Similarly, any respondent from high SES background, with a highly educated father who didn’t repeat a grade, will get assigned a math_score of \\(527\\): That is it. That is a decision tree in it’s simplest form. It contains a root node and several internal and leaf nodes and it can be interpreted just as we just did. The right branch of the tree can be summarized with the same interpretation. For example, for high SES respondents, father’s education (FISCED) is more important than REPEAT to separate between math scores: This is the case because it comes first in the tree. Substantially, this might be due to the fact that there is higher variation in education credentials for parents of high SES background than for those of low SES background. We can see that those with the highest father’s education (FISCED above \\(5.5\\)), the average math score is \\(524\\) whereas those with father’s education below \\(5.5\\) have a math score of \\(478\\). Did you notice that we haven’t interpreted any coefficients? That’s right. Decision trees have no coefficients and many other machine learning algorithms also don’t produce coefficients. Although for the case of decision trees this is because the model produces information in another way (through the visualization of trees), lack of coefficients is common in machine learning models because they are too complex to generate coefficients for single predictors. These models are non-linear, non-parametric in nature, producing very complex relationships that are difficult to summarize as coefficients. Instead, they produce predictions. We’ll be delving into this topic in future sections in detail. These examples show that decision trees are a great tool for exploratory analysis and I strongly believe they have an inmense potential for exploring interactions in social science research. In case you didn’t notice it, we literally just interpreted an interaction term that social scientists would routinely use in linear models. Without having to worry about statistical significance or plotting marginal effects, social scientists can use decision trees as an exploratory medium to understand interactions in an intuitive way. You might be asking yourself, how do we fit these models and visualize them? tidyflow and tidymodels have got you covered. For example, for fitting the model from above, we can begin our tidyflow, add a split, a formula and define the decision tree: # Define the decision tree and tell it the the dependent # variable is continuous (&#39;mode&#39; = &#39;regression&#39;) mod1 &lt;- set_engine(decision_tree(mode = &quot;regression&quot;), &quot;rpart&quot;) tflow &lt;- # Plug the data pisa %&gt;% # Begin the tidyflow tidyflow(seed = 23151) %&gt;% # Separate the data into training/testing plug_split(initial_split) %&gt;% # Plug the formula plug_formula(math_score ~ FISCED + HISEI + REPEAT) %&gt;% # Plug the model plug_model(mod1) vanilla_fit &lt;- fit(tflow) tree &lt;- pull_tflow_fit(vanilla_fit)$fit rpart.plot(tree) All plug_* functions serve to build your machine learning workflow and the model decision_tree serves to define the decision tree and all of the arguments. rpart.plot on the other hand, is a function used specifically for plotting decision trees (that is why we loaded the package rpart.plot at the beginning). No need to delve much into this function. It just works if you pass it a decision tree model: that is why pull the model fit before calling it. I’ve told all the good things about decision trees but they have important disadvantages. There are two that we’ll discuss in this chapter. The first one is that decision trees tend to overfit a lot. Just for the sake of exemplifying this, let’s switch to another example. Let’s say we’re trying to understand which variables are related to whether teachers set goals in the classroom. Substantially, this example might not make a lot of sense, but but let’s follow along just to show how much trees can overfit the data. This variable is named ST102Q01TA. Let’s plug it into our tidyflow and visualize the tree: ## ST100Q01TA ## ST102Q01TA ## IC009Q07NA ## ST011Q03TA ## ST011Q05TA ## ST011Q10TA # We can recicle the entire `tflow` from above and just # replace the formula: tflow &lt;- tflow %&gt;% replace_formula(ST102Q01TA ~ .) fit_complex &lt;- fit(tflow) tree &lt;- pull_tflow_fit(fit_complex)$fit rpart.plot(tree) The tree is quite big compared to our previous example and makes the interpretation more difficult. However, equally important, some leaf nodes are very small. Decision trees can capture a lot of noise and mimic the data very closely. \\(6\\) leaf nodes have less than \\(3\\%\\) of the sample. These are leaf nodes with very weak statistical power: What would happen if a tiny \\(1\\%\\) of those leaf nodes respondend slightly different? It is possible we get a complete different tree. Decision trees are not well known for being robust. In fact, it is one of its main weaknesses. However, decision trees have an argument called min_n that force the tree to discard any node that has a number of observations below your specified minimum. Let’s run the model above and set the minimum number of observation per node to be \\(200\\): dectree &lt;- update(mod1, min_n = 200) tflow &lt;- tflow %&gt;% replace_model(dectree) fit_complex &lt;- fit(tflow) tree &lt;- pull_tflow_fit(fit_complex)$fit rpart.plot(tree) The tree was reduced considerably now. There are fewer leaf nodes and all nodes have a greater sample size than before. You might be wondering: what should the minimum sample size be? There is no easy answer for this. The rule of thumb should be relative to your data and research question. In particular, the identification of small nodes should be analyzed with care. Perhaps there is a group of outliers that consitute a node and it’s not a problem of statistical noise. By increasing the minimum sample size for each node you would be destroying that statistical finding. For example, suppose we are studying welfare social expenditure as the dependent variable and then we had other independent variables, among which are country names. Scandinavian countries might group pretty well into a solitary node because they are super powers in welfare spending (these are Denmark, Norway, Sweden and Finland). If we increased the minimum sample size to \\(10\\), we might group them with Germany and France, which are completely different in substantive terms. The best rule of thumb I can recommend is no other than to study your problem at hand with great care and make decisions accordingly. It might make sense to increase the sample or it might not depending on the research question, the sample size, whether you’re exploring the data or whether you’re interested in predicting on new data. Despite min_n helping to make the tree more robust, there are still several nodes with low sample sizes. Another way to approach this problem is through the depth of the tree. As can be seen from the previous plot, decision trees can create leaf nodes which are very small. In other more complicated scenarios, your tree might get huge. Yes, huge: More often that not, these huge trees are just overfitting the data. They are creating very small nodes that capture noise from the data and when you’re predicting on new data, they perform terribly bad. As well as the min_n argument, decision trees have another argument called tree_depth. This argument forces the tree to stop growing if it passes the maximum depth of the tree as measured in nodes. Let’s run our previous example with only a depth of three nodes: dectree &lt;- update(mod1, min_n = 200, tree_depth = 3) tflow &lt;- tflow %&gt;% replace_model(dectree) fit_complex &lt;- fit(tflow) tree &lt;- pull_tflow_fit(fit_complex)$fit rpart.plot(tree) The tree was reduced considerably now in combination with the minimun number of respondents within each node. In fact, there is only one node that has a sample size lower than \\(3\\%\\). The min_n and tree_depth can help you reduce the overfitting of your tree, but don’t think these are easy fixes. Decision trees are simply to easy to overfit the data and as we’ll see, there are more advanced tree methods that can help to fix this. Note that we’ve been interpreting decision trees in a ‘subjective’ fashion. That is, we’ve been cutting the nodes of the trees from subjective criteria that makes sense to our research problem. This is how we social scientists would analyze the data. The tree should model our theoretical problem and make substantive sense. However, for machine learning, we have other criteria: how well it predicts. Let’s check how our model predicts at this point: fit_complex %&gt;% predict_training() %&gt;% rmse(ST102Q01TA, .pred) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 0.514 Our predictions for each set goals is off by around \\(.5\\) in a scale of \\(1\\) through \\(4\\). This is not terribly bad. For example, it means that for every child that answered a \\(2\\), on average, we have an error of around \\(.5\\). This means that any prediction for a single number runs the risk of being wrongly predicting either the number from above or below (a child with a \\(2\\) might get a wrong prediction of \\(3\\) or a \\(1\\) but hardly a \\(4\\)). To improve prediction, we can allow tidyflow to search for the best combination of min_n and tree_depth that maximizes prediction. Let’s perform a grid search for these two tuning values. However, let’s set the range of tuning values ourselves: tune_mod &lt;- update(dectree, min_n = tune(), tree_depth = tune()) tflow &lt;- tflow %&gt;% plug_resample(vfold_cv, v = 5) %&gt;% plug_grid( expand.grid, tree_depth = c(1, 3, 9), min_n = c(50, 100) ) %&gt;% replace_model(tune_mod) fit_tuned &lt;- fit(tflow) fit_tuned %&gt;% pull_tflow_fit_tuning() %&gt;% show_best(metric = &quot;rmse&quot;) ## # A tibble: 5 x 7 ## tree_depth min_n .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 9 50 rmse standard 0.459 5 0.0126 ## 2 9 100 rmse standard 0.459 5 0.0126 ## 3 3 50 rmse standard 0.518 5 0.0116 ## 4 3 100 rmse standard 0.518 5 0.0116 ## 5 1 50 rmse standard 0.649 5 0.0102 It seems that our predictions on the training data were slightly overfitting the data, as the best error from the cross-validation search is centered around 0.459 with a standard error of 0.01. Let’s explore whether the error changes between the minimum sample size and the tree depth: tree_depth_lvl &lt;- paste0(&quot;Tree depth: &quot;, c(1, 3, 9)) fit_tuned %&gt;% pull_tflow_fit_tuning() %&gt;% collect_metrics() %&gt;% mutate(ci_low = mean - (1.96 * std_err), ci_high = mean + (1.96 * std_err), tree_depth = factor(paste0(&quot;Tree depth: &quot;, tree_depth), levels = tree_depth_lvl), min_n = factor(min_n, levels = c(&quot;50&quot;, &quot;100&quot;))) %&gt;% filter(.metric == &quot;rmse&quot;) %&gt;% ggplot(aes(min_n, mean)) + geom_point() + geom_errorbar(aes(ymin = ci_low, ymax = ci_high), width = .1) + scale_x_discrete(&quot;Minimum sample size per node&quot;) + scale_y_continuous(&quot;Average RMSE&quot;) + facet_wrap(~ tree_depth, nrow = 1) + theme_minimal() On the x axis we have the minimum sample size per node (these are the values for min_n) and on the y axis we have the error of the model through cross-validation (the \\(RMSE\\)). The lower each points is on the plot, the better, since it means that the error is lower. Let’s begin with the first plot on the left. The points represent the error of the model with different sample sizes for the nodes with a fixed tree depth of \\(1\\). For a tree depth of \\(1\\), the error of the model is around .65. However, as the number of trees increases (the additional plots to the right), the error comes down to nearly .47 when there is a tree_depth of 9. It seems that the simplest model with the lowest \\(RMSE\\) has a tree_depth of 9 and a minimum sample size of 50. We calculated this ourselves for this example, but complete_tflow can calculate this for you: final_model &lt;- fit_tuned %&gt;% complete_tflow(metric = &quot;rmse&quot;, tree_depth, method = &quot;select_by_one_std_err&quot;) train_err &lt;- final_model %&gt;% predict_training() %&gt;% rmse(ST102Q01TA, .pred) test_err &lt;- final_model %&gt;% predict_testing() %&gt;% rmse(ST102Q01TA, .pred) c(&quot;testing error&quot; = test_err$.estimate, &quot;training error&quot; = train_err$.estimate) ## testing error training error ## 0.4644939 0.4512248 Our testing error and our training error have a difference of only \\(0.01\\), not bad. The cross-validation tuning seemed to have helped avoid a great deal of overfitting. Before we go through the next section, I want to briefly mention an alternative to tree_depth and min_n. A technique called ‘tree pruning’ is also very common for modeling decision trees. It first grows a very large and complex tree and then starts pruning the leafs. This technique is also very useful but due to the lack of time, we won’t cover this in the course. You can check out the material on this technique from the resources outlined in the first paragraph of this section. 3.1.1 Advanced: how do trees choose where to split? Throughout most of the chapter we’ve seen that trees find optimal ‘splits’ that make the respondents very different between the splits and very similar within them. But how do decision trees make these splits? Let’s work out a simple example using the HISEI variable from the first model in this section. HISEI is an index for the socio-economic status of families. It’s continuous and has a distribution like this: As we saw in the first tree of this section, HISEI is the root node. To decide on the root node, the decision tree algorithm chooses a random location in the distribution of HISEI and draws a split: The two sides have an average math_score which serves as the baseline for how different these two groups are. At this point, the algorithm does something very simple: for each split, it calculates the Residual Sum of Squares (RSS). This is just the sum of the math_score of each respondent (\\(math_i\\)) minus the average math_score (\\(\\hat{math}\\)) for that split squared. In other words, it applies the \\(RSS\\) for each split: \\[\\begin{equation} RSS = \\sum_{k = 1}^n(math_i - \\hat{math})^2 \\end{equation}\\] Each side of the split then has a corresponding \\(RSS\\): After that, it calculates the total \\(RSS\\) of the split by adding the two \\(RSS\\): So far we should have a single random split with an associated \\(RSS\\) for \\(HISEI\\). The decision tree algorithm is called recursive binary splitting because it is recursive: it repeats itself again many times. It repeats the strategy of \\(Split\\) -&gt; \\(RSS_{split}\\) -&gt; \\(RSS_{total}\\) many times such that we get a distribution of splits and \\(RSS\\) for \\(HISEI\\): This produces a distribution of random splits with an associated metric of fit (\\(RSS\\)) for \\(HISEI\\). Recursive binary splitting applies this same logic to every single variable in the model such that you have a distribution of splits for every single variable: ## # A tibble: 8 x 3 ## variable random_split total_rss ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &quot;HISEI&quot; 41.22 &quot;Total RSS: 34423362&quot; ## 2 &quot;HISEI&quot; 53.77 &quot;Total RSS: 34400218&quot; ## 3 &quot;HISEI&quot; 56.57 &quot;Total RSS: 32523560&quot; ## 4 &quot;&quot; ... &quot;&quot; ## 5 &quot;FISCED&quot; 2 &quot;Total RSS: 35901660&quot; ## 6 &quot;FISCED&quot; 1 &quot;Total RSS: 36085201&quot; ## 7 &quot;FISCED&quot; 5 &quot;Total RSS: 34083264&quot; ## 8 &quot;&quot; ... &quot;&quot; With such a distribution, the algorithm can objectively ask: which random split best separates the data into two branches with the lowest \\(RSS\\)? And based on that answer, the first node is chosen. After this first node is chosen, two branches grow to both sides. The algorithm then applies exactly the same set of rules recursively for each branch until a maximum depth is reached. Although this explanation will be in nearly all cases invisible to you, this intuition can help you understand better which criteria is used for choosing a split. For example, understanding how this splitting is done gives you insight into how outliers do not affect the selection of splits because the splitting criteria is random and navigates the entire distribution. In addition, there might be cases where you might want to switch the \\(RSS\\) for another loss function because it makes sense for your problem. For example, using decision trees with binary dependent variables merits another type of loss function: Gini impurity. We won’t delve into this but it serves as an example that these are things which are not fixed. These are decision that depend on your research problem and it might make sense to experiment with them if needed. 3.2 Bagging The problem with decision trees is that even if you work really hard to avoid overfitting, they can be very susceptible to the exact format of the data. For some cases, you might even get completely different trees every time you run your model. Quite literally, running the same model might offer very different trees if some part of the sample changes. This small simluation predicts math_score on all variables in the pisa data set but samples different percentages of the whole sample: This drastic differences between trees is because decision trees have a lot of variance and very little bias. They learn the current data very well (little bias) but if you generalize them to new data, they can perform very badly (a lot of variance). This is where bagging, or Bootstrap Aggregation comes in. Before we explain what bagging is all about, let’s spend a minute explaining what bootstrapping is. Let’s work out a manual example and limit our pisa dataset to only five rows, keep a few selected columns and add a unique id for each row: sel_cols &lt;- c(&quot;math_score&quot;, &quot;HISEI&quot;, &quot;REPEAT&quot;, &quot;IMMIG&quot;, &quot;read_score&quot;) pisa_small &lt;- pisa[1:5, sel_cols] pisa_small$id &lt;- 1:5 pisa_small ## math_score HISEI REPEAT IMMIG read_score id ## 1 512.7125 28.60 0 1 544.2085 1 ## 2 427.3615 59.89 0 1 432.2518 2 ## 3 449.9545 39.02 0 1 503.9496 3 ## 4 474.5553 26.60 0 1 437.7777 4 ## 5 469.1545 76.65 0 1 535.9487 5 Bootstraping is a statistical technique where you resample your data to such that some rows are randomly duplicated. We can do this manually in R: # Sample from the number of rows in `pisa_small` # and allow certain numbers to be replaced. set.seed(23551) row_index &lt;- sample(nrow(pisa_small), replace = TRUE) pisa_small[row_index, ] ## math_score HISEI REPEAT IMMIG read_score id ## 1 512.7125 28.60 0 1 544.2085 1 ## 4 474.5553 26.60 0 1 437.7777 4 ## 4.1 474.5553 26.60 0 1 437.7777 4 ## 3 449.9545 39.02 0 1 503.9496 3 ## 5 469.1545 76.65 0 1 535.9487 5 We randomly sampled the same number of rows and got the respondent number four repeated twice. We can run this many times and get many resamples of our data: lapply(1:2, function(x) { row_index &lt;- sample(nrow(pisa_small), replace = TRUE) pisa_small[row_index, ] }) ## [[1]] ## math_score HISEI REPEAT IMMIG read_score id ## 3 449.9545 39.02 0 1 503.9496 3 ## 5 469.1545 76.65 0 1 535.9487 5 ## 3.1 449.9545 39.02 0 1 503.9496 3 ## 1 512.7125 28.60 0 1 544.2085 1 ## 3.2 449.9545 39.02 0 1 503.9496 3 ## ## [[2]] ## math_score HISEI REPEAT IMMIG read_score id ## 1 512.7125 28.6 0 1 544.2085 1 ## 4 474.5553 26.6 0 1 437.7777 4 ## 4.1 474.5553 26.6 0 1 437.7777 4 ## 4.2 474.5553 26.6 0 1 437.7777 4 ## 4.3 474.5553 26.6 0 1 437.7777 4 Since the number of rows that we sample is random, in some instances we might the same row repeated 10 times, in others only 1 time and other even 0 times! This is what bootstrapping is all about. If we run \\(10\\) bootstraps, it just means we got the same 10 datasets but with some rows repeated many times and others randomly removed. Bootstrapping is mainly used to calculate statistics such as standard errors and standard deviations because it has very nice properties to estimate uncertainty in situations where its impossible to calculate it. However, it also has advantages for reducing the variance in models such as decision trees. Let’s get back to how bagging works. Bagging works by bootstraping your data \\(N\\) times and fitting \\(N\\) decision trees. Each of these decision trees has a lot of variance because we allow the tree to overfit the data. The trick with bagging is that we average over the predictions of all the \\(N\\) decision trees, improving the high variability of each single decision tree. In the same spirit as before, let’s work out a manual example just so you can truly grasp that intuition. However, don’t worry, there are functions inside tidymodels and tidyflow that will perform all of this for you. Let’s adapt the code from above to use the original pisa data, sample only 60% of the data in each bootstrap and generate 20 copies of our data with random picks of rows in each iteration: pisa$id &lt;- 1:nrow(pisa) bootstrap_pisa &lt;- lapply(1:20, function(x) { row_index &lt;- sample(nrow(pisa) * .6, replace = TRUE) pisa[row_index, ] }) The result is named bootstrap_pisa and is list with 20 data frames. You can inspect the first two with bootstrap_pisa[[1]] and bootstrap_pisa[[2]]. Inside each of these, there should be a data frame with 60% of the original number of rows of the pisa data where each row was randomly picked. Some of these might be repeated many times, other might just be there once and others might not even be there. Let’s now loop over these 20 datasets, fit a decision tree to each one and predict on the original pisa data. The result of this loop should be 20 data frames each with a prediction for every respondent: tflow &lt;- tidyflow() %&gt;% plug_formula(math_score ~ .) %&gt;% plug_model(decision_tree(mode = &quot;regression&quot;) %&gt;% set_engine(&quot;rpart&quot;)) all_pred_models &lt;- lapply(bootstrap_pisa, function(x) { small_model &lt;- tflow %&gt;% plug_data(x) %&gt;% fit() cbind( pisa[&quot;id&quot;], predict(small_model, new_data = pisa) ) }) The first slot contains predictions for all respondents. Let’s look confirm that: head(all_pred_models[[1]]) ## id .pred ## 1 1 493.6071 ## 2 2 378.5172 ## 3 3 440.5835 ## 4 4 440.5835 ## 5 5 493.6071 ## 6 6 440.5835 Let’s confirm the same thing for the second slot: head(all_pred_models[[2]]) ## id .pred ## 1 1 486.7747 ## 2 2 432.6909 ## 3 3 432.6909 ## 4 4 432.6909 ## 5 5 486.7747 ## 6 6 486.7747 The second slot also contains predictions for all respondents but they are different from the first one because they are based on a random sample. This same logic is repeated 20 times such that every respondent has 20 predictions. The trick behind bagging is that it averages the prediction of each respondent over the 20 bootstraps. This averaging has two advantages. First, it allows each single tree to grow as much as possible, allowing it to have a lot of variance and little bias. This has a good property which is little bias but a negative aspect, which is a lot of variance. Bagging compensates this high level of variance by averaging the predictions of all the small trees: # Combine all the 20 predictions into one data frame all_combined &lt;- all_pred_models[[1]] for (i in seq_along(all_pred_models)[-1]) { all_combined &lt;- cbind(all_combined, all_pred_models[[i]][-1]) } # Average over the 20 predictions res &lt;- data.frame(id = all_combined[1], final_pred = rowMeans(all_combined[-1])) head(res) ## id final_pred ## 1 1 494.1934 ## 2 2 403.6330 ## 3 3 436.1936 ## 4 4 443.5922 ## 5 5 491.6506 ## 6 6 457.9670 We get a final prediction for each respondent. If we wanted to, we could calculate the standard deviation of these 20 predictions for each respondent and generate uncertainty intervals around each respondent’s predictions. More often than not, this is a good idea. In the previous example we used 20 bootstraps for the sake of simplicity but generally speaking as the number of trees increases, the less variance we will have in the final prediction and thus a stronger prediction. We can see more clearly the power of combining many trees with the simulation below: The x axis shows the number of bootstraps (or fitted trees, it’s the same) and the y axis shows the average \\(RMSE\\) in math_score for each of these bagged trees. As we increase the number of small trees (or bootstraps, it’s the same), there is a substantial reduction in the error rate of math_score. This is an impressive reduction relative to our initial decision tree. Having seen the power of increasing the number of trees, how many trees should your model use? For models which exhibit reasonable levels of variability (like our math_score example), \\(100\\)-\\(200\\) bootstraps is often enough to stabilize the error in the predictions. However, very unstable models might require up to \\(500\\). Let’s fit the same model we implemented manually above using tidymodels and tidyflow. Bagged trees can be implemented with the function bag_tree from the package baguette. With this package we can control the number of bootstraps with the argument times. We can define our model as usual using tidyflow: btree &lt;- bag_tree(mode = &quot;regression&quot;) %&gt;% set_engine(&quot;rpart&quot;, times = 50) tflow &lt;- pisa %&gt;% tidyflow(seed = 566521) %&gt;% plug_split(initial_split) %&gt;% plug_formula(math_score ~ .) %&gt;% plug_model(btree) tflow ## ══ Tidyflow ════════════════════════════════════════════════════════════════════ ## Data: 4.84K rows x 502 columns ## Split: initial_split w/ default args ## Formula: math_score ~ . ## Resample: None ## Grid: None ## Model: ## Bagged Decision Tree Model Specification (regression) ## ## Main Arguments: ## cost_complexity = 0 ## min_n = 2 ## ## Engine-Specific Arguments: ## times = 50 ## ## Computational engine: rpart You might be asking yourself, why don’t we define bootstraps inside plug_resample? After all,bootstraps is a resampling technique. We could do that but it doesn’t make sense in this context. plug_resample is aimed more towards doing grid search of tuning values together with plug_grid. Since bag_trees is not performing any type grid search but rather fitting a model many times and making predictions, it automatically incorporates this procedure inside bag_trees. If instead we were doing a grid search of let’s say, min_n and tree_depth for bag_tree, using plug_resample with boostraps would be perfectly reasonable. Let’s fit both a simple decision tree and the bagged decision tree, predict on the training set and record the average \\(RMSE\\) for both: res_btree &lt;- tflow %&gt;% fit() res_dtree &lt;- tflow %&gt;% replace_model(decision_tree() %&gt;% set_engine(&quot;rpart&quot;)) %&gt;% fit() rmse_dtree &lt;- res_dtree %&gt;% predict_training() %&gt;% rmse(math_score, .pred) rmse_btree &lt;- res_btree %&gt;% predict_training() %&gt;% rmse(math_score, .pred) c(&quot;Decision tree&quot; = rmse_dtree$.estimate, &quot;Bagged decision tree&quot; = rmse_btree$.estimate) ## Decision tree Bagged decision tree ## 33.85131 11.33018 The bagged decision tree improves the error rate from \\(33\\) math test points to \\(11\\). That is a \\(66\\%\\) reduction in the error rate! That is an impressive improvement for such a simple extension of decision trees. As all other models, bagging also has limitations. First, although bagged decision trees offer improved predictions over decision trees, they do this at the expense of interpretability. Unfortunately, there is no equivalent of an ‘average’ tree that we can visualize. Remember, we have \\(100\\) predictions from \\(100\\) different trees. It is not possible nor advisable to visualize \\(100\\) trees. Instead, we can look at the average variable importance. Bagging offers the ‘contribution’ of each variable using loss functions. For continuous variables, it uses the \\(RSS\\) (which we have described and used throughout this chapter) and for binary variables it uses the Gini index. We can look at the importance of the variables to get a notion of which variables are the important ones for good prediction: res_btree %&gt;% pull_tflow_fit() %&gt;% .[[&#39;fit&#39;]] %&gt;% var_imp() ## # A tibble: 501 x 4 ## term value std.error used ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 scie_score 23363949. 75426. 50 ## 2 read_score 17033482. 69939. 50 ## 3 ST166Q03HA 5913918. 66479. 50 ## 4 METASPAM 5671665. 68871. 50 ## 5 IC152Q08HA 3850699. 304274. 49 ## 6 PISADIFF 3046729. 362250. 50 ## 7 IC010Q06NA 2691482. 355147. 50 ## 8 ST013Q01TA 433681. 142604. 50 ## 9 ESCS 329367. 16981. 50 ## 10 HOMEPOS 258437. 11440. 50 ## # … with 491 more rows Secondly, bagging might seem like a deal breaker for any type of model (you can apply it to any type of model such as logistic regression, regularized regression, etc..) but it works well only for models which are very unstable. For example, linear regression and logistic regression are very robust models. With enough sample size, running a bagged linear regression should return very similar estimates as a single fitted model. 3.3 Exercises Explain that you have not performed any grid search for bagged trees but that they have to do it themselves. References "],
["syllabus.html", "Chapter 4 Syllabus 4.1 Course description 4.2 Schedule 4.3 Software 4.4 Prerequisites 4.5 About the author", " Chapter 4 Syllabus 4.1 Course description With the increasing amounts of data being collected on a daily basis, the field of machine learning has gained mainstream attention. By shifting away from focusing on inference, machine learning is a field at the intersection of statistics and computer science that is focused on maximizing predictive performance by learning patterns from data. That is, the goal of machine learning is to predict something – and predict it very well, regardless of whether you understand it. These techniques are common in business settings where, for example, stakeholders are interested in knowing the probability of a client leaving a company or the propensity of a client for buying a particular product. The field can be intimidating as it is vast and growing every year. However, scholars in the social sciences are beginning to understand the importance of the machine learning framework and how it can unlock new knowledge in fields such as sociology, political science, economics and psychology. On this course we will introduce students to the basic ideas of the machine learning framework and touch upon the basic algorithms used for prediction and discussing the potential it can have in the social sciences. In particular, we will introduce predictive algorithms such as regularized regressions, classification trees and clustering techniques through basic examples. We will discuss their advantages and disadvantages while paying great attention to how it’s been used in research. Although many social scientists do not see how predictive models can help explain social phenomena, we will also focus on how machine learning can play a role as a tool for discovery, improving causal inference and generalizing our classical models through cross validation. We will end the course with a prediction challenge that will put to test all of your acquired knowledge. Starting with a discussion on the role of predictive challenges such as the Fragile Families Challenge in the social sciences, our predictive challenge will require the student to run machine learning algorithms, test their out-of-sample error rate and discuss strategies on how the results are useful. This will give the class a real hands-on example of how to incorporate machine learning into their research right away. Below is a detailed description of the syllabus. 4.2 Schedule Session 1 July 6th 09:00h-10:45h Introduction to the Machine Learning Framework Inference vs Prediction Can inference and prediction complement each other? Bias-variance / Interpretability-prediction tradeoffs Resampling for understtanding your model “The Fragile Families Challenge” Readings: Sections 2.1 and 2.2 from James, Gareth, et al. An Introduction To Statistical Learning. Vol. 112. New York: springer, 2013 Molina, M., &amp; Garip, F. (2019). Machine Learning for Sociology. Annual Review of Sociology, 45. Hofman, J. M., Sharma, A., &amp; Watts, D. J. (2017). Prediction and explanation in social systems. Science, 355(6324), 486-488. Mullainathan, S., &amp; Spiess, J. (2017). Machine learning: an applied econometric approach. Journal of Economic Perspectives, 31(2), 87-106. Watts, D. J. (2014). Common sense and sociological explanations. American Journal of Sociology, 120(2), 313-351. Breiman, L. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199-231. Break 10:45h-11:15h Session 2 July 6th 11:15h-13:00h Linear regression and regularization Continuous predictions and loss functions Lasso Advantages/Disadvantages R example Ridge regression Advantages/Disadvantages R example Elastic Net Advantages/Disadvantages R example Exercises Readings: For a theoretical introduction to Lasso/Ridge, sections 6.1, 6.2 and 6.6 from James, Gareth, et al. (2013) An Introduction To Statistical Learning. Vol. 112. New York: springer For hands-on examples, Chapter 6 of Boehmke &amp; Greenwell (2019) Hands-On Machine Learning with R, 1st Edition, Chapman &amp; Hall/CRC The R Series. Accessible at: https://bradleyboehmke.github.io/HOML/ Session 3 July 7th 09:00h-10:45h Supervised Regression Introduction to supervised regression Classification Confusion matrices ROC Curves Classification Trees Advantages/Disadvantages R example Exercises Readings: For an introduction to classification trees, Section 8.1, 8.3.1 and 8.3.2 from James, Gareth, et al. An Introduction To Statistical Learning. Vol. 112. New York: springer, 2013 For hands-on examples, chapter 9 from Boehmke &amp; Greenwell (2019) Hands-On Machine Learning with R, 1st Edition, Chapman &amp; Hall/CRC The R Series. Accessible at: https://bradleyboehmke.github.io/HOML/ For real-world applications of Classification Trees: Billari, F. C., Fürnkranz, J., &amp; Prskawetz, A. (2006). Timing, sequencing, and quantum of life course events: A machine learning approach. European Journal of Population/Revue Européenne de Démographie, 22(1), 37-65. Chapter 3 of Nolan, D., &amp; Lang, D. T. (2015). Data science in R: a case studies approach to computational reasoning and problem solving. CRC Press. Break 10:45h-11:15h Session 4 July 7th 11:15h-13:00h Supervised Regression Bagging Advantages/Disadvantages R example Random Forest Advantages/Disadvantages R example Gradient Boosting Advantages/Disadvantages R example Exercises Readings: For an introduction to bagging/random forests/boosting, Chapter 8 from James, Gareth, et al. An Introduction To Statistical Learning. Vol. 112. New York: springer, 2013 For hands-on examples, chapter 10, 11 and 12 from Boehmke &amp; Greenwell (2019) Hands-On Machine Learning with R, 1st Edition, Chapman &amp; Hall/CRC The R Series. Accessible at: https://bradleyboehmke.github.io/HOML/ For real-world applications of Random Forests: Perry, C. (2013). Machine learning and conflict prediction: a use case. Stability: International Journal of Security and Development, 2(3), 56. Berk, R. A., Sorenson, S. B., &amp; Barnes, G. (2016). Forecasting domestic violence: A machine learning approach to help inform arraignment decisions. Journal of Empirical Legal Studies, 13(1), 94-115. Session 5 July 8th 09h-10:45h Unsupervised Regression Introduction to unsupervised learning Principal Component Analysis (PCA) Advantages/Disadvantages R example K-Means clustering Advantages/Disadvantages R example Exercises Readings: For an introduction to unsupervised learning, Section 10.1 from James, Gareth, et al. An Introduction To Statistical Learning. Vol. 112. New York: springer, 2013 For an introduction to PCA Section 10.2 and 10.4 from James, Gareth, et al. An Introduction To Statistical Learning. 112. New York: springer, 2013 For hands-on examples, chapter 17 from Boehmke &amp; Greenwell (2019) Hands-On Machine Learning with R, 1st Edition, Chapman &amp; Hall/CRC The R Series. Accessible at: https://bradleyboehmke.github.io/HOML/ For an introduction to K-Means clustering Section 10.5 from James, Gareth, et al. An Introduction To Statistical Learning. Vol. 112. New York: springer, 2013 For hands-on examples, chapter 20 from Boehmke &amp; Greenwell (2019) Hands-On Machine Learning with R, 1st Edition, Chapman &amp; Hall/CRC The R Series. Accessible at: https://bradleyboehmke.github.io/HOML/ For real-world applications of K-means clustering: Garip, F. (2012). Discovering diverse mechanisms of migration: The Mexico–US Stream 1970–2000. Population and Development Review, 38(3), 393-433. Bail, C. A. (2008). The configuration of symbolic boundaries against immigrants in Europe. American Sociological Review, 73(1), 37-59. Break 10:45h-11:15h Session 6 July 8th 11:15h-13:00h Unsupervised Regression Hierarchical clustering Advantages/Disadvantages R example Final challenge: Prediction competition Explanation of strategies No free lunch theorem Presentation of results Readings: For an introduction to hierarchical clustering, sections 10.3.2, 10.3.3, 10.5.2 from James, Gareth, et al. An Introduction To Statistical Learning. Vol. 112. New York: springer, 2013 For hands-on examples, chapter 21 from Boehmke &amp; Greenwell (2019) Hands-On Machine Learning with R, 1st Edition, Chapman &amp; Hall/CRC The R Series. Accessible at: https://bradleyboehmke.github.io/HOML/ For examples on prediction competitions: Glaeser, E. L., Hillis, A., Kominers, S. D., &amp; Luca, M. (2016). Crowdsourcing city government: Using tournaments to improve inspection accuracy. American Economic Review, 106(5), 114-18. Salganik, M. J., Lundberg, I., Kindel, A. T., &amp; McLanahan, S. (2019). Introduction to the Special Collection on the Fragile Families Challenge. Socius, 5, 2378023119871580. Accessible at https://www.researchgate.net/publication/335733962_Introduction_to_the_Special_Collection_on_the_Fragile_Families_Challenge 4.3 Software We will be using the R software together with the Rstudio interface. No laptop is required as the seminars will take place in the RECSM facilities. Any packages we plan to use will be already downloaded previous to the session. 4.4 Prerequisites The course assumes that the student is familiar with R and should be familiar with reading, manipulating and cleaning data frames. Ideally, the student has conducted some type of research using the software. Students should have solid knowledge of basic statistics such as linear and logistic regression, ideally with more advanced concepts such as multilevel modelling. 4.5 About the author Jorge Cimentada has a PhD in Sociology from Pompeu Fabra University and is currently a Research Scientist at the Laboratory of Digital and Computational Demography at the Max Planck Institute for Demographic Research. His research is mainly focused on the study of educational inequality, inequality in spatial mobility and computational social science. He has worked on data science projects both in the private sector and in academic research and is interested in merging cutting edge machine learning techniques with classical social statistics. You can check out his blog at cimentadaj.github.io or contact him through twitter at @cimentadaj. "]
]
