---
title: 'An introduction to regularization'
author: Jorge Cimentada
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r knitr-setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      fig.path = "../figs/",
                      fig.align = "center",
                      fig.asp = 0.618,
                      out.width = "80%")
```


Example of a true model with p > n running on OLS and regularization
Example showing the collinearity doesn't not affect prediction
Example showing that regularization can be used where there is a lot of collinearity to improve accuracy

This penalty parameter constrains the size of the coefficients such that the
only way the coefficients can increase is if we experience a comparable decrease
in the sum of squared errors (SSE).

Ridge works well for reducing the effect to zero but keeping all variables in the model

Hyperparameters are values determined by the analyst

## Ridge regularization

Do no let others fool you into thinking that ridge regression is a fancy AI algorithm. Are you familiar with linear regression? If you are, then ridge regression is just a very **simple** adaptation of linear regression. 

The whole aim of linear regression, or Ordinary Least Squares (OLS), is to minimize the sum of the squared residuals. In other words, fit `N` number of regression lines to the data and keep only the one that has the lowest sum of squared residuals. In simple formula jargon, OLS tries to **minimize** this:

\begin{equation}
RSS = \sum_{k = 1}^n(actual_i - predicted_i)^2
\end{equation}

For each fitted regression line, you compare the predicted value ($predicted_i$) versus the actual value ($actual_i$), square it, and the add it all up. Each fitted regression line then has an associated Residual Sum of Squares (RSS) and the linear model choose the line with the lowest RSS.

> Note: Social scientists are familiar with the RSS and call it just by it's name. However, be aware that in Machine Learning jargon, the RSS belongs to a general family called  **loss functions**. Loss functions are metrics that evaluate the **fit** of your model and there are many around (such as AIC or BIC, R2).

Ridge regression takes the previous RSS loss function and adds one term:

\begin{equation}
RSS + \lambda \sum_{k = 1}^n \beta^2_j
\end{equation}

The new term is called a *shrinkage penalty* because it forces each coefficient $\beta_j$ closer to zero by squaring it. The shrinkage part is clearer once you think of this term as forcing each coefficient to be as small as possible but also considering having the smallest Residual Sum of Squares (RSS). In other words, we want the smallest coefficients that doesn't affect the fit of the line (RSS).

An intuitive example is to think of RSS and $\sum_{k = 1}^n \beta^2_j$ as to separate things. RSS estimates how the model fits the data and $\sum_{k = 1}^n \beta^2_j$ limits how much the data is being fit. Finally, the little $\lambda$ between these two terms can be interpreted as a "weight". The higher the lambda, the higher the weight that will be given to the shrinkage term of the equation. If $\lambda$ is 0, then multiplying 0 by $\sum_{k = 1}^n \beta^2_j$ will always return zero forcing our previous equation to simply be reduced to the single term $RSS$.

Why is there a need to "limit" how well the model fits the data? Because we, social scientists and data scientists, can **overfit** the data. The plot below shows a simulation from [Simon Jackson](https://drsimonj.svbtle.com/ridge-regression-with-glmnet) where we can see that when tested on a training set, OLS and Ridge tend to overfit the data. However, when tested on the test data, Ridge regression has lower out of sample error as the $R2$ is higher for models with different observations.

```{r, echo = FALSE}
# This chunk was taken and adapted from https://drsimonj.svbtle.com/ridge-regression-with-glmnet
library(broom)
library(glmnet)
library(tidyverse)

# Compute R^2 from true and predicted values
rsquare <- function(true, predicted) {
  sse <- sum((predicted - true)^2)
  sst <- sum((true - mean(true))^2)
  rsq <- 1 - sse / sst

  # For this post, impose floor...
  if (rsq < 0) rsq <- 0

  return (rsq)
}

# Train ridge and OLS regression models on simulated data set with `n_train`
# observations and a number of features as a proportion to `n_train`,
# `p_features`. Return R squared for both models on:
#   - y values of the training set
#   - y values of a simualted test data set of `n_test` observations
#   - The beta coefficients used to simulate the data
ols_vs_ridge <- function(n_train, p_features, n_test = 200) {
  ## Simulate datasets
  n_features <- floor(n_train * p_features)
  betas <- rnorm(n_features)

  x <- matrix(rnorm(n_train * n_features), nrow = n_train)
  y <- x %*% betas + rnorm(n_train)
  train <- data.frame(y = y, x)

  x <- matrix(rnorm(n_test * n_features), nrow = n_test)
  y <- x %*% betas + rnorm(n_test)
  test <- data.frame(y = y, x)

  ## OLS
  lm_fit <- lm(y ~ ., train)

  # Match to beta coefficients
  lm_betas <- tidy(lm_fit) %>%
    filter(term != "(Intercept)") %>%
    {.$estimate}
  lm_betas_rsq <- rsquare(betas, lm_betas)

  # Fit to training data
  lm_train_rsq <- glance(lm_fit)$r.squared

  # Fit to test data
  lm_test_yhat <- predict(lm_fit, newdata = test)
  lm_test_rsq  <- rsquare(test$y, lm_test_yhat)

  ## Ridge regression
  lambda_vals <- 10^seq(3, -2, by = -.1)  # Lambda values to search
  cv_glm_fit  <- cv.glmnet(as.matrix(train[,-1]), train$y, alpha = 0, lambda = lambda_vals, nfolds = 5)
  opt_lambda  <- cv_glm_fit$lambda.min  # Optimal Lambda
  glm_fit     <- cv_glm_fit$glmnet.fit

  # Match to beta coefficients
  glm_betas <- tidy(glm_fit) %>%
    filter(term != "(Intercept)", lambda == opt_lambda) %>% 
    {.$estimate}
  glm_betas_rsq <- rsquare(betas, glm_betas)

  # Fit to training data
  glm_train_yhat <- predict(glm_fit, s = opt_lambda, newx = as.matrix(train[,-1]))
  glm_train_rsq  <- rsquare(train$y, glm_train_yhat)

  # Fit to test data
  glm_test_yhat <- predict(glm_fit, s = opt_lambda, newx = as.matrix(test[,-1]))
  glm_test_rsq  <- rsquare(test$y, glm_test_yhat)

  data.frame(
    model = c("OLS", "Ridge"),
    betas_rsq  = c(lm_betas_rsq, glm_betas_rsq),
    train_rsq = c(lm_train_rsq, glm_train_rsq),
    test_rsq = c(lm_test_rsq, glm_test_rsq)
  )

}

# Function to run `ols_vs_ridge()` `n_replications` times
repeated_comparisons <- function(n_train, p_features, n_replications = 1) {
  map(seq(n_replications), ~ ols_vs_ridge(n_train, p_features)) %>% 
    map2(seq(.), ~ mutate(.x, replicate = .y)) %>% 
    reduce(rbind)
}

set.seed(23141)
d <- purrr::cross_df(list(
  n_train = seq(20, 200, 20),
  p_features = .95
))

d <-
  d %>% 
  mutate(results = map2(n_train, p_features, repeated_comparisons))

d %>%
  unnest() %>% 
  group_by(model, n_train) %>%
  summarise(
    train_rsq = mean(train_rsq),
    test_rsq = mean(test_rsq)) %>% 
  gather(data, rsq, contains("rsq")) %>%
  ungroup() %>% 
  mutate(data = factor(gsub("_rsq", "", data), levels = c("train", "test"))) %>% 
  ggplot(aes(n_train, rsq, color = model)) +
  geom_line() +
  geom_point(size = 4, alpha = .3) +
  facet_wrap(~ data) +
  theme_minimal() +
  labs(x = "Number of training observations",
       y = "R squared")

```

The strength of the ridge regression comes from the fact that it compromises fitting the training data really well for improved generalization. In other words, we increase **bias** for lower **variance**.

The whole gist behind ridge regression is penalizing very large coefficients for better generalization. Having that intuition in mind, the predictors of the ridge regression need to be standardized. Why is this the case? Because due to the scale of a predictor, its coefficient can be more penalized than other predictors. Suppose that you have the income of a particular person (measured in thousands per months) and time spent with their families (measured in seconds) and you're trying to predict happiness. A one unit increase in salary could be penalized much more than a one unit increase in time spent with their families just because a one unit increase in salary is much bigger due to it's metric.

In R, you can fit a ridge regression (and nearly all other machine learning models) through the `caret` package. Let's load the packages that we will work with and read the data:

```{r}
library(caret) # Fitting machine learning models
library(rsample) # Create data partitions
library(vip) # For figuring out important variables for prediction

pisa <- read.csv("./data/pisa_us_2018.csv")
```

We separate the training and test data. All of our model tuning will be done on the training data and the test data is saved for later (the test data must be completely ignored until you have your final tuned model).

```{r}

# Separate training/testing split
set.seed(23141)
split_pisa <- initial_split(data = pisa, prop = .7)
pisa_test <- testing(split_pisa)
pisa_train <- training(split_pisa)

```

The ridge regression has a parameter called alpha which needs to be set by us. alpha is the "weight" term in the ridge equation, which controls how much weight do we want to give to the "shrinkage penalty". If this lambda is 0, it means we attach **no** weight to the penalty term and we will get the same result over OLS. Let's try that:

```{r}
ridge_grid <- data.frame(
  # Here we specify the lambda to be zero
  lambda = 0,
  # Here we specify the type of penalized regression: 0 is ridge regression
  alpha = 0
)

# The train function accepts several arguments
ridge_mod <- train(
  # math_score is the dependen variable and all other are independent variables
  math_score ~ MISCED + FISCED + HISEI + REPEAT + IMMIG + DURECEC + BSMJ,
  # The training data
  data = pisa_train,
  # The R package that runs the ridge regression
  method = "glmnet",
  # Here is where we pass the lambda argument
  tuneGrid = ridge_grid,
  lambda = 0,
  # Here is where the function standardizes the predictors before
  # fitting the models
  preProc = c("center", "scale"),
  trControl = trainControl(method = "none")
)

# Get ridge coefficients
res <- ridge_mod$finalModel
ridge_coef <- predict(res, s = 0, type = "coefficients")

# Get linear model coefficients
iv_vars <- c("MISCED", "FISCED", "HISEI", "REPEAT", "IMMIG", "DURECEC", "BSMJ")
pisa_tst <- pisa_train
pisa_tst[iv_vars] <- scale(pisa_tst[iv_vars])

lm_coef <- coef(
  lm(math_score ~ MISCED + FISCED + HISEI + REPEAT + IMMIG + DURECEC + BSMJ,
     data = pisa_tst)
)

comparison <-
  data.frame(coefs = names(lm_coef),
             `Linear coefficients` = unname(round(lm_coef, 2)),
             `Ridge coefficients` = round(as.vector(ridge_coef), 2))

knitr::kable(comparison)
```

Coming from a social science background, it might seem counterintuitive that the researcher has to specify tuning parameters for the model. In traditional social science statistics, models usually estimate similar values internally and the user doesn't have to think about them. However, there are strategies already implemented to explore the combination of many possible values. With out previous example, we just have to add a number of lambda values and `train` will find the best one:


```{r}
set.seed(663421)

ridge_grid <- data.frame(
  # Here we specify the lambda to several possible values
  lambda = seq(0, 3, length.out = 300),
  alpha = 0
)

ridge_mod <- train(
  math_score ~ MISCED + FISCED + HISEI + REPEAT + IMMIG + DURECEC + BSMJ,
  data = pisa_train,
  method = "glmnet",
  tuneGrid = ridge_grid,
  preProc = c("center", "scale"),
  # Performs cross validation through all grid parameters
  trControl = trainControl(method = "cv")
)

plot(ridge_mod$finalModel, xvar = "lambda", label = TRUE)
```

Here we can see how our coefficients are affected by increasing weight of the lamba parameter. And we can figure out the best lambda inspecting `bestTune` inside `ridge_mod`:

```{r}
best_lambda_ridge <- ridge_mod$bestTune$lambda
```

However, there's no need to rerun the model with this optimal value; since `train` **had** to run that model, it saves it as the most optimal:

```{r}

holdout_ridge <-
  RMSE(
    predict(ridge_mod, pisa_test, s = best_lambda_ridge),
    pisa_test$math_score
  )

train_rmse_ridge <-
  ridge_mod$results %>%
  filter(lambda == best_lambda_ridge) %>%
  pull(RMSE)

c(holdout_rmse = holdout_ridge, train_rmse = train_rmse_ridge)
```

The holdout RMSE will always be higher than the training as the training set nearly always **memorizes** the data slightly better for the training.


## Lasso regularization

The Lasso regularization is very similar to the ridge regularization where only one thing changes: the penalty term. Instead of squaring the coefficients in the penalty term, the lasso regularization takes the absolute value of the coefficient.

\begin{equation}
RSS + \lambda \sum_{k = 1}^n |\beta_j|
\end{equation}

Althought it might not be self-evident from this, the lasso reguralization has an important distinction: it can force a coefficient to be zero. This means that lasso does a selection of variables which have big coefficients while not compromising the RSS of the model. 

For example, if we define the same model from above using a lasso, you'll see that it forces coefficients to be **exactly zero** if they don't add anything relative to the RSS of the model:

```{r}
set.seed(663421)

lasso_grid <- data.frame(
  # Here we specify the lambda to several possible values
  lambda = seq(0, 3, length.out = 300),
  # 1 is equal to lasso
  alpha = 1
)

lasso_mod <- train(
  math_score ~ MISCED + FISCED + HISEI + REPEAT + IMMIG + DURECEC + BSMJ,
  data = pisa_train,
  method = "glmnet",
  tuneGrid = lasso_grid,
  preProc = c("center", "scale"),
  trControl = trainControl(method = "cv")
)

plot(lasso_mod$finalModel, xvar = "lambda", label = TRUE)
```

See that at some point the coefficient actually go to zero, meaning that the lasso makes a selection of the variables with the higher coefficients and eliminates those which do not have a strong relationship. Lasso is usually better at model interpretation because it removes redundant variables while ridge can be useful if you want to keep a number of variables in the model, despite them being weak predictors (as controls, for example).

The lasso actually works exactly as the ridge in the `caret` package, meaning that it automatically checks the most optimal value for lambda:

```{r}
best_lambda_lasso <- lasso_mod$bestTune$lambda
best_lambda_lasso
```

To actually check the final model and which variables are kept, we can access it:

```{r}

holdout_lasso <-
  RMSE(
    predict(lasso_mod, pisa_test, s = best_lambda_lasso),
    pisa_test$math_score
  )

train_rmse_lasso <-
  lasso_mod$results %>%
  filter(lambda == best_lambda_lasso) %>%
  pull(RMSE)

c(holdout_rmse = holdout_lasso, train_rmse = train_rmse_lasso)

```

So far, we can check which model is performing better:

```{r}
model_comparison <-
  data.frame(
    type = c("test RMSE", "training RMSE"),
    ridge = c(holdout_ridge, train_rmse_ridge),
    lasso = c(holdout_lasso, train_rmse_lasso)
  )

model_comparison
```

Currently the ridge regression has a very minor advantaged over the lasso yet the difference is probably within the margin of error. Depending on your aim, you might want to choose either if the models. If your aim is to interpret the model well, then lasso might be a better choice because it could contain lower.

## Elastic Net regularization

If you're aware of ridge and lasso, then you can elastic net is a logical step. Elastic Net (the name sounds fancy, as if there's something complex and mathematical to it) combines both penalties to form one equation.

Here we define our ridge penalty:

$$ridge = \lambda \sum_{k = 1}^n |\beta_j|$$

And here we define our lasso penalty:

$$lasso = \lambda \sum_{k = 1}^n \beta_j^2$$

Elastic net regularization is the addition of these two penalties in comparison to the RSS:

$$RSS + lasso + ridge$$

I think the best explanation for elastic net reguarlization comes from Boehmke & Greenwell (2019):

> Although lasso models perform feature selection, when two strongly correlated features are pushed towards zero, one may be pushed fully to zero while the other remains in the model. Furthermore, the process of one being in and one being out is not very systematic. In contrast, the ridge regression penalty is a little more effective in systematically handling correlated features together. Consequently, the advantage of the elastic net penalty is that it enables effective regularization via the ridge penalty with the feature selection characteristics of the lasso penalty.

Essentially, you now have two 'lambdas'. In R, instead of specifying an alpha of `0` (ridge) or `1` (lasso), `caret` will slide through several values ranging from 0 to 1 and compare that to several values of lambda.

However, `train` can already take of it and calculate the most optimal value automatically:

```{r }
set.seed(663421)

elnet_mod <- train(
  math_score ~ MISCED + FISCED + HISEI + REPEAT + IMMIG + DURECEC + BSMJ,
  data = pisa_train,
  method = "glmnet",
  preProc = c("center", "scale"),
  trControl = trainControl(method = "cv"),
  tuneLength = 25
)

best_lambda_elnet <- elnet_mod$bestTune$lambda

holdout_elnet <-
  RMSE(
    predict(elnet_mod, pisa_test),
    pisa_test$math_score
  )

train_rmse_elnet <-
  elnet_mod$results %>%
  filter(alpha == elnet_mod$bestTune$alpha, lambda == best_lambda_elnet) %>%
  pull(RMSE)

c(holdout_rmse = holdout_elnet, train_rmse = train_rmse_elnet)
```

The RMSE of the elastic net is somewhat lower than then ridge and lasso but also probably within the margin of error. Let's compare it visually:

```{r}

model_comparison$elnet <- c(holdout_elnet, train_rmse_elnet)
model_comparison

model_comparison %>%
  pivot_longer(-type) %>%
  ggplot(aes(name, value, color = type, group = type)) +
  geom_point(position = "dodge") +
  geom_line() +
  scale_y_continuous(name = "RMSE") +
  scale_x_discrete(name = "Models") +
  theme_minimal()

```


## Exercises

### 1) How can we improve accuracy?

The current accuracy of the model is around 79 points in mathematics. In the previous analysis we concentrated on a few selected variables. However, the strength of regularized methods is to use all available information and the model takes care of finding the optimal values of regularization and do a subset selection on the best variables.

- Run a model using `train` which excludes 

```{r, echo = FALSE}

elnet_mod <- train(
  math_score ~ .,
  data = pisa_train[!grepl("PV.+MATH", names(pisa_train))],
  method = "glmnet",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", allowParallel = TRUE),
  tuneLength = 10
)

best_lambda_elnet <- elnet_mod$bestTune$lambda

holdout_elnet <-
  RMSE(
    predict(elnet_mod, pisa_test),
    pisa_test$math_score
  )

train_rmse_elnet <-
  elnet_mod$results %>%
  filter(alpha == elnet_mod$bestTune$alpha, lambda == best_lambda_elnet) %>%
  pull(RMSE)

c(holdout_rmse = holdout_elnet, train_rmse = train_rmse_elnet)
```


## Bibliography

Boehmke, B., & Greenwell, B. M. (2019). Hands-On Machine Learning with R. CRC Press.
