---
title: 'An introduction to regularization'
author: Jorge Cimentada
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r knitr-setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      fig.path = "../figs/",
                      fig.align = "center",
                      fig.asp = 0.618,
                      out.width = "80%")
```


Example of a true model with p > n running on OLS and regularization
Example showing the collinearity doesn't not affect prediction
Example showing that regularization can be used where there is a lot of collinearity to improve accuracy

This penalty parameter constrains the size of the coefficients such that the
only way the coefficients can increase is if we experience a comparable decrease
in the sum of squared errors (SSE).

Ridge works well for reducing the effect to zero but keeping all variables in the model

Hyperparameters are values determined by the analyst

## Ridge regularization

Do no let others fool you into thinking that ridge regression is a fancy AI algorithm. Are you familiar with linear regression? If you are, then ridge regression is just a very **simple** adaptation of linear regression. 

The whole aim of linear regression, or Ordinary Least Squares (OLS), is to minimize the sum of the squared residuals. In other words, fit `N` number of regression lines to the data and keep only the one that has the lowest sum of squared residuals. In simple formula jargon, OLS tries to **minimize** this:

\begin{equation}
RSS = \sum_{k = 1}^n(actual_i - predicted_i)^2
\end{equation}

For each fitted regression line, you compare the predicted value ($predicted_i$) versus the actual value ($actual_i$), square it, and the add it all up. Each fitted regression line then has an associated Residual Sum of Squares (RSS) and the linear model choose the line with the lowest RSS.

> Note: Social scientists are familiar with the RSS and call it just by it's name. However, be aware that in Machine Learning jargon, the RSS belongs to a general family called  **loss functions**. Loss functions are metrics that evaluate the **fit** of your model and there are many around (such as AIC or BIC, R2).

Ridge regression takes the previous RSS loss function and adds one term:

\begin{equation}
RSS + \lambda \sum_{k = 1}^n \beta^2_j
\end{equation}

The new term is called a *shrinkage penalty* because it forces each coefficient $\beta_j$ closer to zero by squaring it. The shrinkage part is clearer once you think of this term as forcing each coefficient to be as small as possible but also considering having the smallest Residual Sum of Squares (RSS). In other words, we want the smallest coefficients that doesn't affect the fit of the line (RSS).

An intuitive example is to think of RSS and $\sum_{k = 1}^n \beta^2_j$ as to separate things. RSS estimates how the model fits the data and $\sum_{k = 1}^n \beta^2_j$ limits how much the data is being fit. Finally, the little $\lambda$ between these two terms can be interpreted as a "weight". The higher the lambda, the higher the weight that will be given to the shrinkage term of the equation. If $\lambda$ is 0, then multiplying 0 by $\sum_{k = 1}^n \beta^2_j$ will always return zero forcing our previous equation to simply be reduced to the single term $RSS$.

Why is there a need to "limit" how well the model fits the data? Because we, social scientists and data scientists, can **overfit** the data. The plot below shows a simulation from [Simon Jackson](https://drsimonj.svbtle.com/ridge-regression-with-glmnet) where we can see that when tested on a training set, OLS and Ridge tend to overfit the data. However, when tested on the test data, Ridge regression has lower out of sample error as the $R2$ is higher for models with different observations.

```{r, echo = FALSE}
# This chunk was taken and adapted from https://drsimonj.svbtle.com/ridge-regression-with-glmnet
library(broom)
library(glmnet)
library(tidyverse)

# Compute R^2 from true and predicted values
rsquare <- function(true, predicted) {
  sse <- sum((predicted - true)^2)
  sst <- sum((true - mean(true))^2)
  rsq <- 1 - sse / sst

  # For this post, impose floor...
  if (rsq < 0) rsq <- 0

  return (rsq)
}

# Train ridge and OLS regression models on simulated data set with `n_train`
# observations and a number of features as a proportion to `n_train`,
# `p_features`. Return R squared for both models on:
#   - y values of the training set
#   - y values of a simualted test data set of `n_test` observations
#   - The beta coefficients used to simulate the data
ols_vs_ridge <- function(n_train, p_features, n_test = 200) {
  ## Simulate datasets
  n_features <- floor(n_train * p_features)
  betas <- rnorm(n_features)

  x <- matrix(rnorm(n_train * n_features), nrow = n_train)
  y <- x %*% betas + rnorm(n_train)
  train <- data.frame(y = y, x)

  x <- matrix(rnorm(n_test * n_features), nrow = n_test)
  y <- x %*% betas + rnorm(n_test)
  test <- data.frame(y = y, x)

  ## OLS
  lm_fit <- lm(y ~ ., train)

  # Match to beta coefficients
  lm_betas <- tidy(lm_fit) %>%
    filter(term != "(Intercept)") %>%
    {.$estimate}
  lm_betas_rsq <- rsquare(betas, lm_betas)

  # Fit to training data
  lm_train_rsq <- glance(lm_fit)$r.squared

  # Fit to test data
  lm_test_yhat <- predict(lm_fit, newdata = test)
  lm_test_rsq  <- rsquare(test$y, lm_test_yhat)

  ## Ridge regression
  lambda_vals <- 10^seq(3, -2, by = -.1)  # Lambda values to search
  cv_glm_fit  <- cv.glmnet(as.matrix(train[,-1]), train$y, alpha = 0, lambda = lambda_vals, nfolds = 5)
  opt_lambda  <- cv_glm_fit$lambda.min  # Optimal Lambda
  glm_fit     <- cv_glm_fit$glmnet.fit

  # Match to beta coefficients
  glm_betas <- tidy(glm_fit) %>%
    filter(term != "(Intercept)", lambda == opt_lambda) %>% 
    {.$estimate}
  glm_betas_rsq <- rsquare(betas, glm_betas)

  # Fit to training data
  glm_train_yhat <- predict(glm_fit, s = opt_lambda, newx = as.matrix(train[,-1]))
  glm_train_rsq  <- rsquare(train$y, glm_train_yhat)

  # Fit to test data
  glm_test_yhat <- predict(glm_fit, s = opt_lambda, newx = as.matrix(test[,-1]))
  glm_test_rsq  <- rsquare(test$y, glm_test_yhat)

  data.frame(
    model = c("OLS", "Ridge"),
    betas_rsq  = c(lm_betas_rsq, glm_betas_rsq),
    train_rsq = c(lm_train_rsq, glm_train_rsq),
    test_rsq = c(lm_test_rsq, glm_test_rsq)
  )

}

# Function to run `ols_vs_ridge()` `n_replications` times
repeated_comparisons <- function(n_train, p_features, n_replications = 1) {
  map(seq(n_replications), ~ ols_vs_ridge(n_train, p_features)) %>% 
    map2(seq(.), ~ mutate(.x, replicate = .y)) %>% 
    reduce(rbind)
}

set.seed(23141)
d <- purrr::cross_df(list(
  n_train = seq(20, 200, 20),
  p_features = .95
))

d <-
  d %>% 
  mutate(results = map2(n_train, p_features, repeated_comparisons))

d %>%
  unnest() %>% 
  group_by(model, n_train) %>%
  summarise(
    train_rsq = mean(train_rsq),
    test_rsq = mean(test_rsq)) %>% 
  gather(data, rsq, contains("rsq")) %>%
  ungroup() %>% 
  mutate(data = factor(gsub("_rsq", "", data), levels = c("train", "test"))) %>% 
  ggplot(aes(n_train, rsq, color = model)) +
  geom_line() +
  geom_point(size = 4, alpha = .3) +
  facet_wrap(~ data) +
  theme_minimal() +
  labs(x = "Number of training observations",
       y = "R squared")

```

The strength of the ridge regression comes from the fact that it compromises fitting the training data really well for improved generalization. In other words, we increase **bias** for lower **variance**.

The whole gist behind ridge regression is penalizing very large coefficients for better generalization. Having that intuition in mind, the predictors of the ridge regression need to be standardized. Why is this the case? Because due to the scale of a predictor, its coefficient can be more penalized than other predictors. Suppose that you have the income of a particular person (measured in thousands per months) and time spent with their families (measured in seconds) and you're trying to predict happiness. A one unit increase in salary could be penalized much more than a one unit increase in time spent with their families just because a one unit increase in salary is much bigger due to it's metric.

In R, you can fit a ridge regression (and nearly all other machine learning models) through the `caret` package. Let's load the packages that we will work with and read the data:

```{r}
library(caret) # Fitting machine learning models
library(rsample) # Create data partitions

pisa <- read.csv("./data/pisa_us_2018.csv")
```

We separate the training and test data. All of our model tuning will be done on the training data and the test data is saved for later (the test data must be completely ignored until you have your final tuned model).

```{r}

# Separate training/testing split
set.seed(23141)
split_pisa <- initial_split(data = pisa, prop = .7)
pisa_test <- testing(split_pisa)
pisa_train <- training(split_pisa)
```

The ridge regression has a parameter called alpha which needs to be set by us. alpha is the "weight" term in the ridge equation, which controls how much weight do we want to give to the "shrinkage penalty". If this lambda is 0, it means we attach **no** weight to the penalty term and we will get the same result over OLS. Let's try that:

```{r}
ridge_grid <- data.frame(
  # Here we specify the lambda to be zero
  lambda = 0,
  # Here we specify the type of penalized regression: 0 is ridge regression
  alpha = 0
)

# The train function accepts several arguments
penalized_mod <- train(
  # math_score is the dependen variable and all other are independent variables
  math_score ~ MISCED + FISCED + HISEI + REPEAT + IMMIG + DURECEC + BSMJ,
  # The training data
  data = pisa_train,
  # The R package that runs the ridge regression
  method = "glmnet",
  # Here is where we pass the lambda argument
  tuneGrid = ridge_grid,
  lambda = 0,
  # Here is where the function standardizes the predictors before
  # fitting the models
  preProc = c("center", "scale"),
  trControl = trainControl(method = "none")
)

# Get ridge coefficients
res <- penalized_mod$finalModel
ridge_coef <- predict(res, s = 0, type = "coefficients")

# Get linear model coefficients
iv_vars <- c("MISCED", "FISCED", "HISEI", "REPEAT", "IMMIG", "DURECEC", "BSMJ")
pisa_tst <- pisa_train
pisa_tst[iv_vars] <- scale(pisa_tst[iv_vars])

lm_coef <- coef(
  lm(math_score ~ MISCED + FISCED + HISEI + REPEAT + IMMIG + DURECEC + BSMJ,
     data = pisa_tst)
)

comparison <-
  data.frame(coefs = names(lm_coef),
             `Linear coefficients` = unname(round(lm_coef, 2)),
             `Ridge coefficients` = round(as.vector(ridge_coef), 2))

knitr::kable(comparison)
```

Coming from a social science background, it might seem counterintuitive that the researcher has to specify tuning parameters for the model. In traditional social science statistics, models usually estimate similar values internally and the user doesn't have to think about them. However, there are strategies already implemented to explore the combination of many possible values. With out previous example, we just have to add a number of lambda values and `train` will find the best one:


```{r}
set.seed(663421)

ridge_grid <- data.frame(
  # Here we specify the lambda to several possible values
  lambda = seq(0, 3, length.out = 300),
  alpha = 0
)

penalized_mod <- train(
  math_score ~ MISCED + FISCED + HISEI + REPEAT + IMMIG + DURECEC + BSMJ,
  data = pisa_train,
  method = "glmnet",
  tuneGrid = ridge_grid,
  preProc = c("center", "scale"),
  # Performs cross validation through all grid parameters
  trControl = trainControl(method = "cv")
)

plot(penalized_mod$finalModel, xvar = "lambda", label = TRUE)
```

Here we can see how our coefficients are affected by increasing weight of the lamba parameter. And we can figure out the best lambda inspecting `bestTune` inside `penalized_mod`:

```{r}
best_lambda <- penalized_mod$bestTune$lambda
```

However, there's no need to rerun the model with this optimal value; since `train` **had** to run that model, it saves it as the most optimal:

```{r}

holdout <-
  RMSE(
    predict(penalized_mod, pisa_test, s = best_lambda),
    pisa_test$math_score
  )

train_rmse <-
  penalized_mod$results %>%
  filter(lambda == best_lambda) %>%
  select(RMSE)

c(holdout_rmse = holdout, train_rmse = train_rmse)
```

```{r, eval = FALSE, echo = FALSE}

mid <- function(x) {
  x <- sort(x)
  even <- (length(x) %% 2) == 0

  if (even) x[length(x) / 2] else x[round(length(x) / 2)]
}

library(tidymodels)
library(haven)
library(tidyverse)
library(dials)
library(tune)
library(broom)

pisa <- read_spss("./data/pisa_us_2018.sav")

# Get indidices of 10 math plausible columns
pv_ind <- grepl("PV[0-9]MATH", names(pisa))

# Calculate the average test score in mathematics as the average of all
# 10 plausible columns
pisa$math_score <- rowMeans(pisa[pv_ind])

# Calculate training/testing split
set.seed(23141)
split_pisa <- initial_split(data = pisa, prop = .7)

pisa_test <- testing(split_pisa)
pisa_train <- training(split_pisa)

pisa_recipe <-
  pisa_train %>%
  recipe(math_score ~ MISCED + FISCED + HISEI + REPEAT + IMMIG + DURECEC + BSMJ + STUBMI,
         data = pisa_train) %>%
  step_zv(all_predictors()) %>%
  step_mutate(IMMIG = as.factor(IMMIG)) %>% 
  step_meanimpute(all_predictors(), -all_nominal()) %>%
  step_center(all_predictors(), -all_nominal()) %>% 
  step_dummy(IMMIG, one_hot = TRUE) %>% 
  prep(retain = TRUE)

lm_mod <-
  linear_reg() %>%
  set_engine("lm") %>%
  fit(formula(pisa_recipe), data = juice(pisa_recipe))

ridge_mod <-
  # Ridge reg
  linear_reg(mixture = 0) %>%
  set_engine("glmnet") %>%
  fit(formula(pisa_recipe), data = juice(pisa_recipe))

lasso_mod <-
  # Ridge reg
  linear_reg(mixture = 1) %>%
  set_engine("glmnet") %>%
  fit(formula(pisa_recipe), data = juice(pisa_recipe))

elnet_mod <-
  # Ridge reg
  linear_reg(mixture = 0.5) %>%
  set_engine("glmnet") %>%
  fit(formula(pisa_recipe), data = juice(pisa_recipe))

size_fun <- max

all_models <- bind_rows(

  lm_mod %>%
    tidy() %>%
    mutate(model = "lm"),
  
  ridge_mod %>%
    tidy() %>%
    filter(lambda == size_fun(.$lambda)) %>%
    mutate(model = "ridge"),
  
  lasso_mod %>%
    tidy() %>%
    filter(lambda == size_fun(.$lambda)) %>%
    mutate(model = "lasso"),

  elnet_mod %>%
    tidy() %>%
    filter(lambda == size_fun(.$lambda)) %>%
    mutate(model = "elnet")
  
) %>%
  select(term, estimate, model) %>%
  mutate(model = factor(model, levels = c("lm", "ridge", "lasso", "elnet")))

all_models %>%
  ggplot(aes(term, estimate, fill = model)) +
  geom_col(position = "dodge", alpha = 1/2) +
  theme_minimal()

ridge_mod %>%
  tidy() %>%
  filter(term != "(Intercept)") %>% 
  ggplot(aes(lambda, estimate, group = term, color = term)) +
  geom_line()

lasso_mod %>%
  tidy() %>%
  filter(term != "(Intercept)") %>% 
  ggplot(aes(lambda, estimate, group = term, color = term)) +
  geom_line()

```

OLS has low bias because it optimizes the loss function of SSR if the sample is representative.
However, it can have a huge variance. Specifically, this happens when:

* The predictor variables are highly correlated with each other;
* There are many predictors.

In particular OLS, can have a lot of variance due to overfitting the current dataset and don't have good generalization.

```{r, eval = FALSE, echo = FALSE}

r2 <- function(model) {
  model_summary <- summary(model)
  out <- list(R2 = model_summary$r.squared, R2_adjusted = model_summary$adj.r.squared)
  names(out$R2) <- "R2"
  names(out$R2_adjusted) <- "adjusted R2"

  out <- lapply(out, round, 3)

  out
}

# Our model is always **over-optimistic** about our results
r2(lm_mod$fit)

pisa_test <- bake(pisa_recipe, new_data = pisa_test)
lm_mod_test <-
  linear_reg() %>%
  set_engine("lm") %>%
  fit(formula(pisa_recipe), data = pisa_test)

r2(lm_mod_test$fit)

```

```{r, eval = FALSE, echo = FALSE}

pisa_recipe

```

```{r, eval = FALSE, echo = FALSE}
library(tidyverse)
library(tidymodels)
library(haven)
library(tune)
library(doParallel)

registerDoParallel(detectCores())

pisa <- read_spss("./data/pisa_us_2018.sav")

# Get indidices of 10 math plausible columns
pv_ind <- grepl("PV[0-9]MATH", names(pisa))

# Calculate the average test score in mathematics as the average of all
# 10 plausible columns
pisa$math_score <- rowMeans(pisa[pv_ind])

# Calculate training/testing split
set.seed(23141)
split_pisa <- initial_split(data = pisa, prop = .7)
pisa_test <- testing(split_pisa)
pisa_train <- training(split_pisa)
pisa_cv <- vfold_cv(pisa_train, v = 10)

pisa_recipe <-
  pisa_train %>%
  recipe(math_score ~ MISCED + FISCED + HISEI + REPEAT,
         data = pisa_train) %>%
  # Remove all cols without variance
  step_zv(all_predictors()) %>%
  step_nzv(all_predictors()) %>%
  ## step_mutate(IMMIG = as.factor(IMMIG)) %>% 
  ## step_center(has_role("predictor")) %>% 
  ## step_dummy(IMMIG, one_hot = TRUE) %>% 
  # Center all variables
  # Impute all variables
  step_meanimpute(all_predictors())

mod1 <-
  linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")

grid_vals <- grid_regular(penalty(), mixture(), levels = c(10, 10))

tuned_mod1 <-
  tune_grid(
    pisa_recipe,
    model = mod1,
    resamples = pisa_cv,
    grid = grid_vals,
    metrics = metric_set(rmse),
    control = control_grid(verbose = FALSE,
                           extract = function (x) extract_model(x)
                           )
  )

tuned_mod1 %>%
  collect_metrics() %>%
  ggplot(aes(mean)) +
  geom_histogram(bins = 100)

select_by_one_std_err(tuned_mod1,
                      metric = "rmse",
                      penalty,
                      maximize = FALSE)

```
