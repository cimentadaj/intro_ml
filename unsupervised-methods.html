<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Unsupervised methods | Machine Learning for Social Scientists</title>
  <meta name="description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Unsupervised methods | Machine Learning for Social Scientists" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://cimentadaj.github.io/ml_socsci/" />
  
  <meta property="og:description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  <meta name="github-repo" content="cimentadaj/ml_socsci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Unsupervised methods | Machine Learning for Social Scientists" />
  
  <meta name="twitter:description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  

<meta name="author" content="Jorge Cimentada" />


<meta name="date" content="2020-06-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="loss-functions.html"/>
<link rel="next" href="no-free-lunch.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.13/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning for Social Scientists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html"><i class="fa fa-check"></i><b>1</b> Machine Learning for Social Scientists</a><ul>
<li class="chapter" data-level="1.1" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#a-different-way-of-thinking"><i class="fa fa-check"></i><b>1.1</b> A different way of thinking</a></li>
<li class="chapter" data-level="1.2" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#split-your-data-into-trainingtesting"><i class="fa fa-check"></i><b>1.2</b> Split your data into training/testing</a></li>
<li class="chapter" data-level="1.3" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#cross-validation"><i class="fa fa-check"></i><b>1.3</b> Cross-validation</a></li>
<li class="chapter" data-level="1.4" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>1.4</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="1.5" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#an-example"><i class="fa fa-check"></i><b>1.5</b> An example</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>2</b> Regularization</a><ul>
<li class="chapter" data-level="2.1" data-path="regularization.html"><a href="regularization.html#ridge-regularization"><i class="fa fa-check"></i><b>2.1</b> Ridge regularization</a></li>
<li class="chapter" data-level="2.2" data-path="regularization.html"><a href="regularization.html#lasso-regularization"><i class="fa fa-check"></i><b>2.2</b> Lasso regularization</a></li>
<li class="chapter" data-level="2.3" data-path="regularization.html"><a href="regularization.html#elastic-net-regularization"><i class="fa fa-check"></i><b>2.3</b> Elastic Net regularization</a></li>
<li class="chapter" data-level="2.4" data-path="regularization.html"><a href="regularization.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>3</b> Tree-based methods</a><ul>
<li class="chapter" data-level="3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.1</b> Decision trees</a><ul>
<li class="chapter" data-level="3.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#advancedsplit"><i class="fa fa-check"></i><b>3.1.1</b> Advanced: how do trees choose where to split?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging"><i class="fa fa-check"></i><b>3.2</b> Bagging</a></li>
<li class="chapter" data-level="3.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests"><i class="fa fa-check"></i><b>3.3</b> Random Forests</a></li>
<li class="chapter" data-level="3.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting"><i class="fa fa-check"></i><b>3.4</b> Boosting</a></li>
<li class="chapter" data-level="3.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercises-1"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="loss-functions.html"><a href="loss-functions.html"><i class="fa fa-check"></i><b>4</b> Loss functions</a><ul>
<li class="chapter" data-level="4.1" data-path="loss-functions.html"><a href="loss-functions.html#continuous-loss-functions"><i class="fa fa-check"></i><b>4.1</b> Continuous loss functions</a><ul>
<li class="chapter" data-level="4.1.1" data-path="loss-functions.html"><a href="loss-functions.html#root-mean-square-error-rmse"><i class="fa fa-check"></i><b>4.1.1</b> Root Mean Square Error (RMSE)</a></li>
<li class="chapter" data-level="4.1.2" data-path="loss-functions.html"><a href="loss-functions.html#mean-absolute-error"><i class="fa fa-check"></i><b>4.1.2</b> Mean Absolute Error</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="loss-functions.html"><a href="loss-functions.html#binary-loss-functions"><i class="fa fa-check"></i><b>4.2</b> Binary loss functions</a><ul>
<li class="chapter" data-level="4.2.1" data-path="loss-functions.html"><a href="loss-functions.html#confusion-matrices"><i class="fa fa-check"></i><b>4.2.1</b> Confusion Matrices</a></li>
<li class="chapter" data-level="4.2.2" data-path="loss-functions.html"><a href="loss-functions.html#roc-curves-and-area-under-the-curve"><i class="fa fa-check"></i><b>4.2.2</b> ROC Curves and Area Under the Curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html"><i class="fa fa-check"></i><b>5</b> Unsupervised methods</a><ul>
<li class="chapter" data-level="5.1" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>5.1</b> Principal Component Analysis (PCA)</a></li>
<li class="chapter" data-level="5.2" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html#k-means-clustering"><i class="fa fa-check"></i><b>5.2</b> K-Means Clustering</a></li>
<li class="chapter" data-level="5.3" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html#hierarchical-clustering"><i class="fa fa-check"></i><b>5.3</b> Hierarchical Clustering</a></li>
<li class="chapter" data-level="5.4" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html#exercises-2"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="no-free-lunch.html"><a href="no-free-lunch.html"><i class="fa fa-check"></i><b>6</b> No free lunch</a><ul>
<li class="chapter" data-level="6.1" data-path="no-free-lunch.html"><a href="no-free-lunch.html#causal-inference"><i class="fa fa-check"></i><b>6.1</b> Causal Inference</a></li>
<li class="chapter" data-level="6.2" data-path="no-free-lunch.html"><a href="no-free-lunch.html#explanaining-complex-models"><i class="fa fa-check"></i><b>6.2</b> Explanaining complex models</a></li>
<li class="chapter" data-level="6.3" data-path="no-free-lunch.html"><a href="no-free-lunch.html#inference"><i class="fa fa-check"></i><b>6.3</b> Inference</a></li>
<li class="chapter" data-level="6.4" data-path="no-free-lunch.html"><a href="no-free-lunch.html#prediction"><i class="fa fa-check"></i><b>6.4</b> Prediction</a></li>
<li class="chapter" data-level="6.5" data-path="no-free-lunch.html"><a href="no-free-lunch.html#prediction-challenge"><i class="fa fa-check"></i><b>6.5</b> Prediction challenge</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="syllabus.html"><a href="syllabus.html"><i class="fa fa-check"></i><b>7</b> Syllabus</a><ul>
<li class="chapter" data-level="7.1" data-path="syllabus.html"><a href="syllabus.html#course-description"><i class="fa fa-check"></i><b>7.1</b> Course description</a></li>
<li class="chapter" data-level="7.2" data-path="syllabus.html"><a href="syllabus.html#schedule"><i class="fa fa-check"></i><b>7.2</b> Schedule</a></li>
<li class="chapter" data-level="7.3" data-path="syllabus.html"><a href="syllabus.html#software"><i class="fa fa-check"></i><b>7.3</b> Software</a></li>
<li class="chapter" data-level="7.4" data-path="syllabus.html"><a href="syllabus.html#prerequisites"><i class="fa fa-check"></i><b>7.4</b> Prerequisites</a></li>
<li class="chapter" data-level="7.5" data-path="syllabus.html"><a href="syllabus.html#about-the-author"><i class="fa fa-check"></i><b>7.5</b> About the author</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Social Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="unsupervised-methods" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Unsupervised methods</h1>
<p>Unsupervised learning is a very popular concept in machine learning. Although we social scientists are aware of some of these methods, we do not take advantage of them as much as machine learning practitioners. What is unsupervised learning? Let’s get that out of the way: this just means that a particular statistical method <strong>does not have a dependent variable</strong>. These are models that look to find relationships between independent variables without the need of a dependent variable.</p>
<p>One common unsupervised method that social scientists are aware of is the <strong>P</strong>rincipal <strong>C</strong>omponent <strong>A</strong>nalysis or <span class="math inline">\(PCA\)</span>. <span class="math inline">\(PCA\)</span> aims to summarize many variables into a small subset of variables that can capture the greatest variance out of all the main variables. We really never thought about this as an ‘unsupervised’ method, but it is used widely for predictive tasks. Before we begin, let’s load the packages and data we’ll be using.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="unsupervised-methods.html#cb131-1"></a><span class="kw">library</span>(tidymodels)</span>
<span id="cb131-2"><a href="unsupervised-methods.html#cb131-2"></a><span class="kw">library</span>(tidyflow)</span>
<span id="cb131-3"><a href="unsupervised-methods.html#cb131-3"></a><span class="kw">library</span>(ggfortify)</span>
<span id="cb131-4"><a href="unsupervised-methods.html#cb131-4"></a></span>
<span id="cb131-5"><a href="unsupervised-methods.html#cb131-5"></a>data_link &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot;</span></span>
<span id="cb131-6"><a href="unsupervised-methods.html#cb131-6"></a>pisa &lt;-<span class="st"> </span><span class="kw">read.csv</span>(data_link)</span></code></pre></div>
<p>Most of the material on this chapter was built upon <span class="citation">Boehmke and Greenwell (<a href="#ref-boehmke2019" role="doc-biblioref">2019</a>)</span> and <span class="citation">James et al. (<a href="#ref-james2013" role="doc-biblioref">2013</a>)</span>. In particular, the section on K-Means clustering contains images directly copied from <span class="citation">James et al. (<a href="#ref-james2013" role="doc-biblioref">2013</a>)</span>.</p>
<div id="principal-component-analysis-pca" class="section level2">
<h2><span class="header-section-number">5.1</span> Principal Component Analysis (PCA)</h2>
<p><strong>P</strong>rincipal <strong>C</strong>omponent <strong>A</strong>nalysis or <span class="math inline">\(PCA\)</span> is a method that tries to summarize many columns into a very small subset that captures the greatest variability of the original columns. Social Scientists often use this method to create more ‘parsimonious’ models and summarize many variables into a few ‘strong’ variables.</p>
<p><span class="math inline">\(PCA\)</span> works by creating several components which are the normalized linear combination of the variables in the model. In the <code>pisa</code> data there are a six variables which asks the student whether they’ve suffered negative behavior from their friends in the past 12 months. In particular, it asks whether</p>
<ul>
<li>Other students left them out of things on purpose</li>
<li>Other students made fun of them</li>
<li>They were threatened by other students</li>
<li>Other students took away or destroyed things that belonged to them</li>
<li>They got hit or pushed around by other students</li>
<li>Other students spread nasty rumours about them</li>
</ul>
<p>For each of these variables, the scale ranges from 1 to 4 where 4 is ‘Once a week or more’ and 1 is ‘Never or almost never’. In other words, the higher the number, the more negative their response.</p>
<p>Let’s rename these variables into more interpretable names and look at their correlation:</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="unsupervised-methods.html#cb132-1"></a>pisa &lt;-</span>
<span id="cb132-2"><a href="unsupervised-methods.html#cb132-2"></a><span class="st">  </span>pisa <span class="op">%&gt;%</span></span>
<span id="cb132-3"><a href="unsupervised-methods.html#cb132-3"></a><span class="st">  </span><span class="kw">rename</span>(</span>
<span id="cb132-4"><a href="unsupervised-methods.html#cb132-4"></a>    <span class="dt">past12_left_out =</span> ST038Q03NA,</span>
<span id="cb132-5"><a href="unsupervised-methods.html#cb132-5"></a>    <span class="dt">past12_madefun_of_me =</span> ST038Q04NA,</span>
<span id="cb132-6"><a href="unsupervised-methods.html#cb132-6"></a>    <span class="dt">past12_threatened =</span> ST038Q05NA,</span>
<span id="cb132-7"><a href="unsupervised-methods.html#cb132-7"></a>    <span class="dt">past12_destroyed_personal =</span> ST038Q06NA,</span>
<span id="cb132-8"><a href="unsupervised-methods.html#cb132-8"></a>    <span class="dt">past12_got_hit =</span> ST038Q07NA,</span>
<span id="cb132-9"><a href="unsupervised-methods.html#cb132-9"></a>    <span class="dt">past12_spread_rumours =</span> ST038Q08NA</span>
<span id="cb132-10"><a href="unsupervised-methods.html#cb132-10"></a>  )</span>
<span id="cb132-11"><a href="unsupervised-methods.html#cb132-11"></a></span>
<span id="cb132-12"><a href="unsupervised-methods.html#cb132-12"></a>pisa_selected &lt;-</span>
<span id="cb132-13"><a href="unsupervised-methods.html#cb132-13"></a><span class="st">  </span>pisa <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb132-14"><a href="unsupervised-methods.html#cb132-14"></a><span class="st">  </span><span class="kw">select</span>(<span class="kw">starts_with</span>(<span class="st">&quot;past12&quot;</span>))</span>
<span id="cb132-15"><a href="unsupervised-methods.html#cb132-15"></a></span>
<span id="cb132-16"><a href="unsupervised-methods.html#cb132-16"></a><span class="kw">cor</span>(pisa_selected)</span></code></pre></div>
<pre><code>##                           past12_left_out past12_madefun_of_me
## past12_left_out                 1.0000000            0.6073982
## past12_madefun_of_me            0.6073982            1.0000000
## past12_threatened               0.4454125            0.4712083
## past12_destroyed_personal       0.4037351            0.4165931
## past12_got_hit                  0.3918129            0.4480862
## past12_spread_rumours           0.4746302            0.5069299
##                           past12_threatened past12_destroyed_personal
## past12_left_out                   0.4454125                 0.4037351
## past12_madefun_of_me              0.4712083                 0.4165931
## past12_threatened                 1.0000000                 0.5685773
## past12_destroyed_personal         0.5685773                 1.0000000
## past12_got_hit                    0.5807617                 0.6206485
## past12_spread_rumours             0.5513099                 0.4543380
##                           past12_got_hit past12_spread_rumours
## past12_left_out                0.3918129             0.4746302
## past12_madefun_of_me           0.4480862             0.5069299
## past12_threatened              0.5807617             0.5513099
## past12_destroyed_personal      0.6206485             0.4543380
## past12_got_hit                 1.0000000             0.4451408
## past12_spread_rumours          0.4451408             1.0000000</code></pre>
<p>Most correlations lie between <code>0.4</code> and <code>0.6</code>, a somewhat acceptable threshold for assesing whether they can be reduced into fewer variables. <span class="math inline">\(PCA\)</span> works by receiving as input <span class="math inline">\(P\)</span> variables (in this case six) and calculating the normalized linear combination of the <span class="math inline">\(P\)</span> variables. This new variable is the linear combination of the six variables that captures the greatest variance out of all of them. <span class="math inline">\(PCA\)</span> continues to calculate other normalized linear combinations <strong>but</strong> with the constraint that they need to be completely uncorrelated to all the other normalized linear combinations.</p>
<p>This approach has the advantage that it constructs as many principal components (new variables) as it can, as long as they all capture 100% of the variability of the original <span class="math inline">\(P\)</span> variables, and each of these new variables are completely uncorrelated between each other.</p>
<p>Each variable is assessed by how much variance it explains of the original <span class="math inline">\(P\)</span> variables and each new variable is completely independent of the others. Depending on the correlation of the <span class="math inline">\(P\)</span> input variables, you might get three principal components that capture all of the variability of the original <span class="math inline">\(P\)</span> variables. In other cases, you can get an you might many more.</p>
<p>This discussion is getting too theoretical. Let’s get get some hands-on experience of how this works. Let’s pass in our six variables to the function <code>prcomp</code>, which estimates these principal components based on our six variables. However, for <span class="math inline">\(PCA\)</span> to work well, we need to center and scale the independent variables. This means that the independent variables will have a mean of zero and a standard deviation of one. <code>prcomp</code> does this for you, but you should be aware of this for future discussion:</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="unsupervised-methods.html#cb134-1"></a>pc &lt;-<span class="st"> </span><span class="kw">prcomp</span>(pisa_selected)</span>
<span id="cb134-2"><a href="unsupervised-methods.html#cb134-2"></a>all_pcs &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(pc<span class="op">$</span>x)</span>
<span id="cb134-3"><a href="unsupervised-methods.html#cb134-3"></a><span class="kw">head</span>(all_pcs)</span></code></pre></div>
<pre><code>##            PC1        PC2         PC3          PC4          PC5          PC6
## 1 -2.836172297 -0.7549602 -1.91065434 -0.232647114 -0.368981283 -1.885607656
## 2 -1.478020766  0.6622561  0.94113153  0.181451711  0.149387648  0.678384471
## 3  1.025953306  0.1602906 -0.03806864 -0.008994148  0.009439987 -0.002391996
## 4 -0.002173173 -0.7902197 -0.10112894 -0.197389118  0.013521080 -0.002718289
## 5 -4.832075955  0.1996595  0.39221922 -0.256660522 -1.178883084  0.150399629
## 6 -1.132036976 -1.8534154 -0.68913950  0.914561923  0.065907346  0.087208533</code></pre>
<p>Let’s explain what just happened. Our dataset <code>pisa_selected</code> contains the six variables of interest. We passed that to <code>prcomp</code> which calculated the principal components. With this model object, we extracted the dataframe with the new principal components. The result of all of this is a dataframe with six new columns. These six new columns are <strong>not</strong> the initial six variables from <code>pisa_selected</code>. Instead, they are variables that summarize the relationship of these six variables.</p>
<p>You might ask yourself, how come six variables <strong>summarize</strong> six variables? That doesn’t make much sense. The whole idea is that fewer variables can summarize the original six. Let’s look at how much variance of the original <span class="math inline">\(P\)</span> variables these ‘index’ variables explain:</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="unsupervised-methods.html#cb136-1"></a><span class="kw">tidy</span>(pc, <span class="st">&quot;pcs&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 4
##      PC std.dev percent cumulative
##   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;
## 1     1   1.34   0.591       0.591
## 2     2   0.640  0.135       0.726
## 3     3   0.530  0.0929      0.819
## 4     4   0.522  0.0899      0.909
## 5     5   0.394  0.0513      0.960
## 6     6   0.347  0.0397      1</code></pre>
<p>This output shows how well each principal component is explaining the original six variables. For example, the first principal component (1st row) explains about 59% of the variance of the six variables. The second principal component explains an additional 13.5%, for a total of 72.6% between the two. This is certainly better. It means that the first two variables seem to have some power in summarizing the original six variables.</p>
<p>Let’s focus on the first two principal components. They are supposed to be completely uncorrelated, so let’s check that ourselves:</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="unsupervised-methods.html#cb138-1"></a><span class="kw">cor</span>(all_pcs[<span class="kw">c</span>(<span class="st">&quot;PC1&quot;</span>, <span class="st">&quot;PC2&quot;</span>)])</span></code></pre></div>
<pre><code>##                         PC1                     PC2
## PC1  1.00000000000000000000 -0.00000000000001545012
## PC2 -0.00000000000001545012  1.00000000000000000000</code></pre>
<p>As expected, the correlation between these two variables is 0.</p>
<p>How do we use these two variables? Well, a typical social scientist would make sure that their expected explanatory power of the two components is high enough for their research problem. If it is, they would include these two columns in their statistical models instead of the six variables.
However, <span class="math inline">\(PCA\)</span> is all about exploratory data analysis. We might want to go further and explore how the original six variables are related to these principal components. These two principal components are a bit of a black box at this point. Which variables do they represent? We can check that with the initial output of <code>prcomp</code>:</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="unsupervised-methods.html#cb140-1"></a>pc<span class="op">$</span>rotation[, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]</span></code></pre></div>
<pre><code>##                                  PC1        PC2
## past12_left_out           -0.4631946 -0.4189125
## past12_madefun_of_me      -0.5649319 -0.5315979
## past12_threatened         -0.3446963  0.4025682
## past12_destroyed_personal -0.2694606  0.3405411
## past12_got_hit            -0.2987481  0.3715999
## past12_spread_rumours     -0.4308453  0.3546832</code></pre>
<p>These two columns show the correlations between the six original variables and the first two principal components. Let’s focus on the first column. The first thing that stands out is that for all the six variables, the correlation is negative. This means that as the respondents answered negatively to the six questions, the first principal component decreases. Informally, we could call this variable a ‘negative-peer index’.</p>
<p>Moving to the second column, four of these six variables correlate positively with the second principal component. At least for these four variables, the principal components tend capture the exact opposite relationship. In other words, at least for these four variables, this is a ‘positive-peer index’. This type of decomposition is precisely where the usefulness of this type of method comes in. It allows us to summarize many variables into a small set of components that capture meaningful variation.</p>
<p>The package <code>ggfortifty</code> contains the function <code>autoplot</code> which can help us visualize these correlations in a more meaningful way:</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="unsupervised-methods.html#cb142-1"></a><span class="kw">set.seed</span>(<span class="dv">6652</span>)</span>
<span id="cb142-2"><a href="unsupervised-methods.html#cb142-2"></a>pc <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb142-3"><a href="unsupervised-methods.html#cb142-3"></a><span class="st">  </span><span class="kw">autoplot</span>(<span class="dt">loadings =</span> <span class="ot">TRUE</span>,</span>
<span id="cb142-4"><a href="unsupervised-methods.html#cb142-4"></a>           <span class="dt">loadings.label =</span> <span class="ot">TRUE</span>,</span>
<span id="cb142-5"><a href="unsupervised-methods.html#cb142-5"></a>           <span class="dt">loadings.label.repel =</span> <span class="ot">TRUE</span>,</span>
<span id="cb142-6"><a href="unsupervised-methods.html#cb142-6"></a>           <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">6</span>) <span class="op">+</span></span>
<span id="cb142-7"><a href="unsupervised-methods.html#cb142-7"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Let’s distill this plot. On the <code>X</code> axis we have the actual column of the first principal component (PC1) (this is literaly the same column we saw in the object <code>all_pcs</code>; if it serves to refresh your memory, check it out with <code>head(all_pcs)</code>). As you can see, the label of the <code>X</code> axis already tells us that this component explains nearly 60% of the variance of these six variables. On the <code>Y</code> axis we have the actual column of the second principal component (PC2) (same as before, you can see this with <code>head(all_pcs)</code>). This principal component explains an additional 13.5% of the variance of the six variables.</p>
<p>What this plot is trying to show is where these six variables are clustered between these two principal components. Since these two variables were centered and scaled to have a mean of zero, the red lines always begin at the intersection of the zero in PC1 and PC2. In other words, we can see more clearly the correlations we saw earlier. For example, remember that the first two variables were both negatively correlated with both PC1 and PC2. These two variables are located in the bottom left of the plot, showing that for both principal components both variables are associated with lower values of PC1 and PC2:</p>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>There is nothing new here. This is the same thing we interpreted from the correlation but from a more intuitive visualization. If you remember the other four variables from the correlation, they showed negative correlations with PC1 and positive correlations with PC2. This means that these variables should cluster <strong>below</strong> the average of PC1 and <strong>higher</strong> than the average of PC2. We can see that more clearly if we first add a line showing the zero values for both variables:</p>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Any values to the <strong>left</strong> of the the vertical line are low values of PC1 while all values <strong>above</strong> the horizontal line are high values for PC2. Building on this intuition, we should find that the remaining four variables cluster at lower values of PC1 and at higher values of PC1:</p>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Depending on these correlations, you might reject to focus on the first two principal components and explore this same plot for PC1 and PC3 or PC2 and PC4. There’s no clear cut rule for the number of principal components to use. The user should instead explore these plots to understand whether there are interesting findings from clustering many variables into fewer variables. Depending on this, you might reject the idea entirely of using principal components. Or you might use these principal components to represent some interesting findings for your theoretical model.</p>
<p>In any case, this method is inherently exploratory. It serves as way to understand whether we can reduce correlated variables into a small subset of variables that represent them. For a social science point of view, this method is often used for reducing the number of variables. However, there is still room for using it as a clustering method to understand whether certain variables can help us summarize our understanding into simpler concepts.</p>
<p>Having said this, for predictive tasks there is an objective measure on how many principal components to use: the ones that improve predictions the most. Using our previous example, we could perform a grid search on a number of components to see which one maximizes predictive accuracy. Let’s run a random forest by regressing the variable <code>math_score</code> on all variables in the dataset. While we do that, let’s try models with different number of principal components:</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="unsupervised-methods.html#cb143-1"></a><span class="co"># Set the number of components `num_comp`</span></span>
<span id="cb143-2"><a href="unsupervised-methods.html#cb143-2"></a><span class="co"># to be tuned</span></span>
<span id="cb143-3"><a href="unsupervised-methods.html#cb143-3"></a>rcp &lt;-</span>
<span id="cb143-4"><a href="unsupervised-methods.html#cb143-4"></a><span class="st">  </span><span class="er">~</span><span class="st"> </span><span class="kw">recipe</span>(.x, math_score <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span></span>
<span id="cb143-5"><a href="unsupervised-methods.html#cb143-5"></a><span class="st">    </span><span class="kw">step_pca</span>(<span class="kw">starts_with</span>(<span class="st">&quot;past12_&quot;</span>), <span class="dt">num_comp =</span> <span class="kw">tune</span>())</span>
<span id="cb143-6"><a href="unsupervised-methods.html#cb143-6"></a></span>
<span id="cb143-7"><a href="unsupervised-methods.html#cb143-7"></a>tflow &lt;-</span>
<span id="cb143-8"><a href="unsupervised-methods.html#cb143-8"></a><span class="st">  </span>pisa <span class="op">%&gt;%</span></span>
<span id="cb143-9"><a href="unsupervised-methods.html#cb143-9"></a><span class="st">  </span><span class="kw">tidyflow</span>(<span class="dt">seed =</span> <span class="dv">25131</span>) <span class="op">%&gt;%</span></span>
<span id="cb143-10"><a href="unsupervised-methods.html#cb143-10"></a><span class="st">  </span><span class="kw">plug_split</span>(initial_split) <span class="op">%&gt;%</span></span>
<span id="cb143-11"><a href="unsupervised-methods.html#cb143-11"></a><span class="st">  </span><span class="kw">plug_recipe</span>(rcp) <span class="op">%&gt;%</span></span>
<span id="cb143-12"><a href="unsupervised-methods.html#cb143-12"></a><span class="st">  </span><span class="kw">plug_model</span>(<span class="kw">set_engine</span>(<span class="kw">rand_forest</span>(<span class="dt">mode =</span> <span class="st">&quot;regression&quot;</span>), <span class="st">&quot;ranger&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb143-13"><a href="unsupervised-methods.html#cb143-13"></a><span class="st">  </span><span class="kw">plug_resample</span>(vfold_cv) <span class="op">%&gt;%</span></span>
<span id="cb143-14"><a href="unsupervised-methods.html#cb143-14"></a><span class="st">  </span><span class="co"># Set `num_comp`in the grid to 1:3</span></span>
<span id="cb143-15"><a href="unsupervised-methods.html#cb143-15"></a><span class="st">  </span><span class="co"># meaning that we&#39;ll try the models with</span></span>
<span id="cb143-16"><a href="unsupervised-methods.html#cb143-16"></a><span class="st">  </span><span class="co"># number of components 1, 2 and 3</span></span>
<span id="cb143-17"><a href="unsupervised-methods.html#cb143-17"></a><span class="st">  </span><span class="kw">plug_grid</span>(expand.grid, <span class="dt">num_comp =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>)</span>
<span id="cb143-18"><a href="unsupervised-methods.html#cb143-18"></a></span>
<span id="cb143-19"><a href="unsupervised-methods.html#cb143-19"></a>res_rf &lt;-<span class="st"> </span><span class="kw">fit</span>(tflow)</span>
<span id="cb143-20"><a href="unsupervised-methods.html#cb143-20"></a></span>
<span id="cb143-21"><a href="unsupervised-methods.html#cb143-21"></a>res_rf <span class="op">%&gt;%</span></span>
<span id="cb143-22"><a href="unsupervised-methods.html#cb143-22"></a><span class="st">  </span><span class="kw">pull_tflow_fit_tuning</span>() <span class="op">%&gt;%</span></span>
<span id="cb143-23"><a href="unsupervised-methods.html#cb143-23"></a><span class="st">  </span><span class="kw">collect_metrics</span>() <span class="op">%&gt;%</span></span>
<span id="cb143-24"><a href="unsupervised-methods.html#cb143-24"></a><span class="st">  </span><span class="kw">filter</span>(.metric <span class="op">==</span><span class="st"> &quot;rmse&quot;</span>)</span></code></pre></div>
<p>These are the average results of running a 10-fold cross-validation trying out models with one, two and three principal components respectively. As we can see from the <code>mean</code> column, there is little difference between the average <span class="math inline">\(RMSE\)</span> of these different models. If there are important reasons to include these variables in the model and we want to reduce the number of variables in the model for simplicity, we could just keep the model with one principal component.</p>
<p>However, there’s also an alternative approach. <code>step_pca</code> allows you to specify the minimum explanatory power of the principal components. As discussed in the documentation of <code>step_pca</code>, <em>you specify the fraction of the total variance that should be covered by the components. For example, <code>threshold = .75</code> means that <code>step_pca</code> should generate enough components to capture 75% of the variance.</em></p>
<p>We can try our previous models with a 90% threshold. Since we will not perform a grid search, we will drop the grid and only keep the cross-validation to get uncertain estimates of our loss function <span class="math inline">\(RMSE\)</span>:</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="unsupervised-methods.html#cb144-1"></a><span class="co"># Define a new recipe</span></span>
<span id="cb144-2"><a href="unsupervised-methods.html#cb144-2"></a><span class="co"># where threshold is .90</span></span>
<span id="cb144-3"><a href="unsupervised-methods.html#cb144-3"></a>rcp &lt;-</span>
<span id="cb144-4"><a href="unsupervised-methods.html#cb144-4"></a><span class="st">  </span><span class="er">~</span><span class="st"> </span><span class="kw">recipe</span>(.x, math_score <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span></span>
<span id="cb144-5"><a href="unsupervised-methods.html#cb144-5"></a><span class="st">    </span><span class="kw">step_pca</span>(<span class="kw">starts_with</span>(<span class="st">&quot;past12_&quot;</span>), <span class="dt">threshold =</span> <span class="fl">.90</span>)</span>
<span id="cb144-6"><a href="unsupervised-methods.html#cb144-6"></a></span>
<span id="cb144-7"><a href="unsupervised-methods.html#cb144-7"></a><span class="co"># Replace the previous recipe</span></span>
<span id="cb144-8"><a href="unsupervised-methods.html#cb144-8"></a><span class="co"># and drop the grid</span></span>
<span id="cb144-9"><a href="unsupervised-methods.html#cb144-9"></a>tflow &lt;-</span>
<span id="cb144-10"><a href="unsupervised-methods.html#cb144-10"></a><span class="st">  </span>tflow <span class="op">%&gt;%</span></span>
<span id="cb144-11"><a href="unsupervised-methods.html#cb144-11"></a><span class="st">  </span><span class="kw">replace_recipe</span>(rcp) <span class="op">%&gt;%</span></span>
<span id="cb144-12"><a href="unsupervised-methods.html#cb144-12"></a><span class="st">  </span><span class="kw">drop_grid</span>()</span>
<span id="cb144-13"><a href="unsupervised-methods.html#cb144-13"></a></span>
<span id="cb144-14"><a href="unsupervised-methods.html#cb144-14"></a>res_rf &lt;-<span class="st"> </span><span class="kw">fit</span>(tflow)</span>
<span id="cb144-15"><a href="unsupervised-methods.html#cb144-15"></a></span>
<span id="cb144-16"><a href="unsupervised-methods.html#cb144-16"></a>res_cv &lt;-</span>
<span id="cb144-17"><a href="unsupervised-methods.html#cb144-17"></a><span class="st">  </span>res_rf <span class="op">%&gt;%</span></span>
<span id="cb144-18"><a href="unsupervised-methods.html#cb144-18"></a><span class="st">  </span><span class="kw">pull_tflow_fit_tuning</span>() <span class="op">%&gt;%</span></span>
<span id="cb144-19"><a href="unsupervised-methods.html#cb144-19"></a><span class="st">  </span><span class="kw">collect_metrics</span>()</span>
<span id="cb144-20"><a href="unsupervised-methods.html#cb144-20"></a></span>
<span id="cb144-21"><a href="unsupervised-methods.html#cb144-21"></a><span class="kw">round</span>(res_cv<span class="op">$</span>mean[<span class="dv">1</span>], <span class="dv">2</span>)</span>
<span id="cb144-22"><a href="unsupervised-methods.html#cb144-22"></a></span>
<span id="cb144-23"><a href="unsupervised-methods.html#cb144-23"></a>res_cv</span></code></pre></div>
<p>This approach offers a very similar <span class="math inline">\(RMSE\)</span> of MISSING CALCULATION. Althought not possible at this moment, <code>tidymodels</code> is expected to allow the <code>threshold</code> parameter to be <code>tune</code> such that you can perform a grid search of this value as well (for those interested, see <a href="https://github.com/tidymodels/recipes/issues/534">here</a>).</p>
<p>Although <span class="math inline">\(PCA\)</span> is a very useful method for summarizing information, it is based on the notion that the variables to be summarized are best summarized through a linear combination. In other instances, non-linear methods can also prove useful as exploratory means.</p>
</div>
<div id="k-means-clustering" class="section level2">
<h2><span class="header-section-number">5.2</span> K-Means Clustering</h2>
<p>K-Means is a method for finding clusters in a dataset of <span class="math inline">\(P\)</span> variables. It is somewhat different from <span class="math inline">\(PCA\)</span> because it attemps to find non-overlapping clusters of respondents using <span class="math inline">\(P\)</span> variables. In terms of interpretation and transparency, K-Means clustering is particularly useful for exploration in the social sciences.</p>
<p>Suppose we have a scatterplot of two variables:</p>
<p><img src="img/km1.png" width="30%" style="display: block; margin: auto;" /></p>
<p>The distribution of this scatterplot shows that there are at least two visible clusters, one in the top part of the plot and one in the bottom part of plot. How does K-Means identify clusters appropriately? First, it begins by <strong>randomly</strong> assigning each point a cluster. Let’s suppose we want to identify three clusters:</p>
<p><img src="img/km2.png" width="30%" style="display: block; margin: auto;" /></p>
<p>Each point has now an associated color. However, these colors were randomly assigned. There’s no evident pattern from the colors and the position of each point. K-Means clustering works by creating something called ‘centroids’ which represent the center of the different clusters.</p>
<p>The centroid is usually the <strong>mean of the <span class="math inline">\(P\)</span> variables</strong>. For our simplified case, we calculate the average of X and Y for the color orange, then the average of X and Y for the color purple and then the average of X and Y for the color green. The end result of this is an average mean of X and Y for the three colors. For example, for the orange points, we can plot a ‘average big point’ that is located at the average of X and the average of Y <strong>only</strong> for the orange points. We repeat this for every color and plot it:</p>
<p><img src="img/km3.png" width="30%" style="display: block; margin: auto;" /></p>
<p>Remember that we assigned the cluster colors to the points randomly? Then the mean of X and Y for the three different colors should be more or less the same. We find that’s the case, as the three ‘average big points’ are located at the center of the plot overlapping among each other. So far, everything that has happened is random. There’s no capturing of real knowledge here. The next step is where the substantive part comes in. We need to calculate something called the Euclidian distance between each point and the three centroids. This is not so difficult. Let’s work it out manually.</p>
<p>Suppose that the three centroids are located with these X and Y values:</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="unsupervised-methods.html#cb145-1"></a>centroids_df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">type =</span> <span class="kw">factor</span>(<span class="kw">c</span>(<span class="st">&quot;orange&quot;</span>, <span class="st">&quot;purple&quot;</span>, <span class="st">&quot;green&quot;</span>), <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;orange&quot;</span>, <span class="st">&quot;purple&quot;</span>, <span class="st">&quot;green&quot;</span>)),</span>
<span id="cb145-2"><a href="unsupervised-methods.html#cb145-2"></a>                           <span class="dt">x =</span> <span class="kw">c</span>(.<span class="dv">54</span>, <span class="fl">.56</span>, <span class="fl">.52</span>),</span>
<span id="cb145-3"><a href="unsupervised-methods.html#cb145-3"></a>                           <span class="dt">y =</span> <span class="kw">c</span>(.<span class="dv">553</span>, <span class="fl">.55</span>, <span class="fl">.56</span>))</span></code></pre></div>
<p>We can visualize them:</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="unsupervised-methods.html#cb146-1"></a>centroids_df <span class="op">%&gt;%</span></span>
<span id="cb146-2"><a href="unsupervised-methods.html#cb146-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y, <span class="dt">color =</span> type)) <span class="op">+</span></span>
<span id="cb146-3"><a href="unsupervised-methods.html#cb146-3"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">4</span>) <span class="op">+</span></span>
<span id="cb146-4"><a href="unsupervised-methods.html#cb146-4"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;orange&quot;</span>, <span class="st">&quot;purple&quot;</span>, <span class="st">&quot;green&quot;</span>)) <span class="op">+</span></span>
<span id="cb146-5"><a href="unsupervised-methods.html#cb146-5"></a><span class="st">  </span><span class="kw">lims</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">+</span></span>
<span id="cb146-6"><a href="unsupervised-methods.html#cb146-6"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Now suppose we add an additional random point in the plot:</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="unsupervised-methods.html#cb147-1"></a>centroids_df <span class="op">%&gt;%</span></span>
<span id="cb147-2"><a href="unsupervised-methods.html#cb147-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y)) <span class="op">+</span></span>
<span id="cb147-3"><a href="unsupervised-methods.html#cb147-3"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> type), <span class="dt">size =</span> <span class="dv">4</span>) <span class="op">+</span></span>
<span id="cb147-4"><a href="unsupervised-methods.html#cb147-4"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="fl">0.25</span>, <span class="dt">y =</span> <span class="fl">0.75</span>)) <span class="op">+</span></span>
<span id="cb147-5"><a href="unsupervised-methods.html#cb147-5"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;orange&quot;</span>, <span class="st">&quot;purple&quot;</span>, <span class="st">&quot;green&quot;</span>)) <span class="op">+</span></span>
<span id="cb147-6"><a href="unsupervised-methods.html#cb147-6"></a><span class="st">  </span><span class="kw">lims</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">+</span></span>
<span id="cb147-7"><a href="unsupervised-methods.html#cb147-7"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>How do we calculate the Euclidean distance between this point and the three clusters? We use this formula:</p>
<p><span class="math display">\[\sqrt{(x_2 - x_1) + (y_2 - y_1)}\]</span></p>
<p>where <span class="math inline">\(x_2\)</span> and <span class="math inline">\(y_2\)</span> are the values for a particular centroid and <span class="math inline">\(x_1\)</span> and <span class="math inline">\(y_1\)</span> are the values for the random point. Here is the manual calculation for each centroid:</p>
<ul>
<li>Orange <span class="math display">\[\sqrt{(0.54 - 0.25) + (0.553 - 0.75)} = 0.304959 \]</span></li>
<li>Purple <span class="math display">\[\sqrt{(0.56 - 0.25) + (0.550 - 0.75)} = 0.3316625 \]</span></li>
<li>Green <span class="math display">\[\sqrt{(0.52 - 0.25) + (0.560 - 0.75)} = 0.2828427 \]</span></li>
</ul>
<p>Since each of these calculations is done on the random point and the three centroids, the only numbers that change between each formula is the location of the centroids. From these results, the random point is closest to the green centroid, as the distance is the smallest (0.28). Let’s assign it to that cluster:</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="unsupervised-methods.html#cb148-1"></a>centroids_df <span class="op">%&gt;%</span></span>
<span id="cb148-2"><a href="unsupervised-methods.html#cb148-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y, <span class="dt">color =</span> type)) <span class="op">+</span></span>
<span id="cb148-3"><a href="unsupervised-methods.html#cb148-3"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">4</span>) <span class="op">+</span></span>
<span id="cb148-4"><a href="unsupervised-methods.html#cb148-4"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">type =</span> <span class="kw">factor</span>(<span class="st">&quot;green&quot;</span>), <span class="dt">x =</span> <span class="fl">0.25</span>, <span class="dt">y =</span> <span class="fl">0.75</span>)) <span class="op">+</span></span>
<span id="cb148-5"><a href="unsupervised-methods.html#cb148-5"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;orange&quot;</span>, <span class="st">&quot;purple&quot;</span>, <span class="st">&quot;green&quot;</span>)) <span class="op">+</span></span>
<span id="cb148-6"><a href="unsupervised-methods.html#cb148-6"></a><span class="st">  </span><span class="kw">lims</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">+</span></span>
<span id="cb148-7"><a href="unsupervised-methods.html#cb148-7"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>The K-Means clustering algorithm applies this calculation for <strong>each point</strong>:</p>
<p><img src="img/km4.png" width="30%" style="display: block; margin: auto;" /></p>
<p>Each point is now assigned the color of the closest centroid. The centroids are still positioned in the center, reflecting the random allocation of the initial points. However, we just reallocated the cluster of every single point. At this stage the K-Means clustering algorithm repeats the same process we just performed manually. It calculates new centroids based on the average of the X and Y of the newly new assigned points:</p>
<p><img src="img/km5.png" width="30%" style="display: block; margin: auto;" /></p>
<p>The next step is to repeat exactly the same strategy again. That is:</p>
<ul>
<li>Calculate the distance between each point and all corresponding clusters</li>
<li>Reassign all points to the cluster of the closest centroid</li>
<li>Recalculate the centroid</li>
</ul>
<p>Mathematically, it can proved that after <span class="math inline">\(N\)</span> iterations, each point will be allocated to a particular centroid and it <strong>will stop being reassigned</strong>:</p>
<p><img src="img/km6.png" width="30%" style="display: block; margin: auto;" /></p>
<p>This approach looks to minimize within-cluster variance and maximize between-cluster variance. That is, respondents are very similar within each cluster with respect to the <span class="math inline">\(P\)</span> variables and very different between clusters.</p>
<p>Althought K-Means clustering is a very fast, interpretable and flexible method, it has important drawbacks. First, K-Means will <strong>always</strong> calculate the number of supplied clusters. That is, if the user supplies three clusters, it will calculate three clusters. If the user supplies, ten clusters, it will also calculate ten clusters. In other words, the clusters calculate by the K-Means algorithm should be interpreted as exploratory and be contrasted with a theoretical description of the problem at hand. The clusters need to make substantive sense rather than statistical sense.</p>
<p>K-Means also has a stability problem. That is, it is completely dependent on the <strong>initial random assignment of the clusters</strong>. Remember how we assigned each point a random cluster in our manual example?</p>
<p><img src="img/km2.png" width="30%" style="display: block; margin: auto;" /></p>
<p>If we repeated the same random assignment again, it is possible we get completely different clusters. For example, here’s a simulation using the same example as before:</p>
<p><img src="img/km7.png" width="80%" style="display: block; margin: auto;" /></p>
<p>These six plots show the exact same model showed above but repeated six times with different random allocation for each one. In some instances, the result is very similar to our initial clusters (for example, the middle top panel) but for others it’s slightly different (for example, the top left panel and the bottom right panel).</p>
<p>This does not mean that the method is useless. It can be very useful to determine clusters whenever the latent distribution of the variables really reflect a clustering. However, instead of taking the clusters at face value, the result of K-Means should be used as an exploratory tool that needs to be compared and complemented with other types of analysis. These methods need to offer evidence of the validity of the clusters as well the robustness of the clusters.</p>
<p>Clustering methods have in general some problems which are important to address. For example, in some instances, centering and scaling variables might be more appropriate, and this can have important implications for the resulting clusters. In addition, outliers can have a big impact on the cluster assignment in general. In addition, small changes in the data can have big impacts on the final clusters.</p>
<p>You might ask yourelf, how can we fit this in <code>R</code>? Let’s suppose that we have reasons to believe that there are different clusters between the socio-economic status of a family and a student’s expected socio-economic status. For example, we might argue that students from low socio-economic status might not have great aspirations, students from middle socio-economic status have average aspirations while students from high socio-economic status might have great aspirations.</p>
<p>The function <code>kmeans</code> is used to calculate the clusters. It accepts two arguments: the dataframe with the <span class="math inline">\(P\)</span> columns and how many clusters the user wants. Let’s pass that to <code>kmeans</code> and visualize the clusters</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="unsupervised-methods.html#cb149-1"></a>res &lt;-</span>
<span id="cb149-2"><a href="unsupervised-methods.html#cb149-2"></a><span class="st">  </span>pisa <span class="op">%&gt;%</span></span>
<span id="cb149-3"><a href="unsupervised-methods.html#cb149-3"></a><span class="st">  </span><span class="kw">select</span>(ESCS, BSMJ) <span class="op">%&gt;%</span></span>
<span id="cb149-4"><a href="unsupervised-methods.html#cb149-4"></a><span class="st">  </span><span class="kw">kmeans</span>(<span class="dt">centers =</span> <span class="dv">3</span>)</span>
<span id="cb149-5"><a href="unsupervised-methods.html#cb149-5"></a></span>
<span id="cb149-6"><a href="unsupervised-methods.html#cb149-6"></a>pisa<span class="op">$</span>clust &lt;-<span class="st"> </span><span class="kw">factor</span>(res<span class="op">$</span>cluster, <span class="dt">levels =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dt">ordered =</span> <span class="ot">TRUE</span>)</span>
<span id="cb149-7"><a href="unsupervised-methods.html#cb149-7"></a></span>
<span id="cb149-8"><a href="unsupervised-methods.html#cb149-8"></a>pisa <span class="op">%&gt;%</span></span>
<span id="cb149-9"><a href="unsupervised-methods.html#cb149-9"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(ESCS, BSMJ, <span class="dt">color =</span> clust)) <span class="op">+</span></span>
<span id="cb149-10"><a href="unsupervised-methods.html#cb149-10"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>) <span class="op">+</span></span>
<span id="cb149-11"><a href="unsupervised-methods.html#cb149-11"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="st">&quot;Index of economic, social and cultural status of family&quot;</span>) <span class="op">+</span></span>
<span id="cb149-12"><a href="unsupervised-methods.html#cb149-12"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="st">&quot;Students expected occupational status&quot;</span>) <span class="op">+</span></span>
<span id="cb149-13"><a href="unsupervised-methods.html#cb149-13"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>Substantively, this example might not make a lot of sense, but it serves to exemplify that the K-Means can find clusters <strong>even</strong> when there aren’t any clusters. From this particular plot, it doesn’t seem to be a clear cut distinction between the three clusters. We can inspect the actual values of the centrois, for example:</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="unsupervised-methods.html#cb150-1"></a>res<span class="op">$</span>centers</span></code></pre></div>
<pre><code>##          ESCS     BSMJ
## 1  0.01219057 55.53109
## 2  0.12278091 75.46021
## 3 -0.29322474 28.82504</code></pre>
<p>Althought some people might think that these methods have no value in the social sciences, I like to think this is because they’ve been trained with a particular hammer, and thus every problem seems like a nail.</p>
<p>We don’t seem problems in a way that can be answered with these techniques because we don’t think about problems with these type of approaches. For example, social scientists working on labor market and technology might want to try to understand why companies cluster into certain cities. K-Means might be a first step towards understanding the variables that discriminate where different types of companies cluster.</p>
<p>A traditional social scientist might think of answering these questions by feeding all variables into a linear model but this defies the whole purpose of clustering: there’s no need for a dependent variable. We can understand the relationship between the variables as a first step towards understanding clustering patterns. These are questions that require creativity from the social science discipline because we’ve been trained in a particular paradigm that is difficult to break.</p>
</div>
<div id="hierarchical-clustering" class="section level2">
<h2><span class="header-section-number">5.3</span> Hierarchical Clustering</h2>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="unsupervised-methods.html#cb152-1"></a>dt &lt;-<span class="st"> </span>USArrests</span>
<span id="cb152-2"><a href="unsupervised-methods.html#cb152-2"></a></span>
<span id="cb152-3"><a href="unsupervised-methods.html#cb152-3"></a>dt <span class="op">%&gt;%</span></span>
<span id="cb152-4"><a href="unsupervised-methods.html#cb152-4"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(Assault, Rape)) <span class="op">+</span></span>
<span id="cb152-5"><a href="unsupervised-methods.html#cb152-5"></a><span class="st">  </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="unsupervised-methods.html#cb153-1"></a><span class="kw">plot</span>(<span class="kw">hclust</span>(<span class="kw">dist</span>(dt[<span class="kw">c</span>(<span class="st">&quot;Assault&quot;</span>, <span class="st">&quot;Rape&quot;</span>)])))</span></code></pre></div>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-28-2.png" width="672" /></p>
</div>
<div id="exercises-2" class="section level2">
<h2><span class="header-section-number">5.4</span> Exercises</h2>
<p>In 2016, the Data Science website Kaggle published a dataset which combines the rankings of more than 2000 universities. This data (which comes from <a href="https://www.kaggle.com/mylesoneill/world-university-rankings/data" class="uri">https://www.kaggle.com/mylesoneill/world-university-rankings/data</a>) contains the name of each university, together with their world ranking, the country where they’re located and several other variables that show the respective ranking in each area. The respective columns are:</p>
<ul>
<li><code>institution</code>: the name of the university</li>
<li><code>world_rank</code>: the overall ranking of the university</li>
<li><code>country</code>: the country where the university is located</li>
<li><code>national_rank</code>: the ranking of the university within each country</li>
<li><code>quality_of_education</code>: the ranking in quality of education</li>
<li><code>alumni_employment</code>: the ranking in employment for their students</li>
<li><code>quality_of_faculty</code>: the ranking of quality of the faculty</li>
<li><code>publications</code>: the ranking in the number of publications</li>
<li><code>influence</code>: the ranking in the overall influence</li>
<li><code>citations</code>: the ranking in the overall number of citations</li>
<li><code>patents</code>: the ranking in the number of patents</li>
<li><code>year</code>: the year of each ranking</li>
</ul>
<p>We can read in the data with the code below:</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="unsupervised-methods.html#cb154-1"></a>dt_all &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/university_ranking_final.csv&quot;</span>)</span>
<span id="cb154-2"><a href="unsupervised-methods.html#cb154-2"></a></span>
<span id="cb154-3"><a href="unsupervised-methods.html#cb154-3"></a><span class="co"># Have a look with:</span></span>
<span id="cb154-4"><a href="unsupervised-methods.html#cb154-4"></a><span class="kw">head</span>(dt_all)</span></code></pre></div>
<pre><code>##                             institution world_rank        country national_rank
## 1                    Harvard University          1            USA             1
## 2 Massachusetts Institute of Technology          2            USA             2
## 3                   Stanford University          3            USA             3
## 4               University of Cambridge          4 United Kingdom             1
## 5    California Institute of Technology          5            USA             4
## 6                  Princeton University          6            USA             5
##   quality_of_education alumni_employment quality_of_faculty publications
## 1                    7                 9                  1            1
## 2                    9                17                  3           12
## 3                   17                11                  5            4
## 4                   10                24                  4           16
## 5                    2                29                  7           37
## 6                    8                14                  2           53
##   influence citations patents year
## 1         1         1       5 2012
## 2         4         4       1 2012
## 3         2         2      15 2012
## 4        16        11      50 2012
## 5        22        22      18 2012
## 6        33        26     101 2012</code></pre>
<div id="ex1" class="section level4 unnumbered">
<h4>1. Explore whether there is room for reducing the number of variables</h4>
<p>Many of these ranking variables (except <code>world_rank</code>) should be very correlated. It could be that universities that score well in quality of faculty, they also score high on the ranking of publications. Explore these ranking variables by using a Principal Component Approach:</p>
<ul>
<li>Limit the data to year 2015</li>
<li>Explore the correlation of the variables <code>quality_of_education</code>, <code>alumni_employment</code>, <code>quality_of_faculty</code>, <code>publications</code>, <code>influence</code>, <code>citations</code> and <code>patents</code> with the function <code>cor</code>.</li>
</ul>
<p>What is the overall conclusion of the correlation? Are these variables weakly or strongly correlated?</p>
<details>
<p><summary><strong>&gt; Answer </strong></summary></p>
<ul>
<li><p>The correlations are quite high for some variables (for example a .84 correlation between publication and influence and a .80 between influence and citation) while lower for others (correlation of .39 between alumni employment and quality of faculty).</p>
<ul>
<li>All in all, these ranking variables seem to correlated to a reasonable degree together.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="unsupervised-methods.html#cb156-1"></a>dt &lt;-</span>
<span id="cb156-2"><a href="unsupervised-methods.html#cb156-2"></a><span class="st">  </span>dt_all <span class="op">%&gt;%</span></span>
<span id="cb156-3"><a href="unsupervised-methods.html#cb156-3"></a><span class="st">  </span><span class="kw">filter</span>(year <span class="op">==</span><span class="st"> </span><span class="dv">2015</span>)</span>
<span id="cb156-4"><a href="unsupervised-methods.html#cb156-4"></a></span>
<span id="cb156-5"><a href="unsupervised-methods.html#cb156-5"></a>dt <span class="op">%&gt;%</span></span>
<span id="cb156-6"><a href="unsupervised-methods.html#cb156-6"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>world_rank, <span class="op">-</span>institution, <span class="op">-</span>country, <span class="op">-</span>national_rank, <span class="op">-</span>year) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb156-7"><a href="unsupervised-methods.html#cb156-7"></a><span class="st">  </span><span class="kw">cor</span>()</span></code></pre></div>
<pre><code>##                      quality_of_education alumni_employment quality_of_faculty
## quality_of_education            1.0000000         0.4736529          0.6859886
## alumni_employment               0.4736529         1.0000000          0.3906046
## quality_of_faculty              0.6859886         0.3906046          1.0000000
## publications                    0.5213777         0.4767965          0.5371659
## influence                       0.5495743         0.4182182          0.5704868
## citations                       0.5210670         0.4491194          0.5599886
## patents                         0.3866523         0.3980353          0.4180885
##                      publications influence citations   patents
## quality_of_education    0.5213777 0.5495743 0.5210670 0.3866523
## alumni_employment       0.4767965 0.4182182 0.4491194 0.3980353
## quality_of_faculty      0.5371659 0.5704868 0.5599886 0.4180885
## publications            1.0000000 0.8451104 0.7888855 0.6169113
## influence               0.8451104 1.0000000 0.8089169 0.5486543
## citations               0.7888855 0.8089169 1.0000000 0.5181444
## patents                 0.6169113 0.5486543 0.5181444 1.0000000</code></pre>
</details>
</div>
<div id="ex2" class="section level4 unnumbered">
<h4>2. Perform a PCA on the ranking variables</h4>
<ul>
<li>Exclude the columns <code>world_rank</code>, <code>institution</code>, <code>country</code>, <code>national_rank</code>, <code>year</code> from the data</li>
<li>Pass the ranking columns to <code>prcomp</code> and set <code>scale = TRUE</code> and <code>center = TRUE</code> to normalize the data</li>
<li>Use <code>tidy</code> with <code>"pcs"</code> to explore the contribution of explained variance by each principal component</li>
<li>Explore what each principal component measures with respect to the original seven variables. This can be done by accessing the object <code>rotation</code> with the <code>$</code> of the result of the <code>prcomp</code></li>
<li>With the package <code>ggfortify</code>, use the function <code>autoplot</code> to plot the principal components</li>
</ul>
<p>How many principal componenets were created? How much variance do the first two and three components explain? What does PC1 measure? What does PC2 measure?</p>
<details>
<p><summary><strong>&gt; Answer </strong></summary></p>
<ul>
<li>Seven principal components were created by <code>prcomp</code>. This is quite a high number, considering that provided seven variables.
<ul>
<li>The first two components, as expected, explain the majority of the variance of the original six variables with a total of 73%.</li>
<li>As PC1 increases, so does the rankings of each institution. This means that higher values of PC1 will reflect university with poorer rankings (because 1st is the best university and 2000 is the worst university within the ranking).</li>
<li>As PC2 increases, <code>publications</code>, <code>influence</code>, <code>citations</code> and <code>patents</code> decrease (meaning better university rankings) while the remaining four increase.</li>
<li>Overall, there doesn’t seem to be great consistency between the principal components and the variables.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="unsupervised-methods.html#cb158-1"></a>res_pc &lt;-</span>
<span id="cb158-2"><a href="unsupervised-methods.html#cb158-2"></a><span class="st">  </span>dt <span class="op">%&gt;%</span></span>
<span id="cb158-3"><a href="unsupervised-methods.html#cb158-3"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>world_rank, <span class="op">-</span>institution, <span class="op">-</span>country, <span class="op">-</span>national_rank, <span class="op">-</span>year) <span class="op">%&gt;%</span></span>
<span id="cb158-4"><a href="unsupervised-methods.html#cb158-4"></a><span class="st">  </span><span class="kw">prcomp</span>(<span class="dt">scale =</span> <span class="ot">TRUE</span>, <span class="dt">center =</span> <span class="ot">TRUE</span>)</span>
<span id="cb158-5"><a href="unsupervised-methods.html#cb158-5"></a></span>
<span id="cb158-6"><a href="unsupervised-methods.html#cb158-6"></a><span class="kw">tidy</span>(res_pc, <span class="st">&quot;pcs&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 7 x 4
##      PC std.dev percent cumulative
##   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;
## 1     1   2.08   0.619       0.619
## 2     2   0.897  0.115       0.734
## 3     3   0.823  0.0967      0.830
## 4     4   0.728  0.0758      0.906
## 5     5   0.551  0.0434      0.950
## 6     6   0.457  0.0299      0.979
## 7     7   0.380  0.0206      1</code></pre>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="unsupervised-methods.html#cb160-1"></a>res_pc<span class="op">$</span>rotation</span></code></pre></div>
<pre><code>##                            PC1        PC2         PC3        PC4         PC5
## quality_of_education 0.3587429  0.5760826  0.16579182 -0.1095131  0.69777431
## alumni_employment    0.3040826  0.3183726 -0.82359376  0.3051580 -0.16374091
## quality_of_faculty   0.3629945  0.4588274  0.35704392 -0.2484045 -0.67321210
## publications         0.4264696 -0.3208572  0.03655215  0.1558644  0.08695217
## influence            0.4240042 -0.2722786  0.20768186  0.2411395  0.09517359
## citations            0.4146066 -0.2442876  0.15335882  0.3319024 -0.11934581
## patents              0.3369240 -0.3456641 -0.31422808 -0.8003621  0.04715163
##                              PC6         PC7
## quality_of_education -0.10366608  0.04813140
## alumni_employment     0.04386293 -0.07752131
## quality_of_faculty    0.12067837  0.02737664
## publications          0.48290226  0.66991734
## influence             0.33664464 -0.72278433
## citations            -0.77984564  0.11110425
## patents              -0.13447189 -0.08587055</code></pre>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="unsupervised-methods.html#cb162-1"></a><span class="kw">autoplot</span>(res_pc,</span>
<span id="cb162-2"><a href="unsupervised-methods.html#cb162-2"></a>         <span class="dt">loadings =</span> <span class="ot">TRUE</span>,</span>
<span id="cb162-3"><a href="unsupervised-methods.html#cb162-3"></a>         <span class="dt">loadings.label =</span> <span class="ot">TRUE</span>,</span>
<span id="cb162-4"><a href="unsupervised-methods.html#cb162-4"></a>         <span class="dt">loadings.label.repel =</span> <span class="ot">TRUE</span>,</span>
<span id="cb162-5"><a href="unsupervised-methods.html#cb162-5"></a>         <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">6</span>) <span class="op">+</span></span>
<span id="cb162-6"><a href="unsupervised-methods.html#cb162-6"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
</details>
</div>
<div id="ex3" class="section level4 unnumbered">
<h4>3. Apply the kmeans to the set of ranking variables</h4>
<ul>
<li>Exclude the columns <code>world_rank</code>, <code>institution</code>, <code>country</code>, <code>national_rank</code>, <code>year</code> from the data</li>
<li>Calculate the average <code>quality_of_education</code> and <code>alumni_employment</code> by country</li>
<li>Pass these country average ranking variables to <code>kmeans</code></li>
<li>Visualize the clusters in a scatterplot of <code>quality_of_education</code> and <code>alumni_employment</code></li>
</ul>
<p>Try several <code>centers</code>. Is there a substantive cluster among these countries?</p>
<details>
<p><summary><strong>&gt; Answer </strong></summary></p>
<ul>
<li>Some of these clusters don’t seem to captury substantial differences. However, the top-left group of countries seems to cluster continually with different centroids.
<ul>
<li>This cluster is continually composed of countries such as France, Spain, Egypt, Argentina, etc…</li>
<li>Do we have reasons to believe that they are very similar in <code>quality_of_education</code> and <code>alumno_employment</code>?</li>
</ul>
An extension of this could hypothesize that we should collect information in terms of growth of labor market and population of students in order to check whether they also cluster on these variables.</li>
</ul>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="unsupervised-methods.html#cb163-1"></a><span class="co">############################# Three clusters ##################################</span></span>
<span id="cb163-2"><a href="unsupervised-methods.html#cb163-2"></a><span class="co">###############################################################################</span></span>
<span id="cb163-3"><a href="unsupervised-methods.html#cb163-3"></a>sum_dt &lt;-<span class="st"> </span></span>
<span id="cb163-4"><a href="unsupervised-methods.html#cb163-4"></a><span class="st">  </span>dt <span class="op">%&gt;%</span></span>
<span id="cb163-5"><a href="unsupervised-methods.html#cb163-5"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>world_rank, <span class="op">-</span>institution, <span class="op">-</span>national_rank, <span class="op">-</span>year) <span class="op">%&gt;%</span></span>
<span id="cb163-6"><a href="unsupervised-methods.html#cb163-6"></a><span class="st">  </span><span class="kw">group_by</span>(country) <span class="op">%&gt;%</span></span>
<span id="cb163-7"><a href="unsupervised-methods.html#cb163-7"></a><span class="st">  </span><span class="kw">summarize_all</span>(mean)</span>
<span id="cb163-8"><a href="unsupervised-methods.html#cb163-8"></a></span>
<span id="cb163-9"><a href="unsupervised-methods.html#cb163-9"></a><span class="kw">set.seed</span>(<span class="dv">523131</span>)</span>
<span id="cb163-10"><a href="unsupervised-methods.html#cb163-10"></a>res_km &lt;-</span>
<span id="cb163-11"><a href="unsupervised-methods.html#cb163-11"></a><span class="st">  </span>sum_dt <span class="op">%&gt;%</span></span>
<span id="cb163-12"><a href="unsupervised-methods.html#cb163-12"></a><span class="st">  </span><span class="kw">select</span>(quality_of_education, alumni_employment) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb163-13"><a href="unsupervised-methods.html#cb163-13"></a><span class="st">  </span><span class="kw">kmeans</span>(<span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">50</span>)</span>
<span id="cb163-14"><a href="unsupervised-methods.html#cb163-14"></a></span>
<span id="cb163-15"><a href="unsupervised-methods.html#cb163-15"></a>sum_dt<span class="op">$</span>.cluster_three &lt;-<span class="st"> </span><span class="kw">factor</span>(res_km<span class="op">$</span>cluster, <span class="dt">levels =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>)</span>
<span id="cb163-16"><a href="unsupervised-methods.html#cb163-16"></a>sum_dt <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb163-17"><a href="unsupervised-methods.html#cb163-17"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(quality_of_education, alumni_employment, <span class="dt">color =</span> .cluster_three)) <span class="op">+</span></span>
<span id="cb163-18"><a href="unsupervised-methods.html#cb163-18"></a><span class="st">  </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> country)) <span class="op">+</span></span>
<span id="cb163-19"><a href="unsupervised-methods.html#cb163-19"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="unsupervised-methods.html#cb164-1"></a><span class="co">############################# Two clusters ####################################</span></span>
<span id="cb164-2"><a href="unsupervised-methods.html#cb164-2"></a><span class="co">###############################################################################</span></span>
<span id="cb164-3"><a href="unsupervised-methods.html#cb164-3"></a></span>
<span id="cb164-4"><a href="unsupervised-methods.html#cb164-4"></a><span class="kw">set.seed</span>(<span class="dv">523131</span>)</span>
<span id="cb164-5"><a href="unsupervised-methods.html#cb164-5"></a>res_km &lt;-</span>
<span id="cb164-6"><a href="unsupervised-methods.html#cb164-6"></a><span class="st">  </span>sum_dt <span class="op">%&gt;%</span></span>
<span id="cb164-7"><a href="unsupervised-methods.html#cb164-7"></a><span class="st">  </span><span class="kw">select</span>(quality_of_education, alumni_employment) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb164-8"><a href="unsupervised-methods.html#cb164-8"></a><span class="st">  </span><span class="kw">kmeans</span>(<span class="dt">centers =</span> <span class="dv">2</span>, <span class="dt">nstart =</span> <span class="dv">50</span>)</span>
<span id="cb164-9"><a href="unsupervised-methods.html#cb164-9"></a></span>
<span id="cb164-10"><a href="unsupervised-methods.html#cb164-10"></a>sum_dt<span class="op">$</span>.cluster_two &lt;-<span class="st"> </span><span class="kw">factor</span>(res_km<span class="op">$</span>cluster, <span class="dt">levels =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)</span>
<span id="cb164-11"><a href="unsupervised-methods.html#cb164-11"></a>sum_dt <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb164-12"><a href="unsupervised-methods.html#cb164-12"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(quality_of_education, alumni_employment, <span class="dt">color =</span> .cluster_two)) <span class="op">+</span></span>
<span id="cb164-13"><a href="unsupervised-methods.html#cb164-13"></a><span class="st">  </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> country)) <span class="op">+</span></span>
<span id="cb164-14"><a href="unsupervised-methods.html#cb164-14"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-32-2.png" width="672" /></p>
</details>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-boehmke2019">
<p>Boehmke, Brad, and Brandon M Greenwell. 2019. <em>Hands-on Machine Learning with R</em>. CRC Press.</p>
</div>
<div id="ref-james2013">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="loss-functions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="no-free-lunch.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
