<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Unsupervised methods | Machine Learning for Social Scientists</title>
  <meta name="description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Unsupervised methods | Machine Learning for Social Scientists" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://cimentadaj.github.io/ml_socsci/" />
  
  <meta property="og:description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  <meta name="github-repo" content="cimentadaj/ml_socsci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Unsupervised methods | Machine Learning for Social Scientists" />
  
  <meta name="twitter:description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  

<meta name="author" content="Jorge Cimentada" />


<meta name="date" content="2020-06-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="loss-functions.html"/>
<link rel="next" href="syllabus.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.13/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning for Social Scientists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html"><i class="fa fa-check"></i><b>1</b> Machine Learning for Social Scientists</a><ul>
<li class="chapter" data-level="1.1" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#a-different-way-of-thinking"><i class="fa fa-check"></i><b>1.1</b> A different way of thinking</a></li>
<li class="chapter" data-level="1.2" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#split-your-data-into-trainingtesting"><i class="fa fa-check"></i><b>1.2</b> Split your data into training/testing</a></li>
<li class="chapter" data-level="1.3" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#cross-validation"><i class="fa fa-check"></i><b>1.3</b> Cross-validation</a></li>
<li class="chapter" data-level="1.4" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>1.4</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="1.5" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#an-example"><i class="fa fa-check"></i><b>1.5</b> An example</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>2</b> Regularization</a><ul>
<li class="chapter" data-level="2.1" data-path="regularization.html"><a href="regularization.html#ridge-regularization"><i class="fa fa-check"></i><b>2.1</b> Ridge regularization</a></li>
<li class="chapter" data-level="2.2" data-path="regularization.html"><a href="regularization.html#lasso-regularization"><i class="fa fa-check"></i><b>2.2</b> Lasso regularization</a></li>
<li class="chapter" data-level="2.3" data-path="regularization.html"><a href="regularization.html#elastic-net-regularization"><i class="fa fa-check"></i><b>2.3</b> Elastic Net regularization</a></li>
<li class="chapter" data-level="2.4" data-path="regularization.html"><a href="regularization.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>3</b> Tree-based methods</a><ul>
<li class="chapter" data-level="3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.1</b> Decision trees</a><ul>
<li class="chapter" data-level="3.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#advancedsplit"><i class="fa fa-check"></i><b>3.1.1</b> Advanced: how do trees choose where to split?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging"><i class="fa fa-check"></i><b>3.2</b> Bagging</a></li>
<li class="chapter" data-level="3.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests"><i class="fa fa-check"></i><b>3.3</b> Random Forests</a></li>
<li class="chapter" data-level="3.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting"><i class="fa fa-check"></i><b>3.4</b> Boosting</a></li>
<li class="chapter" data-level="3.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercises-1"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="loss-functions.html"><a href="loss-functions.html"><i class="fa fa-check"></i><b>4</b> Loss functions</a><ul>
<li class="chapter" data-level="4.1" data-path="loss-functions.html"><a href="loss-functions.html#continuous-loss-functions"><i class="fa fa-check"></i><b>4.1</b> Continuous loss functions</a><ul>
<li class="chapter" data-level="4.1.1" data-path="loss-functions.html"><a href="loss-functions.html#root-mean-square-error-rmse"><i class="fa fa-check"></i><b>4.1.1</b> Root Mean Square Error (RMSE)</a></li>
<li class="chapter" data-level="4.1.2" data-path="loss-functions.html"><a href="loss-functions.html#mean-absolute-error"><i class="fa fa-check"></i><b>4.1.2</b> Mean Absolute Error</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="loss-functions.html"><a href="loss-functions.html#binary-loss-functions"><i class="fa fa-check"></i><b>4.2</b> Binary loss functions</a><ul>
<li class="chapter" data-level="4.2.1" data-path="loss-functions.html"><a href="loss-functions.html#confusion-matrices"><i class="fa fa-check"></i><b>4.2.1</b> Confusion Matrices</a></li>
<li class="chapter" data-level="4.2.2" data-path="loss-functions.html"><a href="loss-functions.html#roc-curves-and-area-under-the-curve"><i class="fa fa-check"></i><b>4.2.2</b> ROC Curves and Area Under the Curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html"><i class="fa fa-check"></i><b>5</b> Unsupervised methods</a><ul>
<li class="chapter" data-level="5.1" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>5.1</b> Principal Component Analysis (PCA)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="syllabus.html"><a href="syllabus.html"><i class="fa fa-check"></i><b>6</b> Syllabus</a><ul>
<li class="chapter" data-level="6.1" data-path="syllabus.html"><a href="syllabus.html#course-description"><i class="fa fa-check"></i><b>6.1</b> Course description</a></li>
<li class="chapter" data-level="6.2" data-path="syllabus.html"><a href="syllabus.html#schedule"><i class="fa fa-check"></i><b>6.2</b> Schedule</a></li>
<li class="chapter" data-level="6.3" data-path="syllabus.html"><a href="syllabus.html#software"><i class="fa fa-check"></i><b>6.3</b> Software</a></li>
<li class="chapter" data-level="6.4" data-path="syllabus.html"><a href="syllabus.html#prerequisites"><i class="fa fa-check"></i><b>6.4</b> Prerequisites</a></li>
<li class="chapter" data-level="6.5" data-path="syllabus.html"><a href="syllabus.html#about-the-author"><i class="fa fa-check"></i><b>6.5</b> About the author</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Social Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="unsupervised-methods" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Unsupervised methods</h1>
<p>Unsupervised learning is a very popular concept in machine learning. Although we social scientists are aware of some of these methods, we do not take advantage of them as much as machine learning practitioners. What is unsupervised learning? Let’s get that out of the way: this just means that a particular statistical method <strong>does not have a dependent variable</strong>. These are models that look to find relationships between independent variables without the need of a dependent variable.</p>
<p>One common unsupervised method that social scientists are aware of is the <strong>P</strong>rincipal <strong>C</strong>omponent <strong>A</strong>nalysis or <span class="math inline">\(PCA\)</span>. <span class="math inline">\(PCA\)</span> aims to summarize many variables into a small subset of variables that can capture the greatest variance out of all the main variables. We really never thought about this as an ‘unsupervised’ method, but it is used widely for predictive tasks. Before we begin, let’s load the packages and data we’ll be using.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="unsupervised-methods.html#cb131-1"></a><span class="kw">library</span>(tidymodels)</span>
<span id="cb131-2"><a href="unsupervised-methods.html#cb131-2"></a><span class="kw">library</span>(tidyflow)</span>
<span id="cb131-3"><a href="unsupervised-methods.html#cb131-3"></a><span class="kw">library</span>(ggfortify)</span>
<span id="cb131-4"><a href="unsupervised-methods.html#cb131-4"></a></span>
<span id="cb131-5"><a href="unsupervised-methods.html#cb131-5"></a>data_link &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot;</span></span>
<span id="cb131-6"><a href="unsupervised-methods.html#cb131-6"></a>pisa &lt;-<span class="st"> </span><span class="kw">read.csv</span>(data_link)</span></code></pre></div>
<div id="principal-component-analysis-pca" class="section level2">
<h2><span class="header-section-number">5.1</span> Principal Component Analysis (PCA)</h2>
<p><strong>P</strong>rincipal <strong>C</strong>omponent <strong>A</strong>nalysis or <span class="math inline">\(PCA\)</span> is a method that tries to summarize many columns into a very small subset that captures the greatest variability of the original columns. Social Scientists often use this method to create more ‘parsimonious’ models and summarize many variables into a few ‘strong’ variables.</p>
<p><span class="math inline">\(PCA\)</span> works by creating several components which are the normalized linear combination of the variables in the model. In the <code>pisa</code> data there are a six variables which asks the student whether they’ve suffered negative behavior from their friends in the past 12 months. In particular, it asks whether</p>
<ul>
<li>Other students left them out of things on purpose</li>
<li>Other students made fun of them</li>
<li>They were threatened by other students</li>
<li>Other students took away or destroyed things that belonged to them</li>
<li>They got hit or pushed around by other students</li>
<li>Other students spread nasty rumours about them</li>
</ul>
<p>For each of these variables, the scale ranges from 1 to 4 where 4 is ‘Once a week or more’ and 1 is ‘Never or almost never’. In other words, the higher the number, the more negative their response.</p>
<p>Let’s rename these variables into more interpretable names and look at their correlation:</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="unsupervised-methods.html#cb132-1"></a>pisa &lt;-</span>
<span id="cb132-2"><a href="unsupervised-methods.html#cb132-2"></a><span class="st">  </span>pisa <span class="op">%&gt;%</span></span>
<span id="cb132-3"><a href="unsupervised-methods.html#cb132-3"></a><span class="st">  </span><span class="kw">rename</span>(</span>
<span id="cb132-4"><a href="unsupervised-methods.html#cb132-4"></a>    <span class="dt">past12_left_out =</span> ST038Q03NA,</span>
<span id="cb132-5"><a href="unsupervised-methods.html#cb132-5"></a>    <span class="dt">past12_madefun_of_me =</span> ST038Q04NA,</span>
<span id="cb132-6"><a href="unsupervised-methods.html#cb132-6"></a>    <span class="dt">past12_threatened =</span> ST038Q05NA,</span>
<span id="cb132-7"><a href="unsupervised-methods.html#cb132-7"></a>    <span class="dt">past12_destroyed_personal =</span> ST038Q06NA,</span>
<span id="cb132-8"><a href="unsupervised-methods.html#cb132-8"></a>    <span class="dt">past12_got_hit =</span> ST038Q07NA,</span>
<span id="cb132-9"><a href="unsupervised-methods.html#cb132-9"></a>    <span class="dt">past12_spread_rumours =</span> ST038Q08NA</span>
<span id="cb132-10"><a href="unsupervised-methods.html#cb132-10"></a>  )</span>
<span id="cb132-11"><a href="unsupervised-methods.html#cb132-11"></a></span>
<span id="cb132-12"><a href="unsupervised-methods.html#cb132-12"></a>pisa_selected &lt;-</span>
<span id="cb132-13"><a href="unsupervised-methods.html#cb132-13"></a><span class="st">  </span>pisa <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb132-14"><a href="unsupervised-methods.html#cb132-14"></a><span class="st">  </span><span class="kw">select</span>(<span class="kw">starts_with</span>(<span class="st">&quot;past12&quot;</span>))</span>
<span id="cb132-15"><a href="unsupervised-methods.html#cb132-15"></a></span>
<span id="cb132-16"><a href="unsupervised-methods.html#cb132-16"></a><span class="kw">cor</span>(pisa_selected)</span></code></pre></div>
<pre><code>##                           past12_left_out past12_madefun_of_me
## past12_left_out                 1.0000000            0.6073982
## past12_madefun_of_me            0.6073982            1.0000000
## past12_threatened               0.4454125            0.4712083
## past12_destroyed_personal       0.4037351            0.4165931
## past12_got_hit                  0.3918129            0.4480862
## past12_spread_rumours           0.4746302            0.5069299
##                           past12_threatened past12_destroyed_personal
## past12_left_out                   0.4454125                 0.4037351
## past12_madefun_of_me              0.4712083                 0.4165931
## past12_threatened                 1.0000000                 0.5685773
## past12_destroyed_personal         0.5685773                 1.0000000
## past12_got_hit                    0.5807617                 0.6206485
## past12_spread_rumours             0.5513099                 0.4543380
##                           past12_got_hit past12_spread_rumours
## past12_left_out                0.3918129             0.4746302
## past12_madefun_of_me           0.4480862             0.5069299
## past12_threatened              0.5807617             0.5513099
## past12_destroyed_personal      0.6206485             0.4543380
## past12_got_hit                 1.0000000             0.4451408
## past12_spread_rumours          0.4451408             1.0000000</code></pre>
<p>Most correlations lie between <code>0.4</code> and <code>0.6</code>, a somewhat acceptable threshold for assesing whether they can be reduced into fewer variables. <span class="math inline">\(PCA\)</span> works by receiving as input <span class="math inline">\(P\)</span> variables (in this case six) and calculating the normalized linear combination of the <span class="math inline">\(P\)</span> variables. This new variable is the linear combination of the six variables that captures the greatest variance out of all of them. <span class="math inline">\(PCA\)</span> continues to calculate other normalized linear combinations <strong>but</strong> with the constraint that they need to be completely uncorrelated to all the other normalized linear combinations.</p>
<p>This approach has the advantage that it constructs as many principal components (new variables) as it can, as long as they all capture 100% of the variability of the original <span class="math inline">\(P\)</span> variables, and each of these new variables are completely uncorrelated between each other.</p>
<p>Each variable is assessed by how much variance it explains of the original <span class="math inline">\(P\)</span> variables and each new variable is completely independent of the others. Depending on the correlation of the <span class="math inline">\(P\)</span> input variables, you might get three principal components that capture all of the variability of the original <span class="math inline">\(P\)</span> variables. In other cases, you can get an you might many more.</p>
<p>This discussion is getting too theoretical. Let’s get get some hands-on experience of how this works. Let’s pass in our six variables to the function <code>prcomp</code>, which estimates these principal components based on our six variables. However, for <span class="math inline">\(PCA\)</span> to work well, we need to center and scale the independent variables. This means that the independent variables will have a mean of zero and a standard deviation of one. <code>prcomp</code> does this for you, but you should be aware of this for future discussion:</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="unsupervised-methods.html#cb134-1"></a>pc &lt;-<span class="st"> </span><span class="kw">prcomp</span>(pisa_selected)</span>
<span id="cb134-2"><a href="unsupervised-methods.html#cb134-2"></a>all_pcs &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(pc<span class="op">$</span>x)</span>
<span id="cb134-3"><a href="unsupervised-methods.html#cb134-3"></a><span class="kw">head</span>(all_pcs)</span></code></pre></div>
<pre><code>##            PC1        PC2         PC3          PC4          PC5          PC6
## 1 -2.836172297 -0.7549602 -1.91065434 -0.232647114 -0.368981283 -1.885607656
## 2 -1.478020766  0.6622561  0.94113153  0.181451711  0.149387648  0.678384471
## 3  1.025953306  0.1602906 -0.03806864 -0.008994148  0.009439987 -0.002391996
## 4 -0.002173173 -0.7902197 -0.10112894 -0.197389118  0.013521080 -0.002718289
## 5 -4.832075955  0.1996595  0.39221922 -0.256660522 -1.178883084  0.150399629
## 6 -1.132036976 -1.8534154 -0.68913950  0.914561923  0.065907346  0.087208533</code></pre>
<p>Let’s explain what just happened. Our dataset <code>pisa_selected</code> contains the six variables of interest. We passed that to <code>prcomp</code> which calculated the principal components. With this model object, we extracted the dataframe with the new principal components. The result of all of this is a dataframe with six new columns. These six new columns are <strong>not</strong> the initial six variables from <code>pisa_selected</code>. Instead, they are variables that summarize the relationship of these six variables.</p>
<p>You might ask yourself, how come six variables <strong>summarize</strong> six variables? That doesn’t make much sense. The whole idea is that fewer variables can summarize the original six. Let’s look at how much variance of the original <span class="math inline">\(P\)</span> variables these ‘index’ variables explain:</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="unsupervised-methods.html#cb136-1"></a><span class="kw">tidy</span>(pc, <span class="st">&quot;pcs&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 4
##      PC std.dev percent cumulative
##   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;
## 1     1   1.34   0.591       0.591
## 2     2   0.640  0.135       0.726
## 3     3   0.530  0.0929      0.819
## 4     4   0.522  0.0899      0.909
## 5     5   0.394  0.0513      0.960
## 6     6   0.347  0.0397      1</code></pre>
<p>This output shows how well each principal component is explaining the original six variables. For example, the first principal component (1st row) explains about 59% of the variance of the six variables. The second principal component explains an additional 13.5%, for a total of 72.6% between the two. This is certainly better. It means that the first two variables seem to have some power in summarizing the original six variables.</p>
<p>Let’s focus on the first two principal components. They are supposed to be completely uncorrelated, so let’s check that ourselves:</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="unsupervised-methods.html#cb138-1"></a><span class="kw">cor</span>(all_pcs[<span class="kw">c</span>(<span class="st">&quot;PC1&quot;</span>, <span class="st">&quot;PC2&quot;</span>)])</span></code></pre></div>
<pre><code>##                         PC1                     PC2
## PC1  1.00000000000000000000 -0.00000000000001545012
## PC2 -0.00000000000001545012  1.00000000000000000000</code></pre>
<p>As expected, the correlation between these two variables is 0.</p>
<p>How do we use these two variables? Well, a typical social scientist would make sure that their expected explanatory power of the two components is high enough for their research problem. If it is, they would include these two columns in their statistical models instead of the six variables.
However, <span class="math inline">\(PCA\)</span> is all about exploratory data analysis. We might want to go further and explore how the original six variables are related to these principal components. These two principal components are a bit of a black box at this point. Which variables do they represent? We can check that with the initial output of <code>prcomp</code>:</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="unsupervised-methods.html#cb140-1"></a>pc<span class="op">$</span>rotation[, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]</span></code></pre></div>
<pre><code>##                                  PC1        PC2
## past12_left_out           -0.4631946 -0.4189125
## past12_madefun_of_me      -0.5649319 -0.5315979
## past12_threatened         -0.3446963  0.4025682
## past12_destroyed_personal -0.2694606  0.3405411
## past12_got_hit            -0.2987481  0.3715999
## past12_spread_rumours     -0.4308453  0.3546832</code></pre>
<p>These two columns show the correlations between the six original variables and the first two principal components. Let’s focus on the first column. The first thing that stands out is that for all the six variables, the correlation is negative. This means that as the respondents answered negatively to the six questions, the first principal component decreases. Informally, we could call this variable a ‘negative-peer index’.</p>
<p>Moving to the second column, four of these six variables correlate positively with the second principal component. At least for these four variables, the principal components tend capture the exact opposite relationship. In other words, at least for these four variables, this is a ‘positive-peer index’. This type of decomposition is precisely where the usefulness of this type of method comes in. It allows us to summarize many variables into a small set of components that capture meaningful variation.</p>
<p>The package <code>ggfortifty</code> contains the function <code>autoplot</code> which can help us visualize these correlations in a more meaningful way:</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="unsupervised-methods.html#cb142-1"></a><span class="kw">set.seed</span>(<span class="dv">6652</span>)</span>
<span id="cb142-2"><a href="unsupervised-methods.html#cb142-2"></a>pc <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb142-3"><a href="unsupervised-methods.html#cb142-3"></a><span class="st">  </span><span class="kw">autoplot</span>(<span class="dt">loadings =</span> <span class="ot">TRUE</span>,</span>
<span id="cb142-4"><a href="unsupervised-methods.html#cb142-4"></a>           <span class="dt">loadings.label =</span> <span class="ot">TRUE</span>,</span>
<span id="cb142-5"><a href="unsupervised-methods.html#cb142-5"></a>           <span class="dt">loadings.label.repel =</span> <span class="ot">TRUE</span>,</span>
<span id="cb142-6"><a href="unsupervised-methods.html#cb142-6"></a>           <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">6</span>) <span class="op">+</span></span>
<span id="cb142-7"><a href="unsupervised-methods.html#cb142-7"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Let’s distill this plot. On the <code>X</code> axis we have the actual column of the first principal component (PC1) (this is literaly the same column we saw in the object <code>all_pcs</code>; if it serves to refresh your memory, check it out with <code>head(all_pcs)</code>). As you can see, the label of the <code>X</code> axis already tells us that this component explains nearly 60% of the variance of these six variables. On the <code>Y</code> axis we have the actual column of the second principal component (PC2) (same as before, you can see this with <code>head(all_pcs)</code>). This principal component explains an additional 13.5% of the variance of the six variables.</p>
<p>What this plot is trying to show is where these six variables are clustered between these two principal components. Since these two variables were centered and scaled to have a mean of zero, the red lines always begin at the intersection of the zero in PC1 and PC2. In other words, we can see more clearly the correlations we saw earlier. For example, remember that the first two variables were both negatively correlated with both PC1 and PC2. These two variables are located in the bottom left of the plot, showing that for both principal components both variables are associated with lower values of PC1 and PC2:</p>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>There is nothing new here. This is the same thing we interpreted from the correlation but from a more intuitive visualization. If you remember the other four variables from the correlation, they showed negative correlations with PC1 and positive correlations with PC2. This means that these variables should cluster <strong>below</strong> the average of PC1 and <strong>higher</strong> than the average of PC2. We can see that more clearly if we first add a line showing the zero values for both variables:</p>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Any values to the <strong>left</strong> of the the vertical line are low values of PC1 while all values <strong>above</strong> the horizontal line are high values for PC2. Building on this intuition, we should find that the remaining four variables cluster at lower values of PC1 and at higher values of PC1:</p>
<p><img src="05_unsupervised_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Depending on these correlations, you might reject to focus on the first two principal components and explore this same plot for PC1 and PC3 or PC2 and PC4. There’s no clear cut rule for the number of principal components to use. The user should instead explore these plots to understand whether there are interesting findings from clustering many variables into fewer variables. Depending on this, you might reject the idea entirely of using principal components. Or you might use these principal components to represent some interesting findings for your theoretical model.</p>
<p>In any case, this method is inherently exploratory. It serves as way to understand whether we can reduce correlated variables into a small subset of variables that represent them. For a social science point of view, this method is often used for reducing the number of variables. However, there is still room for using it as a clustering method to understand whether certain variables can help us summarize our understanding into simpler concepts.</p>
<p>Having said this, for predictive tasks there is an objective measure on how many principal components to use: the ones that improve predictions the most. Using our previous example, we could perform a grid search on a number of components to see which one maximizes predictive accuracy. Let’s run a random forest by regressing the variable <code>math_score</code> on all variables in the dataset. While we do that, let’s try models with different number of principal components:</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="unsupervised-methods.html#cb143-1"></a><span class="co"># Set the number of components `num_comp`</span></span>
<span id="cb143-2"><a href="unsupervised-methods.html#cb143-2"></a><span class="co"># to be tuned</span></span>
<span id="cb143-3"><a href="unsupervised-methods.html#cb143-3"></a>rcp &lt;-</span>
<span id="cb143-4"><a href="unsupervised-methods.html#cb143-4"></a><span class="st">  </span><span class="er">~</span><span class="st"> </span><span class="kw">recipe</span>(.x, math_score <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span></span>
<span id="cb143-5"><a href="unsupervised-methods.html#cb143-5"></a><span class="st">    </span><span class="kw">step_pca</span>(<span class="kw">starts_with</span>(<span class="st">&quot;past12_&quot;</span>), <span class="dt">num_comp =</span> <span class="kw">tune</span>())</span>
<span id="cb143-6"><a href="unsupervised-methods.html#cb143-6"></a></span>
<span id="cb143-7"><a href="unsupervised-methods.html#cb143-7"></a>tflow &lt;-</span>
<span id="cb143-8"><a href="unsupervised-methods.html#cb143-8"></a><span class="st">  </span>pisa <span class="op">%&gt;%</span></span>
<span id="cb143-9"><a href="unsupervised-methods.html#cb143-9"></a><span class="st">  </span><span class="kw">tidyflow</span>(<span class="dt">seed =</span> <span class="dv">25131</span>) <span class="op">%&gt;%</span></span>
<span id="cb143-10"><a href="unsupervised-methods.html#cb143-10"></a><span class="st">  </span><span class="kw">plug_split</span>(initial_split) <span class="op">%&gt;%</span></span>
<span id="cb143-11"><a href="unsupervised-methods.html#cb143-11"></a><span class="st">  </span><span class="kw">plug_recipe</span>(rcp) <span class="op">%&gt;%</span></span>
<span id="cb143-12"><a href="unsupervised-methods.html#cb143-12"></a><span class="st">  </span><span class="kw">plug_model</span>(<span class="kw">set_engine</span>(<span class="kw">rand_forest</span>(<span class="dt">mode =</span> <span class="st">&quot;regression&quot;</span>), <span class="st">&quot;ranger&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb143-13"><a href="unsupervised-methods.html#cb143-13"></a><span class="st">  </span><span class="kw">plug_resample</span>(vfold_cv) <span class="op">%&gt;%</span></span>
<span id="cb143-14"><a href="unsupervised-methods.html#cb143-14"></a><span class="st">  </span><span class="co"># Set `num_comp`in the grid to 1:3</span></span>
<span id="cb143-15"><a href="unsupervised-methods.html#cb143-15"></a><span class="st">  </span><span class="co"># meaning that we&#39;ll try the models with</span></span>
<span id="cb143-16"><a href="unsupervised-methods.html#cb143-16"></a><span class="st">  </span><span class="co"># number of components 1, 2 and 3</span></span>
<span id="cb143-17"><a href="unsupervised-methods.html#cb143-17"></a><span class="st">  </span><span class="kw">plug_grid</span>(expand.grid, <span class="dt">num_comp =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>)</span>
<span id="cb143-18"><a href="unsupervised-methods.html#cb143-18"></a></span>
<span id="cb143-19"><a href="unsupervised-methods.html#cb143-19"></a>res_rf &lt;-<span class="st"> </span><span class="kw">fit</span>(tflow)</span>
<span id="cb143-20"><a href="unsupervised-methods.html#cb143-20"></a></span>
<span id="cb143-21"><a href="unsupervised-methods.html#cb143-21"></a>res_rf <span class="op">%&gt;%</span></span>
<span id="cb143-22"><a href="unsupervised-methods.html#cb143-22"></a><span class="st">  </span><span class="kw">pull_tflow_fit_tuning</span>() <span class="op">%&gt;%</span></span>
<span id="cb143-23"><a href="unsupervised-methods.html#cb143-23"></a><span class="st">  </span><span class="kw">collect_metrics</span>() <span class="op">%&gt;%</span></span>
<span id="cb143-24"><a href="unsupervised-methods.html#cb143-24"></a><span class="st">  </span><span class="kw">filter</span>(.metric <span class="op">==</span><span class="st"> &quot;rmse&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 3 x 6
##   num_comp .metric .estimator  mean     n std_err
##      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1        1 rmse    standard    40.8    10   0.402
## 2        2 rmse    standard    40.8    10   0.438
## 3        3 rmse    standard    40.8    10   0.422</code></pre>
<p>These are the average results of running a 10-fold cross-validation trying out models with one, two and three principal components respectively. As we can see from the <code>mean</code> column, there is little difference between the average <span class="math inline">\(RMSE\)</span> of these different models. If there are important reasons to include these variables in the model and we want to reduce the number of variables in the model for simplicity, we could just keep the model with one principal component.</p>
<p>However, there’s also an alternative approach. <code>step_pca</code> allows you to specify the minimum explanatory power of the principal components. As discussed in the documentation of <code>step_pca</code>, <em>you specify the fraction of the total variance that should be covered by the components. For example, <code>threshold = .75</code> means that <code>step_pca</code> should generate enough components to capture 75% of the variance.</em></p>
<p>We can try our previous models with a 90% threshold. Since we will not perform a grid search, we will drop the grid and only keep the cross-validation to get uncertain estimates of our loss function <span class="math inline">\(RMSE\)</span>:</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="unsupervised-methods.html#cb145-1"></a><span class="co"># Define a new recipe</span></span>
<span id="cb145-2"><a href="unsupervised-methods.html#cb145-2"></a><span class="co"># where threshold is .90</span></span>
<span id="cb145-3"><a href="unsupervised-methods.html#cb145-3"></a>rcp &lt;-</span>
<span id="cb145-4"><a href="unsupervised-methods.html#cb145-4"></a><span class="st">  </span><span class="er">~</span><span class="st"> </span><span class="kw">recipe</span>(.x, math_score <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span></span>
<span id="cb145-5"><a href="unsupervised-methods.html#cb145-5"></a><span class="st">    </span><span class="kw">step_pca</span>(<span class="kw">starts_with</span>(<span class="st">&quot;past12_&quot;</span>), <span class="dt">threshold =</span> <span class="fl">.90</span>)</span>
<span id="cb145-6"><a href="unsupervised-methods.html#cb145-6"></a></span>
<span id="cb145-7"><a href="unsupervised-methods.html#cb145-7"></a><span class="co"># Replace the previous recipe</span></span>
<span id="cb145-8"><a href="unsupervised-methods.html#cb145-8"></a><span class="co"># and drop the grid</span></span>
<span id="cb145-9"><a href="unsupervised-methods.html#cb145-9"></a>tflow &lt;-</span>
<span id="cb145-10"><a href="unsupervised-methods.html#cb145-10"></a><span class="st">  </span>tflow <span class="op">%&gt;%</span></span>
<span id="cb145-11"><a href="unsupervised-methods.html#cb145-11"></a><span class="st">  </span><span class="kw">replace_recipe</span>(rcp) <span class="op">%&gt;%</span></span>
<span id="cb145-12"><a href="unsupervised-methods.html#cb145-12"></a><span class="st">  </span><span class="kw">drop_grid</span>()</span>
<span id="cb145-13"><a href="unsupervised-methods.html#cb145-13"></a></span>
<span id="cb145-14"><a href="unsupervised-methods.html#cb145-14"></a>res_rf &lt;-<span class="st"> </span><span class="kw">fit</span>(tflow)</span>
<span id="cb145-15"><a href="unsupervised-methods.html#cb145-15"></a></span>
<span id="cb145-16"><a href="unsupervised-methods.html#cb145-16"></a>res_cv &lt;-</span>
<span id="cb145-17"><a href="unsupervised-methods.html#cb145-17"></a><span class="st">  </span>res_rf <span class="op">%&gt;%</span></span>
<span id="cb145-18"><a href="unsupervised-methods.html#cb145-18"></a><span class="st">  </span><span class="kw">pull_tflow_fit_tuning</span>() <span class="op">%&gt;%</span></span>
<span id="cb145-19"><a href="unsupervised-methods.html#cb145-19"></a><span class="st">  </span><span class="kw">collect_metrics</span>()</span>
<span id="cb145-20"><a href="unsupervised-methods.html#cb145-20"></a></span>
<span id="cb145-21"><a href="unsupervised-methods.html#cb145-21"></a>res_cv</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric .estimator   mean     n std_err
##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 rmse    standard   40.8      10 0.377  
## 2 rsq     standard    0.835    10 0.00392</code></pre>
<p>This approach offers a very similar <span class="math inline">\(RMSE\)</span> of 40.8. Althought not possible at this moment, <code>tidymodels</code> is expected to allow the <code>threshold</code> parameter to be <code>tune</code> such that you can perform a grid search of this value as well (for those interested, see <a href="https://github.com/tidymodels/recipes/issues/534">here</a>).</p>
<p>Although <span class="math inline">\(PCA\)</span> is a very useful method for summarizing information, it is based on the notion that the variables to be summarized are best summarized through a linear combination. In other instances, non-linear methods can also prove useful as exploratory means.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="loss-functions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="syllabus.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
