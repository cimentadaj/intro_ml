<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Unsupervised methods | Machine Learning for Social Scientists</title>
  <meta name="description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Unsupervised methods | Machine Learning for Social Scientists" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://cimentadaj.github.io/ml_socsci/" />
  
  <meta property="og:description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  <meta name="github-repo" content="cimentadaj/ml_socsci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Unsupervised methods | Machine Learning for Social Scientists" />
  
  <meta name="twitter:description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  

<meta name="author" content="Jorge Cimentada" />


<meta name="date" content="2020-06-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="loss-functions.html"/>
<link rel="next" href="syllabus.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.13/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning for Social Scientists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html"><i class="fa fa-check"></i><b>1</b> Machine Learning for Social Scientists</a><ul>
<li class="chapter" data-level="1.1" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#a-different-way-of-thinking"><i class="fa fa-check"></i><b>1.1</b> A different way of thinking</a></li>
<li class="chapter" data-level="1.2" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#split-your-data-into-trainingtesting"><i class="fa fa-check"></i><b>1.2</b> Split your data into training/testing</a></li>
<li class="chapter" data-level="1.3" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#cross-validation"><i class="fa fa-check"></i><b>1.3</b> Cross-validation</a></li>
<li class="chapter" data-level="1.4" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>1.4</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="1.5" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#an-example"><i class="fa fa-check"></i><b>1.5</b> An example</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>2</b> Regularization</a><ul>
<li class="chapter" data-level="2.1" data-path="regularization.html"><a href="regularization.html#ridge-regularization"><i class="fa fa-check"></i><b>2.1</b> Ridge regularization</a></li>
<li class="chapter" data-level="2.2" data-path="regularization.html"><a href="regularization.html#lasso-regularization"><i class="fa fa-check"></i><b>2.2</b> Lasso regularization</a></li>
<li class="chapter" data-level="2.3" data-path="regularization.html"><a href="regularization.html#elastic-net-regularization"><i class="fa fa-check"></i><b>2.3</b> Elastic Net regularization</a></li>
<li class="chapter" data-level="2.4" data-path="regularization.html"><a href="regularization.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>3</b> Tree-based methods</a><ul>
<li class="chapter" data-level="3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.1</b> Decision trees</a><ul>
<li class="chapter" data-level="3.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#advancedsplit"><i class="fa fa-check"></i><b>3.1.1</b> Advanced: how do trees choose where to split?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging"><i class="fa fa-check"></i><b>3.2</b> Bagging</a></li>
<li class="chapter" data-level="3.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests"><i class="fa fa-check"></i><b>3.3</b> Random Forests</a></li>
<li class="chapter" data-level="3.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting"><i class="fa fa-check"></i><b>3.4</b> Boosting</a></li>
<li class="chapter" data-level="3.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercises-1"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="loss-functions.html"><a href="loss-functions.html"><i class="fa fa-check"></i><b>4</b> Loss functions</a><ul>
<li class="chapter" data-level="4.1" data-path="loss-functions.html"><a href="loss-functions.html#continuous-loss-functions"><i class="fa fa-check"></i><b>4.1</b> Continuous loss functions</a><ul>
<li class="chapter" data-level="4.1.1" data-path="loss-functions.html"><a href="loss-functions.html#root-mean-square-error-rmse"><i class="fa fa-check"></i><b>4.1.1</b> Root Mean Square Error (RMSE)</a></li>
<li class="chapter" data-level="4.1.2" data-path="loss-functions.html"><a href="loss-functions.html#mean-absolute-error"><i class="fa fa-check"></i><b>4.1.2</b> Mean Absolute Error</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="loss-functions.html"><a href="loss-functions.html#binary-loss-functions"><i class="fa fa-check"></i><b>4.2</b> Binary loss functions</a><ul>
<li class="chapter" data-level="4.2.1" data-path="loss-functions.html"><a href="loss-functions.html#confusion-matrices"><i class="fa fa-check"></i><b>4.2.1</b> Confusion Matrices</a></li>
<li class="chapter" data-level="4.2.2" data-path="loss-functions.html"><a href="loss-functions.html#roc-curves-and-area-under-the-curve"><i class="fa fa-check"></i><b>4.2.2</b> ROC Curves and Area Under the Curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html"><i class="fa fa-check"></i><b>5</b> Unsupervised methods</a><ul>
<li class="chapter" data-level="5.1" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>5.1</b> Principal Component Analysis (PCA)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="syllabus.html"><a href="syllabus.html"><i class="fa fa-check"></i><b>6</b> Syllabus</a><ul>
<li class="chapter" data-level="6.1" data-path="syllabus.html"><a href="syllabus.html#course-description"><i class="fa fa-check"></i><b>6.1</b> Course description</a></li>
<li class="chapter" data-level="6.2" data-path="syllabus.html"><a href="syllabus.html#schedule"><i class="fa fa-check"></i><b>6.2</b> Schedule</a></li>
<li class="chapter" data-level="6.3" data-path="syllabus.html"><a href="syllabus.html#software"><i class="fa fa-check"></i><b>6.3</b> Software</a></li>
<li class="chapter" data-level="6.4" data-path="syllabus.html"><a href="syllabus.html#prerequisites"><i class="fa fa-check"></i><b>6.4</b> Prerequisites</a></li>
<li class="chapter" data-level="6.5" data-path="syllabus.html"><a href="syllabus.html#about-the-author"><i class="fa fa-check"></i><b>6.5</b> About the author</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Social Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="unsupervised-methods" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Unsupervised methods</h1>
<p>Unsupervised learning is a very popular concept in machine learning. Although we social scientists are aware of some of them, we do not take advantage of them as much as machine learning practitioners. What is unsupervised learning? Let’s get that out of the way: it simply means that it <strong>does not have a dependent variable</strong>. These are models that look to find relationships between independent variables.</p>
<p>One common unsupervised method that social scientists are aware of are <strong>P</strong>rincipal <strong>C</strong>omponent <strong>A</strong>nalysis or PCA. PCA aims to summarize many variables into a small subset of variables that can capture the greatest variance out of all the main variables. We really never thought about this as a ‘unsupervised’ method, but it is used widely for predictive tasks. Before we begin, let’s load the packages and data we’ll be using.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="unsupervised-methods.html#cb131-1"></a><span class="kw">library</span>(tidymodels)</span>
<span id="cb131-2"><a href="unsupervised-methods.html#cb131-2"></a><span class="kw">library</span>(tidyflow)</span>
<span id="cb131-3"><a href="unsupervised-methods.html#cb131-3"></a><span class="kw">library</span>(ggfortify)</span>
<span id="cb131-4"><a href="unsupervised-methods.html#cb131-4"></a></span>
<span id="cb131-5"><a href="unsupervised-methods.html#cb131-5"></a>data_link &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot;</span></span>
<span id="cb131-6"><a href="unsupervised-methods.html#cb131-6"></a>pisa &lt;-<span class="st"> </span><span class="kw">read.csv</span>(data_link)</span></code></pre></div>
<div id="principal-component-analysis-pca" class="section level2">
<h2><span class="header-section-number">5.1</span> Principal Component Analysis (PCA)</h2>
<p><strong>P</strong>rincipal <strong>C</strong>omponent <strong>A</strong>nalysis or PCA is a method that tries to summarize many columns into a very small subset that captures the greatest variability of the original columns. Social Scientists often use this method to create more ‘parsimonious’ models and summarize many variables into a few ‘strong’ variables.</p>
<p>PCA works by creating several components which are the normalized linear combination of the variables in the model. In the <code>pisa</code> data there are a six variables which asks the student whether they’ve suffered negative behavior from their friends in the past 12 months. In particular, it asks whether</p>
<ul>
<li>Other students left them out of things on purpose</li>
<li>Other students made fun of them</li>
<li>They were threatened by other students</li>
<li>Other students took away or destroyed things that belonged to them</li>
<li>They got hit or pushed around by other students</li>
<li>Other students spread nasty rumours about them</li>
</ul>
<p>Let’s rename these variables into more interpretable names and look at their correlation:</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="unsupervised-methods.html#cb132-1"></a>pisa_selected &lt;-</span>
<span id="cb132-2"><a href="unsupervised-methods.html#cb132-2"></a><span class="st">  </span>pisa <span class="op">%&gt;%</span></span>
<span id="cb132-3"><a href="unsupervised-methods.html#cb132-3"></a><span class="st">  </span><span class="kw">rename</span>(</span>
<span id="cb132-4"><a href="unsupervised-methods.html#cb132-4"></a>    <span class="dt">past12_left_out =</span> ST038Q03NA,</span>
<span id="cb132-5"><a href="unsupervised-methods.html#cb132-5"></a>    <span class="dt">past12_madefun_of_me =</span> ST038Q04NA,</span>
<span id="cb132-6"><a href="unsupervised-methods.html#cb132-6"></a>    <span class="dt">past12_threatened =</span> ST038Q05NA,</span>
<span id="cb132-7"><a href="unsupervised-methods.html#cb132-7"></a>    <span class="dt">past12_destroyed_personal =</span> ST038Q06NA,</span>
<span id="cb132-8"><a href="unsupervised-methods.html#cb132-8"></a>    <span class="dt">past12_got_hit =</span> ST038Q07NA,</span>
<span id="cb132-9"><a href="unsupervised-methods.html#cb132-9"></a>    <span class="dt">past12_spread_rumours =</span> ST038Q08NA</span>
<span id="cb132-10"><a href="unsupervised-methods.html#cb132-10"></a>  ) <span class="op">%&gt;%</span></span>
<span id="cb132-11"><a href="unsupervised-methods.html#cb132-11"></a><span class="st">  </span><span class="kw">select</span>(<span class="kw">starts_with</span>(<span class="st">&quot;past12&quot;</span>))</span>
<span id="cb132-12"><a href="unsupervised-methods.html#cb132-12"></a></span>
<span id="cb132-13"><a href="unsupervised-methods.html#cb132-13"></a><span class="kw">cor</span>(pisa_selected)</span></code></pre></div>
<pre><code>##                           past12_left_out past12_madefun_of_me
## past12_left_out                 1.0000000            0.6073982
## past12_madefun_of_me            0.6073982            1.0000000
## past12_threatened               0.4454125            0.4712083
## past12_destroyed_personal       0.4037351            0.4165931
## past12_got_hit                  0.3918129            0.4480862
## past12_spread_rumours           0.4746302            0.5069299
##                           past12_threatened past12_destroyed_personal
## past12_left_out                   0.4454125                 0.4037351
## past12_madefun_of_me              0.4712083                 0.4165931
## past12_threatened                 1.0000000                 0.5685773
## past12_destroyed_personal         0.5685773                 1.0000000
## past12_got_hit                    0.5807617                 0.6206485
## past12_spread_rumours             0.5513099                 0.4543380
##                           past12_got_hit past12_spread_rumours
## past12_left_out                0.3918129             0.4746302
## past12_madefun_of_me           0.4480862             0.5069299
## past12_threatened              0.5807617             0.5513099
## past12_destroyed_personal      0.6206485             0.4543380
## past12_got_hit                 1.0000000             0.4451408
## past12_spread_rumours          0.4451408             1.0000000</code></pre>
<p>Most correlations lie between <code>0.4</code> and <code>0.6</code>, a somewhat acceptable threshold for assesing whether they can be reduced into fewer variables. PCA works by receiving as input <span class="math inline">\(P\)</span> variables (in this case six) and calculating the normalized linear combination of the <span class="math inline">\(P\)</span> variables. This new variable is the combination of the <span class="math inline">\(P\)</span> that captures the greatest variance out of the <span class="math inline">\(P\)</span> variables. PCA continuous to calculate other normalized linear combinations <strong>but</strong> with the constraint that they need to be completely uncorrelated to all the other normalized linear combinations.</p>
<p>This approach has the advantage that it constructs as many principal components (new variables) as it can. Each variable is assessed by how much variance it explains of the original <span class="math inline">\(P\)</span> variables and each new variable is completely independent of the other. Depending on the correlation of the <span class="math inline">\(P\)</span> input variables, you might get three principal components that capture all of the variability in the <span class="math inline">\(P\)</span> variables. In other cases, you can get many variables (more than 20).</p>
<p>This discussion is getting too theoretical. Let’s get get some hands-on experience. Let’s pass in our six variables to the function <code>prcomp</code>, which estimates these principal components based on our six variables:</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="unsupervised-methods.html#cb134-1"></a>pc &lt;-<span class="st"> </span><span class="kw">prcomp</span>(pisa_selected)</span>
<span id="cb134-2"><a href="unsupervised-methods.html#cb134-2"></a>all_pcs &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">predict</span>(pc, <span class="dt">newdata =</span> pisa_selected))</span>
<span id="cb134-3"><a href="unsupervised-methods.html#cb134-3"></a><span class="kw">head</span>(all_pcs)</span></code></pre></div>
<pre><code>##            PC1        PC2         PC3          PC4          PC5          PC6
## 1 -2.836172297 -0.7549602 -1.91065434 -0.232647114 -0.368981283 -1.885607656
## 2 -1.478020766  0.6622561  0.94113153  0.181451711  0.149387648  0.678384471
## 3  1.025953306  0.1602906 -0.03806864 -0.008994148  0.009439987 -0.002391996
## 4 -0.002173173 -0.7902197 -0.10112894 -0.197389118  0.013521080 -0.002718289
## 5 -4.832075955  0.1996595  0.39221922 -0.256660522 -1.178883084  0.150399629
## 6 -1.132036976 -1.8534154 -0.68913950  0.914561923  0.065907346  0.087208533</code></pre>
<p>Let’s explain what just happened. Our dataset <code>pisa_selected</code> contains the six variables of interest. We passed that to <code>prcomp</code> which calculated the principal components. With this model object, we used this model object to <code>predict</code> the actual columns using our initial <span class="math inline">\(P\)</span> variables in <code>pisa_selected</code>. The result of all of this is a dataframe with sex new columns. These six new columns are <strong>not</strong> the initial six variables from <code>pisa_selected</code>. Instead, they are variables that summarize the relationship of these six variables. You might ask yourself, how come six variables <strong>summarize</strong> six variables? Let’s look at how much variance of the original <span class="math inline">\(P\)</span> variables these ‘index’ variables explain:</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="unsupervised-methods.html#cb136-1"></a><span class="kw">tidy</span>(pc, <span class="st">&quot;pcs&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 4
##      PC std.dev percent cumulative
##   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;
## 1     1   1.34   0.591       0.591
## 2     2   0.640  0.135       0.726
## 3     3   0.530  0.0929      0.819
## 4     4   0.522  0.0899      0.909
## 5     5   0.394  0.0513      0.960
## 6     6   0.347  0.0397      1</code></pre>
<p>This output show how well each principal component is explaining the original six variables. For example, the first principal component (1st row) explains about 59% of the variance of the six variables. The second principal component explains 13.5%, for a total of 72.6% between the two.
Let’s focus on these two principal components. They are supposed to be completely uncorrelated, so let’s check that ourselves:</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="unsupervised-methods.html#cb138-1"></a><span class="kw">cor</span>(all_pcs[<span class="kw">c</span>(<span class="st">&quot;PC1&quot;</span>, <span class="st">&quot;PC2&quot;</span>)])</span></code></pre></div>
<pre><code>##                         PC1                     PC2
## PC1  1.00000000000000000000 -0.00000000000001545012
## PC2 -0.00000000000001545012  1.00000000000000000000</code></pre>
<p>As expected, the correlation between these two variables are 0. How do we use these two variables? Well, a typical social scientist would make sure that their expected explanatory power of the two components is high enough for their research problem. If it is, they would include these two columns in their modelling instead of the six variables. However, PCA is all about exploratory data analysis. We might want to go further. These two principal components are a bit of a black box at this point. Which variables do they represent? We can check that with the initial output of <code>prcomp</code>:</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="unsupervised-methods.html#cb140-1"></a>pc<span class="op">$</span>rotation[, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]</span></code></pre></div>
<pre><code>##                                  PC1        PC2
## past12_left_out           -0.4631946 -0.4189125
## past12_madefun_of_me      -0.5649319 -0.5315979
## past12_threatened         -0.3446963  0.4025682
## past12_destroyed_personal -0.2694606  0.3405411
## past12_got_hit            -0.2987481  0.3715999
## past12_spread_rumours     -0.4308453  0.3546832</code></pre>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="unsupervised-methods.html#cb142-1"></a>pc <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb142-2"><a href="unsupervised-methods.html#cb142-2"></a><span class="st">  </span><span class="kw">autoplot</span>(<span class="dt">loadings =</span> <span class="ot">TRUE</span>,</span>
<span id="cb142-3"><a href="unsupervised-methods.html#cb142-3"></a>           <span class="dt">loadings.label =</span> <span class="ot">TRUE</span>,</span>
<span id="cb142-4"><a href="unsupervised-methods.html#cb142-4"></a>           <span class="dt">loadings.label.repel =</span> <span class="ot">TRUE</span>,</span>
<span id="cb142-5"><a href="unsupervised-methods.html#cb142-5"></a>           <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">6</span>) <span class="op">+</span></span>
<span id="cb142-6"><a href="unsupervised-methods.html#cb142-6"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span>
<span id="cb142-7"><a href="unsupervised-methods.html#cb142-7"></a></span>
<span id="cb142-8"><a href="unsupervised-methods.html#cb142-8"></a>res &lt;-</span>
<span id="cb142-9"><a href="unsupervised-methods.html#cb142-9"></a><span class="st">  </span>pisa <span class="op">%&gt;%</span></span>
<span id="cb142-10"><a href="unsupervised-methods.html#cb142-10"></a><span class="st">  </span><span class="kw">recipe</span>(math_score <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span></span>
<span id="cb142-11"><a href="unsupervised-methods.html#cb142-11"></a><span class="st">  </span><span class="kw">step_center</span>(<span class="kw">starts_with</span>(<span class="st">&quot;ST038&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb142-12"><a href="unsupervised-methods.html#cb142-12"></a><span class="st">  </span><span class="kw">step_scale</span>(<span class="kw">starts_with</span>(<span class="st">&quot;ST038&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb142-13"><a href="unsupervised-methods.html#cb142-13"></a><span class="st">  </span><span class="kw">step_pca</span>(<span class="kw">starts_with</span>(<span class="st">&quot;ST038&quot;</span>), <span class="dt">num_comp =</span> <span class="dv">2</span>, <span class="dt">res =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb142-14"><a href="unsupervised-methods.html#cb142-14"></a><span class="st">  </span><span class="kw">prep</span>()</span>
<span id="cb142-15"><a href="unsupervised-methods.html#cb142-15"></a></span>
<span id="cb142-16"><a href="unsupervised-methods.html#cb142-16"></a>res <span class="op">%&gt;%</span></span>
<span id="cb142-17"><a href="unsupervised-methods.html#cb142-17"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(PC1, PC2)) <span class="op">+</span></span>
<span id="cb142-18"><a href="unsupervised-methods.html#cb142-18"></a><span class="st">  </span><span class="kw">geom_point</span>()</span></code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="loss-functions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="syllabus.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
