<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Loss functions | Machine Learning for Social Scientists</title>
  <meta name="description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Loss functions | Machine Learning for Social Scientists" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://cimentadaj.github.io/ml_socsci/" />
  
  <meta property="og:description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  <meta name="github-repo" content="cimentadaj/ml_socsci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Loss functions | Machine Learning for Social Scientists" />
  
  <meta name="twitter:description" content="Notes, content and exercises for the RECSM 2020 course Machine Learning for Social Scientists." />
  

<meta name="author" content="Jorge Cimentada" />


<meta name="date" content="2020-07-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="tree-based-methods.html"/>
<link rel="next" href="unsupervised-methods.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.13/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning for Social Scientists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html"><i class="fa fa-check"></i><b>1</b> Machine Learning for Social Scientists</a><ul>
<li class="chapter" data-level="1.1" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#a-different-way-of-thinking"><i class="fa fa-check"></i><b>1.1</b> A different way of thinking</a></li>
<li class="chapter" data-level="1.2" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#split-your-data-into-trainingtesting"><i class="fa fa-check"></i><b>1.2</b> Split your data into training/testing</a></li>
<li class="chapter" data-level="1.3" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#cross-validation"><i class="fa fa-check"></i><b>1.3</b> Cross-validation</a></li>
<li class="chapter" data-level="1.4" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>1.4</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="1.5" data-path="machine-learning-for-social-scientists.html"><a href="machine-learning-for-social-scientists.html#an-example"><i class="fa fa-check"></i><b>1.5</b> An example</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>2</b> Regularization</a><ul>
<li class="chapter" data-level="2.1" data-path="regularization.html"><a href="regularization.html#ridge-regularization"><i class="fa fa-check"></i><b>2.1</b> Ridge regularization</a></li>
<li class="chapter" data-level="2.2" data-path="regularization.html"><a href="regularization.html#lasso-regularization"><i class="fa fa-check"></i><b>2.2</b> Lasso regularization</a></li>
<li class="chapter" data-level="2.3" data-path="regularization.html"><a href="regularization.html#elastic-net-regularization"><i class="fa fa-check"></i><b>2.3</b> Elastic Net regularization</a></li>
<li class="chapter" data-level="2.4" data-path="regularization.html"><a href="regularization.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>3</b> Tree-based methods</a><ul>
<li class="chapter" data-level="3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.1</b> Decision trees</a><ul>
<li class="chapter" data-level="3.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#advancedsplit"><i class="fa fa-check"></i><b>3.1.1</b> Advanced: how do trees choose where to split?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging"><i class="fa fa-check"></i><b>3.2</b> Bagging</a></li>
<li class="chapter" data-level="3.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests"><i class="fa fa-check"></i><b>3.3</b> Random Forests</a></li>
<li class="chapter" data-level="3.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting"><i class="fa fa-check"></i><b>3.4</b> Boosting</a></li>
<li class="chapter" data-level="3.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercises-1"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="loss-functions.html"><a href="loss-functions.html"><i class="fa fa-check"></i><b>4</b> Loss functions</a><ul>
<li class="chapter" data-level="4.1" data-path="loss-functions.html"><a href="loss-functions.html#continuous-loss-functions"><i class="fa fa-check"></i><b>4.1</b> Continuous loss functions</a><ul>
<li class="chapter" data-level="4.1.1" data-path="loss-functions.html"><a href="loss-functions.html#root-mean-square-error-rmse"><i class="fa fa-check"></i><b>4.1.1</b> Root Mean Square Error (RMSE)</a></li>
<li class="chapter" data-level="4.1.2" data-path="loss-functions.html"><a href="loss-functions.html#mean-absolute-error"><i class="fa fa-check"></i><b>4.1.2</b> Mean Absolute Error</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="loss-functions.html"><a href="loss-functions.html#binary-loss-functions"><i class="fa fa-check"></i><b>4.2</b> Binary loss functions</a><ul>
<li class="chapter" data-level="4.2.1" data-path="loss-functions.html"><a href="loss-functions.html#confusion-matrices"><i class="fa fa-check"></i><b>4.2.1</b> Confusion Matrices</a></li>
<li class="chapter" data-level="4.2.2" data-path="loss-functions.html"><a href="loss-functions.html#roc-curves-and-area-under-the-curve"><i class="fa fa-check"></i><b>4.2.2</b> ROC Curves and Area Under the Curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html"><i class="fa fa-check"></i><b>5</b> Unsupervised methods</a><ul>
<li class="chapter" data-level="5.1" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>5.1</b> Principal Component Analysis (PCA)</a></li>
<li class="chapter" data-level="5.2" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html#k-means-clustering"><i class="fa fa-check"></i><b>5.2</b> K-Means Clustering</a></li>
<li class="chapter" data-level="5.3" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html#exercises-2"><i class="fa fa-check"></i><b>5.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="no-free-lunch.html"><a href="no-free-lunch.html"><i class="fa fa-check"></i><b>6</b> No free lunch</a><ul>
<li class="chapter" data-level="6.1" data-path="no-free-lunch.html"><a href="no-free-lunch.html#causal-inference"><i class="fa fa-check"></i><b>6.1</b> Causal Inference</a></li>
<li class="chapter" data-level="6.2" data-path="no-free-lunch.html"><a href="no-free-lunch.html#explaining-complex-models"><i class="fa fa-check"></i><b>6.2</b> Explaining complex models</a></li>
<li class="chapter" data-level="6.3" data-path="no-free-lunch.html"><a href="no-free-lunch.html#inference"><i class="fa fa-check"></i><b>6.3</b> Inference</a></li>
<li class="chapter" data-level="6.4" data-path="no-free-lunch.html"><a href="no-free-lunch.html#prediction"><i class="fa fa-check"></i><b>6.4</b> Prediction</a></li>
<li class="chapter" data-level="6.5" data-path="no-free-lunch.html"><a href="no-free-lunch.html#prediction-challenge"><i class="fa fa-check"></i><b>6.5</b> Prediction challenge</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="syllabus.html"><a href="syllabus.html"><i class="fa fa-check"></i><b>7</b> Syllabus</a><ul>
<li class="chapter" data-level="7.1" data-path="syllabus.html"><a href="syllabus.html#course-description"><i class="fa fa-check"></i><b>7.1</b> Course description</a></li>
<li class="chapter" data-level="7.2" data-path="syllabus.html"><a href="syllabus.html#schedule"><i class="fa fa-check"></i><b>7.2</b> Schedule</a></li>
<li class="chapter" data-level="7.3" data-path="syllabus.html"><a href="syllabus.html#software"><i class="fa fa-check"></i><b>7.3</b> Software</a></li>
<li class="chapter" data-level="7.4" data-path="syllabus.html"><a href="syllabus.html#prerequisites"><i class="fa fa-check"></i><b>7.4</b> Prerequisites</a></li>
<li class="chapter" data-level="7.5" data-path="syllabus.html"><a href="syllabus.html#about-the-author"><i class="fa fa-check"></i><b>7.5</b> About the author</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Social Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="loss-functions" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Loss functions</h1>
<p>To evaluate a model’s fit, social scientists use metrics such as the <span class="math inline">\(R^2\)</span>, <span class="math inline">\(AIC\)</span>, <span class="math inline">\(Log\text{ }likelihood\)</span> or <span class="math inline">\(BIC\)</span>. We almost always use these metrics and their purpose is to inform some of our modeling choices. However, they are rarely determinant for us in publishing results, excluding variables or making important modeling decisions. Machine Learning practitioners use these exact metrics but they are at the core of their modeling workflow.</p>
<p>In machine learning, metrics such as the <span class="math inline">\(R^2\)</span> and the <span class="math inline">\(AIC\)</span> are called ‘loss functions’. Remember that, because you’ll see that term popping in everywhere in machine learning. Despite this fancy name, loss functions evaluate the same thing we social scientists evaluate: a model’s fit.</p>
<p>In this chapter we won’t focus on loss functions that social scientists are used to such as <span class="math inline">\(R^2\)</span>, <span class="math inline">\(BIC\)</span> and <span class="math inline">\(AIC\)</span>. Instead, we’ll discuss loss functions which are targeted towards prediction. Let’s load the packages we’ll use in the chapter before we begin:</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="loss-functions.html#cb115-1"></a><span class="kw">library</span>(tidymodels)</span>
<span id="cb115-2"><a href="loss-functions.html#cb115-2"></a><span class="kw">library</span>(tidyflow)</span>
<span id="cb115-3"><a href="loss-functions.html#cb115-3"></a><span class="kw">library</span>(plotly)</span>
<span id="cb115-4"><a href="loss-functions.html#cb115-4"></a><span class="kw">library</span>(ggplot2)</span>
<span id="cb115-5"><a href="loss-functions.html#cb115-5"></a></span>
<span id="cb115-6"><a href="loss-functions.html#cb115-6"></a>data_link &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/cimentadaj/ml_socsci/master/data/pisa_us_2018.csv&quot;</span></span>
<span id="cb115-7"><a href="loss-functions.html#cb115-7"></a>pisa &lt;-<span class="st"> </span><span class="kw">read.csv</span>(data_link)</span></code></pre></div>
<div id="continuous-loss-functions" class="section level2">
<h2><span class="header-section-number">4.1</span> Continuous loss functions</h2>
<p>Continuous variables require completely different loss functions than binary variables. In this section we’ll discuss the <span class="math inline">\(RMSE\)</span> and the <span class="math inline">\(MAE\)</span>.</p>
<div id="root-mean-square-error-rmse" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Root Mean Square Error (RMSE)</h3>
<p>The most common loss function used for continuous variables is the <strong>R</strong>oot <strong>M</strong>ean <strong>S</strong>quared <strong>E</strong>rror, or <span class="math inline">\(RMSE\)</span>. The <span class="math inline">\(RMSE\)</span> works by evaluating how far our predictions are from the actual data. More concretely, suppose we have a linear relationship between <code>X</code> and <code>Y</code>:</p>
<p><img src="04_lossfuns_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>We fit a model and plot the predicted values:</p>
<p><img src="04_lossfuns_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>How can we evaluate whether this line is accurate? The most common approach is to subtract the actual <span class="math inline">\(Y_{i}\)</span> score of each respondent from the predicted <span class="math inline">\(\hat{Y_{i}}\)</span> for each respondent:</p>
<p><img src="04_lossfuns_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>After the subtraction occurs, we square these differences (because squaring removes the negative and positive sign of subtracting over and under predicted values), calculate the average error and take the square root of that. Social scientists are aware of this. Here’s the formula, in case this sounds familiar:</p>
<p><span class="math display">\[ RMSE = \sqrt{\sum_{i = 1}^n{\frac{(\hat{y_{i}} - y_{i})^2}{N}}} \]</span></p>
<p>In machine learning, this metric is used to identify models which have the lowest predictive error. For example, let’s fit two linear models using the <code>pisa</code> data set. One will regress <code>math_score</code> (mathematics test score) on <code>scie_score</code> and <code>read_score</code> (they each measure test scores in science and literacy respectively) and the other will regress <code>math_score</code> on all variables excluding science and literacy scores.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="loss-functions.html#cb116-1"></a><span class="co"># Model with only scie_score and read_score</span></span>
<span id="cb116-2"><a href="loss-functions.html#cb116-2"></a>mod1 &lt;-</span>
<span id="cb116-3"><a href="loss-functions.html#cb116-3"></a><span class="st">  </span>pisa <span class="op">%&gt;%</span></span>
<span id="cb116-4"><a href="loss-functions.html#cb116-4"></a><span class="st">  </span><span class="kw">tidyflow</span>(<span class="dt">seed =</span> <span class="dv">4113</span>) <span class="op">%&gt;%</span></span>
<span id="cb116-5"><a href="loss-functions.html#cb116-5"></a><span class="st">  </span><span class="kw">plug_formula</span>(math_score <span class="op">~</span><span class="st"> </span>scie_score <span class="op">+</span><span class="st"> </span>read_score) <span class="op">%&gt;%</span></span>
<span id="cb116-6"><a href="loss-functions.html#cb116-6"></a><span class="st">  </span><span class="kw">plug_split</span>(initial_split) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb116-7"><a href="loss-functions.html#cb116-7"></a><span class="st">  </span><span class="kw">plug_model</span>(<span class="kw">set_engine</span>(<span class="kw">linear_reg</span>(), <span class="st">&quot;lm&quot;</span>))</span>
<span id="cb116-8"><a href="loss-functions.html#cb116-8"></a></span>
<span id="cb116-9"><a href="loss-functions.html#cb116-9"></a><span class="co"># Model with all variables except scie_score and read_score</span></span>
<span id="cb116-10"><a href="loss-functions.html#cb116-10"></a>mod2 &lt;-</span>
<span id="cb116-11"><a href="loss-functions.html#cb116-11"></a><span class="st">  </span>mod1 <span class="op">%&gt;%</span></span>
<span id="cb116-12"><a href="loss-functions.html#cb116-12"></a><span class="st">  </span><span class="kw">replace_formula</span>(math_score <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>scie_score <span class="op">-</span><span class="st"> </span>read_score)</span>
<span id="cb116-13"><a href="loss-functions.html#cb116-13"></a></span>
<span id="cb116-14"><a href="loss-functions.html#cb116-14"></a><span class="co"># Fit the models</span></span>
<span id="cb116-15"><a href="loss-functions.html#cb116-15"></a>res1 &lt;-<span class="st"> </span><span class="kw">fit</span>(mod1)</span>
<span id="cb116-16"><a href="loss-functions.html#cb116-16"></a>res2 &lt;-<span class="st"> </span><span class="kw">fit</span>(mod2)</span>
<span id="cb116-17"><a href="loss-functions.html#cb116-17"></a></span>
<span id="cb116-18"><a href="loss-functions.html#cb116-18"></a><span class="co"># Calculate the RMSE of model 1</span></span>
<span id="cb116-19"><a href="loss-functions.html#cb116-19"></a>rmse_<span class="dv">1</span> &lt;-</span>
<span id="cb116-20"><a href="loss-functions.html#cb116-20"></a><span class="st">  </span>res1 <span class="op">%&gt;%</span></span>
<span id="cb116-21"><a href="loss-functions.html#cb116-21"></a><span class="st">  </span><span class="kw">predict_training</span>() <span class="op">%&gt;%</span></span>
<span id="cb116-22"><a href="loss-functions.html#cb116-22"></a><span class="st">  </span><span class="kw">rmse</span>(math_score, .pred) <span class="op">%&gt;%</span></span>
<span id="cb116-23"><a href="loss-functions.html#cb116-23"></a><span class="st">  </span><span class="kw">pull</span>(.estimate)</span>
<span id="cb116-24"><a href="loss-functions.html#cb116-24"></a></span>
<span id="cb116-25"><a href="loss-functions.html#cb116-25"></a><span class="co"># Calculate the RMSE of model 2</span></span>
<span id="cb116-26"><a href="loss-functions.html#cb116-26"></a>rmse_<span class="dv">2</span> &lt;-</span>
<span id="cb116-27"><a href="loss-functions.html#cb116-27"></a><span class="st">  </span>res2 <span class="op">%&gt;%</span></span>
<span id="cb116-28"><a href="loss-functions.html#cb116-28"></a><span class="st">  </span><span class="kw">predict_training</span>() <span class="op">%&gt;%</span></span>
<span id="cb116-29"><a href="loss-functions.html#cb116-29"></a><span class="st">  </span><span class="kw">rmse</span>(math_score, .pred) <span class="op">%&gt;%</span></span>
<span id="cb116-30"><a href="loss-functions.html#cb116-30"></a><span class="st">  </span><span class="kw">pull</span>(.estimate)</span>
<span id="cb116-31"><a href="loss-functions.html#cb116-31"></a></span>
<span id="cb116-32"><a href="loss-functions.html#cb116-32"></a><span class="kw">c</span>(<span class="st">&quot;RMSE first model&quot;</span> =<span class="st"> </span>rmse_<span class="dv">1</span>, <span class="st">&quot;RMSE second model&quot;</span> =<span class="st"> </span>rmse_<span class="dv">2</span>)</span></code></pre></div>
<pre><code>##  RMSE first model RMSE second model 
##          30.73546          40.72052</code></pre>
<p>The first model has the lowest error. The <span class="math inline">\(RMSE\)</span> can be interpreted as the average error in the metric of the dependent variable. For example, the first <span class="math inline">\(RMSE\)</span> can be interpreted as having an average error of 30.74 points in mathematics.</p>
<p>The <span class="math inline">\(RMSE\)</span> has the benefit of penalizing large errors more than small errors. In other words, taking the square root of a big number penalizes this large number more than taking the same square root of a small number. Let’s show an example.</p>
<p>The result of <code>sqrt(1000)</code> is 31.62 whereas the result for <code>sqrt(100)</code> is 10. The difference in the decrease is not proportional to the difference between <code>1000</code> and <code>100</code>. The difference between <code>1000</code> and <code>100</code> is much larger than the difference between 31.62 and 10 because <code>sqrt</code> penalizes more the number <code>1000</code>. This property makes the <span class="math inline">\(RMSE\)</span> attractive to work with when you don’t care about the proportion of the errors.</p>
<p>In the case of our <code>math_score</code> variable, this is not desirable since having a <span class="math inline">\(RMSE\)</span> of 30 is twice as worse as having a <span class="math inline">\(RMSE\)</span> of 15. It doesn’t make sense to penalize large errors more in test scores. Whenever the scale of the dependent variable does reflects this, it might be worth exploring other loss functions, such as the <span class="math inline">\(MAE\)</span>.</p>
</div>
<div id="mean-absolute-error" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Mean Absolute Error</h3>
<p>The <strong>M</strong>ean <strong>A</strong>bsolute <strong>E</strong>rror or <span class="math inline">\(MAE\)</span> is very similar to the <span class="math inline">\(RMSE\)</span>. It calculates the average error in the same metric as the dependent variable. However, instead of squaring the difference and taking the square root, it takes the absolute value of the difference:</p>
<p><span class="math display">\[ MAE = \sum_{i = 1}^n{\frac{|\hat{y_{i}} - y_{i}|}{N}} \]</span></p>
<p>This approach doesn’t penalize any values and just takes the absolute error of the predictions. Let’s calculate the <span class="math inline">\(MAE\)</span> for our previous two models:</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="loss-functions.html#cb118-1"></a>mae1 &lt;-</span>
<span id="cb118-2"><a href="loss-functions.html#cb118-2"></a><span class="st">  </span>res1 <span class="op">%&gt;%</span></span>
<span id="cb118-3"><a href="loss-functions.html#cb118-3"></a><span class="st">  </span><span class="kw">predict_training</span>() <span class="op">%&gt;%</span></span>
<span id="cb118-4"><a href="loss-functions.html#cb118-4"></a><span class="st">  </span><span class="kw">mae</span>(math_score, .pred) <span class="op">%&gt;%</span></span>
<span id="cb118-5"><a href="loss-functions.html#cb118-5"></a><span class="st">  </span><span class="kw">pull</span>(.estimate)</span>
<span id="cb118-6"><a href="loss-functions.html#cb118-6"></a></span>
<span id="cb118-7"><a href="loss-functions.html#cb118-7"></a>mae2 &lt;-</span>
<span id="cb118-8"><a href="loss-functions.html#cb118-8"></a><span class="st">  </span>res2 <span class="op">%&gt;%</span></span>
<span id="cb118-9"><a href="loss-functions.html#cb118-9"></a><span class="st">  </span><span class="kw">predict_training</span>() <span class="op">%&gt;%</span></span>
<span id="cb118-10"><a href="loss-functions.html#cb118-10"></a><span class="st">  </span><span class="kw">mae</span>(math_score, .pred) <span class="op">%&gt;%</span></span>
<span id="cb118-11"><a href="loss-functions.html#cb118-11"></a><span class="st">  </span><span class="kw">pull</span>(.estimate)</span>
<span id="cb118-12"><a href="loss-functions.html#cb118-12"></a></span>
<span id="cb118-13"><a href="loss-functions.html#cb118-13"></a><span class="kw">c</span>(<span class="st">&quot;MAE first model&quot;</span> =<span class="st"> </span>mae1, <span class="st">&quot;MAE second model&quot;</span> =<span class="st"> </span>mae2)</span></code></pre></div>
<pre><code>##  MAE first model MAE second model 
##         24.42091         32.21999</code></pre>
<p>The <span class="math inline">\(MAE\)</span> is fundamentally simpler to interpret than the <span class="math inline">\(RMSE\)</span> since it’s just the average absolute error. The main difference with respect to the <span class="math inline">\(RMSE\)</span> is that it does not penalize larger errors but rather allows each error to have it’s absolute value.</p>
</div>
</div>
<div id="binary-loss-functions" class="section level2">
<h2><span class="header-section-number">4.2</span> Binary loss functions</h2>
<p>Binary loss functions are more elaborate than the ones discussed in the continuous loss function section. In particular, it introduces many terms which social scientists are rarely aware of. The pillar of these new terms is the confusion matrix, the topic we’ll be discussing next.</p>
<div id="confusion-matrices" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Confusion Matrices</h3>
<p>A confusion matrix is a very simple concept. It is the table comparing the predicted category of a model versus the actual category. Let’s work out an example.</p>
<p>The city of Berlin is working on developing an ‘early warning’ system that is aimed at predicting whether a family is in need of childcare support. They are trying to build this system on a dataset they’ve been collecting for the past 10 years where they have all families in the city. Families which received childcare support are flagged with a 1 and families which didn’t received childcare support are flagged with a 0. Below is a figure showing how this fake data would look like:</p>
<p><img src="img/base_df_lossfunction.svg" width="25%" style="display: block; margin: auto;" /></p>
<p>The column <code>DV</code> is whether a family has received childcare support and <code>X1</code> and <code>X2</code> are independent variables that are assumed to be correlated with receiving childcare support. Suppose we fit a logistic regression that returns a predicted probability for each family:</p>
<p><img src="img/df_lossfunction_prob.svg" width="45%" style="display: block; margin: auto;" /></p>
<p>For simplicity, we could assign a 1 to every respondent who has a probability above <code>0.5</code> and a 0 to every respondent with a probability below <code>0.5</code>:</p>
<p><img src="img/df_lossfunction_class.svg" width="60%" style="display: block; margin: auto;" /></p>
<p>Confusion matrices are two-way tables between the real category (this is the column <code>DV</code> in the data) and predicted category (this is the column <code>Pred class</code> in the data):</p>
<p><img src="img/confusion_matrix_template.svg" width="55%" style="display: block; margin: auto;" /></p>
<p>On the left axis, we have the <strong>predicted</strong> values of whether a family received childcare support (a 1 or a 0) and on the top axis of the table the <strong>true</strong> value of whether a family received childcare. This template is easy enough to calculate with our own data. Let’s do that using the labels of each cell (lable (A), (B), etc…):</p>
<p><img src="img/confusion_matrix_50.svg" width="55%" style="display: block; margin: auto;" /></p>
<p>Let’s focus on the top row. In cell (A), three respondents received childcare support that were predicted correctly by the logistic regression. These are rows 1, 5 and 7 from the data. Cell (A) corresponds to those that had a 1 and got predicted a 1. To the right side of this cell, cell (B) has those that didn’t receive childcare support but the model predicted they did. That is, they had a 0 in the dependent variable but the model predicted a 1. These were rows 2 and 6 from the data.</p>
<p>For the bottom row, cell (C) contains those which received childcare support but the model predicted they didn’t. This is only one respondent from our data, namely row 4. Finally, cell (D) contains those which didn’t receive childcare support and the model predicted accurately that they didn’t receive childcare support. This corresponds only to row 3.</p>
<p>These cells in the confusion matrix are used to generate several loss functions. The most standard is the <strong>accuracy</strong> of the model. The accuracy is the sum of all correctly predicted rows divided by the total number of predictions:</p>
<p><img src="img/confusion_matrix_50_accuracy.svg" width="55%" style="display: block; margin: auto;" /></p>
<p>In our example, the correct predictions are in cell (A) and cell (B). Cell (A) contains the number of families with childcare support that were assigned childcare support successfully. Cell (B) contains the number of families without childcare support that were correctly predicted to not have childcare support. We can calculate the accuracy as the sum of these two cells divided by all cells:</p>
<ul>
<li>Accuracy: <span class="math inline">\((3 + 1) / (3 + 1 + 2 + 2) = 50\%\)</span></li>
</ul>
<p>If you randomly picked a person from this dataset, you can be 50% confident that you can predict whether they received childcare support. This is not a good accuracy. By chance alone, we could also achieve a prediction accuracy of 50% if we made repeated predictions. That is why in certain scenarios, the <strong>sensitivity</strong> and <strong>specificity</strong> of a model is often a more important loss function.</p>
<p>The <strong>sensitivity</strong> of a model is a fancy name for the true positive rate. For the accuracy calculations we compared those that were correctly predicted (regardless of whether they received childcare support or whether they didn’t). Sensitivity measures the same thing but only focused on the true positive rate:</p>
<p><img src="img/confusion_matrix_50_sensitivity.svg" width="55%" style="display: block; margin: auto;" /></p>
<p>That is, we take those which were predicted childcare support correctly and divide them by the total number of respondents with childcare support. In substantive terms, it can be interpreted as: out of all the people who received childcare support, how many were classified correctly? With our small confusion matrix we can calculate it manually:</p>
<ul>
<li>Sensitivity: <span class="math inline">\(3 / (3 + 1) = 75\%\)</span></li>
</ul>
<p>This true positive rate of 75% is substantially higher than the 50% of the accuracy. However, we’re measuring different things. The sensitivity calculation completely ignores the accuracy of those which didn’t receive childcare support. For that, the opposite calculation is called specificity.</p>
<p>The <strong>specificity</strong> of a model measures the true false rate. That is, out of all the respondents who didn’t receive childcare, how many were classified correctly? Our confusion matrix highlights the two corresponding cells:</p>
<p><img src="img/confusion_matrix_50_specificity.svg" width="55%" style="display: block; margin: auto;" /></p>
<p>We know that the cell (D) contains those which were correctly assigned no childcare support and cell (B) contains those which were incorrectly assigned childcare support when in fact they didn’t have it. We can calculate specificity the same way we did sensitivity:</p>
<ul>
<li>Specificity: <span class="math inline">\(1 / (1 + 2) = 33\%\)</span></li>
</ul>
<p>The model has a 33% accuracy for those who didn’t receive childcare support. In other words, if you randomly picked someone who didn’t receive childcare support, you can be 33% confident that you can predict their childcare support status.</p>
<p>The model is biased towards predicting better childcare support and negatively biased towards those that didn’t receive childcare support. But this is och, as long as your research question is aligned with your loss function.</p>
<p>For example, it might be more costly in terms of time and money for the city of Berlin to fail to give childcare support to a family that needs it and thus we would prefer to improve the sensitivity of this model. Giving childcare support to a family that doesn’t need it is better than <strong>not</strong> giving childcare support to a family that needs it. This discussion is just to exemplify that having poor accuracy or poor measures of other types of loss functions depends on your research question rather than the actual value.</p>
<p>Although the previous exercises seemed easy enough, they are contingent on one assumption: that the cutoff value in the probabilities is <code>0.5</code>. Remember that we assumed that everyone who had a probability above 50% would be assigned a 1 and otherwise a 0. That’s a strong assumption. Perhaps the real cutoff should be <code>0.65</code>. Or it should be something more conservative, such as <code>0.40</code>. To address this problem, we have to look at the sensitivity and specificity for different thresholds. This is where the ROC curve comes in.</p>
</div>
<div id="roc-curves-and-area-under-the-curve" class="section level3">
<h3><span class="header-section-number">4.2.2</span> ROC Curves and Area Under the Curve</h3>
<p>The ROC curve (ROC means Receiver Operating Characteristic Curve) is just another fancy name for something that is just a representation of sensitivity and specificity. Let’s work out a manual example.</p>
<p>Suppose we have the same example of trying to predict whether a family will need childcare support. In that example, we calculated the sensitivity and specificity but assuming that the threshold for being 1 in the probability of each respondent is <code>0.5</code>.</p>
<p>We could assess how much the sensitivity and specificity of the model changes by changing this cutoff to 0.3 and 0.7:</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="loss-functions.html#cb120-1"></a><span class="co"># Create some very fake data for childcare_support</span></span>
<span id="cb120-2"><a href="loss-functions.html#cb120-2"></a><span class="co"># Do not interpret this as real!!</span></span>
<span id="cb120-3"><a href="loss-functions.html#cb120-3"></a>childcare_support &lt;-</span>
<span id="cb120-4"><a href="loss-functions.html#cb120-4"></a><span class="st">  </span>USArrests <span class="op">%&gt;%</span></span>
<span id="cb120-5"><a href="loss-functions.html#cb120-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">dv =</span> <span class="kw">factor</span>(<span class="kw">ifelse</span>(Rape <span class="op">&gt;=</span><span class="st"> </span><span class="kw">mean</span>(Rape), <span class="dv">1</span>, <span class="dv">0</span>))) <span class="op">%&gt;%</span></span>
<span id="cb120-6"><a href="loss-functions.html#cb120-6"></a><span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span></span>
<span id="cb120-7"><a href="loss-functions.html#cb120-7"></a><span class="st">  </span><span class="kw">select</span>(dv, <span class="kw">everything</span>(), <span class="op">-</span>Rape)</span>
<span id="cb120-8"><a href="loss-functions.html#cb120-8"></a><span class="kw">names</span>(childcare_support) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;dv&quot;</span>, <span class="kw">paste0</span>(<span class="st">&quot;X&quot;</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>))</span>
<span id="cb120-9"><a href="loss-functions.html#cb120-9"></a></span>
<span id="cb120-10"><a href="loss-functions.html#cb120-10"></a><span class="co"># Define the tidyflow with the logistic regression</span></span>
<span id="cb120-11"><a href="loss-functions.html#cb120-11"></a>tflow &lt;-</span>
<span id="cb120-12"><a href="loss-functions.html#cb120-12"></a><span class="st">  </span>childcare_support <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb120-13"><a href="loss-functions.html#cb120-13"></a><span class="st">  </span><span class="kw">tidyflow</span>(<span class="dt">seed =</span> <span class="dv">23151</span>) <span class="op">%&gt;%</span></span>
<span id="cb120-14"><a href="loss-functions.html#cb120-14"></a><span class="st">  </span><span class="kw">plug_split</span>(initial_split) <span class="op">%&gt;%</span></span>
<span id="cb120-15"><a href="loss-functions.html#cb120-15"></a><span class="st">  </span><span class="kw">plug_formula</span>(dv <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span></span>
<span id="cb120-16"><a href="loss-functions.html#cb120-16"></a><span class="st">  </span><span class="kw">plug_model</span>(<span class="kw">set_engine</span>(<span class="kw">logistic_reg</span>(), <span class="st">&quot;glm&quot;</span>))</span>
<span id="cb120-17"><a href="loss-functions.html#cb120-17"></a></span>
<span id="cb120-18"><a href="loss-functions.html#cb120-18"></a><span class="co"># Run the model</span></span>
<span id="cb120-19"><a href="loss-functions.html#cb120-19"></a>res &lt;-<span class="st"> </span>tflow <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>()</span>
<span id="cb120-20"><a href="loss-functions.html#cb120-20"></a></span>
<span id="cb120-21"><a href="loss-functions.html#cb120-21"></a><span class="co"># Get the probabilities</span></span>
<span id="cb120-22"><a href="loss-functions.html#cb120-22"></a>res1 &lt;-</span>
<span id="cb120-23"><a href="loss-functions.html#cb120-23"></a><span class="st">  </span>res <span class="op">%&gt;%</span></span>
<span id="cb120-24"><a href="loss-functions.html#cb120-24"></a><span class="st">  </span><span class="kw">predict_training</span>(<span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb120-25"><a href="loss-functions.html#cb120-25"></a></span>
<span id="cb120-26"><a href="loss-functions.html#cb120-26"></a><span class="co"># Calculate the sensitivity and specificity</span></span>
<span id="cb120-27"><a href="loss-functions.html#cb120-27"></a><span class="co"># of different thresholds</span></span>
<span id="cb120-28"><a href="loss-functions.html#cb120-28"></a>all_loss &lt;-</span>
<span id="cb120-29"><a href="loss-functions.html#cb120-29"></a><span class="st">  </span><span class="kw">lapply</span>(<span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>), <span class="cf">function</span>(x) {</span>
<span id="cb120-30"><a href="loss-functions.html#cb120-30"></a>    res &lt;-</span>
<span id="cb120-31"><a href="loss-functions.html#cb120-31"></a><span class="st">      </span>res1 <span class="op">%&gt;%</span></span>
<span id="cb120-32"><a href="loss-functions.html#cb120-32"></a><span class="st">      </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">factor</span>(<span class="kw">as.numeric</span>(.pred_<span class="dv">1</span> <span class="op">&gt;=</span><span class="st"> </span>x)))</span>
<span id="cb120-33"><a href="loss-functions.html#cb120-33"></a>    </span>
<span id="cb120-34"><a href="loss-functions.html#cb120-34"></a>    sens &lt;-<span class="st"> </span><span class="kw">sensitivity</span>(res, dv, pred)</span>
<span id="cb120-35"><a href="loss-functions.html#cb120-35"></a>    speci &lt;-<span class="st"> </span><span class="kw">specificity</span>(res, dv, pred)</span>
<span id="cb120-36"><a href="loss-functions.html#cb120-36"></a></span>
<span id="cb120-37"><a href="loss-functions.html#cb120-37"></a>    <span class="kw">data.frame</span>(<span class="dt">cutoff =</span> x,</span>
<span id="cb120-38"><a href="loss-functions.html#cb120-38"></a>               <span class="dt">sensitivity =</span> <span class="kw">round</span>(sens<span class="op">$</span>.estimate, <span class="dv">2</span>),</span>
<span id="cb120-39"><a href="loss-functions.html#cb120-39"></a>               <span class="dt">specificity =</span> <span class="kw">round</span>(speci<span class="op">$</span>.estimate, <span class="dv">2</span>))</span>
<span id="cb120-40"><a href="loss-functions.html#cb120-40"></a>  })</span>
<span id="cb120-41"><a href="loss-functions.html#cb120-41"></a></span>
<span id="cb120-42"><a href="loss-functions.html#cb120-42"></a>res_loss &lt;-<span class="st"> </span><span class="kw">do.call</span>(rbind, all_loss)</span>
<span id="cb120-43"><a href="loss-functions.html#cb120-43"></a>res_loss</span></code></pre></div>
<pre><code>##   cutoff sensitivity specificity
## 1    0.3        0.74        0.87
## 2    0.5        0.87        0.80
## 3    0.7        0.96        0.53</code></pre>
<p>The result contains the corresponding sensitivity and specificity when assigning a 1 or a 0 based on different cutoffs. For example, assigning a 1 if the probability was above <code>0.3</code> is associated with a true positive rate (sensitivity) of <code>0.74</code>. In contrast, switching the cutoff to <code>0.7</code>, increases the true positive rate to <code>0.95</code>, quite an impressive benchmark. This means that if we randomly picked someone who received childcare support, our model can accurately predict 95% of the time whether they did receive childcare support.</p>
<p>However, at the expense of increasing sensitivity, the true false rate decreases from <code>0.87</code> to <code>0.53</code>. It seems that we need to find the most optimal value between these two accuracy measures. We want a cutoff that maximizes both the true positive rate and true false rate. For that, we need to try many different values from 0 to 1 and calculate the sensitivity and specificity:</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="loss-functions.html#cb122-1"></a>all_loss &lt;-</span>
<span id="cb122-2"><a href="loss-functions.html#cb122-2"></a><span class="st">  </span><span class="kw">lapply</span>(<span class="kw">seq</span>(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="dt">by =</span> <span class="fl">0.01</span>), <span class="cf">function</span>(x) {</span>
<span id="cb122-3"><a href="loss-functions.html#cb122-3"></a>    res &lt;-</span>
<span id="cb122-4"><a href="loss-functions.html#cb122-4"></a><span class="st">      </span>res1 <span class="op">%&gt;%</span></span>
<span id="cb122-5"><a href="loss-functions.html#cb122-5"></a><span class="st">      </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">factor</span>(<span class="kw">as.numeric</span>(.pred_<span class="dv">1</span> <span class="op">&gt;=</span><span class="st"> </span>x)))</span>
<span id="cb122-6"><a href="loss-functions.html#cb122-6"></a>    </span>
<span id="cb122-7"><a href="loss-functions.html#cb122-7"></a>    sens &lt;-<span class="st"> </span><span class="kw">sensitivity</span>(res, dv, pred)</span>
<span id="cb122-8"><a href="loss-functions.html#cb122-8"></a>    speci &lt;-<span class="st"> </span><span class="kw">specificity</span>(res, dv, pred)</span>
<span id="cb122-9"><a href="loss-functions.html#cb122-9"></a></span>
<span id="cb122-10"><a href="loss-functions.html#cb122-10"></a>    <span class="kw">data.frame</span>(<span class="dt">cutoff =</span> x,</span>
<span id="cb122-11"><a href="loss-functions.html#cb122-11"></a>               <span class="dt">sensitivity =</span> sens<span class="op">$</span>.estimate,</span>
<span id="cb122-12"><a href="loss-functions.html#cb122-12"></a>               <span class="dt">specificity =</span> speci<span class="op">$</span>.estimate)</span>
<span id="cb122-13"><a href="loss-functions.html#cb122-13"></a>  })</span>
<span id="cb122-14"><a href="loss-functions.html#cb122-14"></a></span>
<span id="cb122-15"><a href="loss-functions.html#cb122-15"></a>res_loss &lt;-<span class="st"> </span><span class="kw">do.call</span>(rbind, all_loss)</span>
<span id="cb122-16"><a href="loss-functions.html#cb122-16"></a>res_loss</span></code></pre></div>
<pre><code>##    cutoff sensitivity specificity
## 1    0.01   0.2608696  1.00000000
## 2    0.02   0.3478261  1.00000000
## 3    0.03   0.3913043  1.00000000
## 4    0.04   0.3913043  1.00000000
## 5    0.05   0.3913043  1.00000000
## 6    0.06   0.3913043  1.00000000
## 7    0.07   0.3913043  1.00000000
## 8    0.08   0.3913043  1.00000000
## 9    0.09   0.4347826  1.00000000
## 10   0.10   0.5217391  0.93333333
## 11   0.11   0.5652174  0.93333333
## 12   0.12   0.5652174  0.93333333
## 13   0.13   0.6086957  0.93333333
## 14   0.14   0.6086957  0.93333333
## 15   0.15   0.6086957  0.93333333
## 16   0.16   0.6086957  0.93333333
## 17   0.17   0.6086957  0.93333333
## 18   0.18   0.6086957  0.93333333
## 19   0.19   0.6521739  0.93333333
## 20   0.20   0.6521739  0.86666667
## 21   0.21   0.6521739  0.86666667
## 22   0.22   0.6521739  0.86666667
## 23   0.23   0.6956522  0.86666667
## 24   0.24   0.6956522  0.86666667
## 25   0.25   0.7391304  0.86666667
## 26   0.26   0.7391304  0.86666667
## 27   0.27   0.7391304  0.86666667
## 28   0.28   0.7391304  0.86666667
## 29   0.29   0.7391304  0.86666667
## 30   0.30   0.7391304  0.86666667
## 31   0.31   0.7391304  0.86666667
## 32   0.32   0.7391304  0.86666667
## 33   0.33   0.7391304  0.86666667
## 34   0.34   0.7391304  0.86666667
## 35   0.35   0.7391304  0.86666667
## 36   0.36   0.7391304  0.80000000
## 37   0.37   0.7391304  0.80000000
## 38   0.38   0.7391304  0.80000000
## 39   0.39   0.7391304  0.80000000
## 40   0.40   0.7826087  0.80000000
## 41   0.41   0.7826087  0.80000000
## 42   0.42   0.7826087  0.80000000
## 43   0.43   0.8260870  0.80000000
## 44   0.44   0.8260870  0.80000000
## 45   0.45   0.8695652  0.80000000
## 46   0.46   0.8695652  0.80000000
## 47   0.47   0.8695652  0.80000000
## 48   0.48   0.8695652  0.80000000
## 49   0.49   0.8695652  0.80000000
## 50   0.50   0.8695652  0.80000000
## 51   0.51   0.8695652  0.80000000
## 52   0.52   0.8695652  0.73333333
## 53   0.53   0.8695652  0.73333333
## 54   0.54   0.9130435  0.73333333
## 55   0.55   0.9130435  0.73333333
## 56   0.56   0.9130435  0.73333333
## 57   0.57   0.9130435  0.73333333
## 58   0.58   0.9130435  0.73333333
## 59   0.59   0.9130435  0.73333333
## 60   0.60   0.9130435  0.73333333
## 61   0.61   0.9130435  0.73333333
## 62   0.62   0.9130435  0.73333333
## 63   0.63   0.9130435  0.73333333
## 64   0.64   0.9130435  0.60000000
## 65   0.65   0.9565217  0.60000000
## 66   0.66   0.9565217  0.60000000
## 67   0.67   0.9565217  0.60000000
## 68   0.68   0.9565217  0.60000000
## 69   0.69   0.9565217  0.60000000
## 70   0.70   0.9565217  0.53333333
## 71   0.71   1.0000000  0.53333333
## 72   0.72   1.0000000  0.53333333
## 73   0.73   1.0000000  0.53333333
## 74   0.74   1.0000000  0.53333333
## 75   0.75   1.0000000  0.53333333
## 76   0.76   1.0000000  0.53333333
## 77   0.77   1.0000000  0.53333333
## 78   0.78   1.0000000  0.53333333
## 79   0.79   1.0000000  0.53333333
## 80   0.80   1.0000000  0.53333333
## 81   0.81   1.0000000  0.53333333
## 82   0.82   1.0000000  0.53333333
## 83   0.83   1.0000000  0.53333333
## 84   0.84   1.0000000  0.53333333
## 85   0.85   1.0000000  0.53333333
## 86   0.86   1.0000000  0.46666667
## 87   0.87   1.0000000  0.46666667
## 88   0.88   1.0000000  0.46666667
## 89   0.89   1.0000000  0.40000000
## 90   0.90   1.0000000  0.40000000
## 91   0.91   1.0000000  0.40000000
## 92   0.92   1.0000000  0.40000000
## 93   0.93   1.0000000  0.33333333
## 94   0.94   1.0000000  0.26666667
## 95   0.95   1.0000000  0.20000000
## 96   0.96   1.0000000  0.13333333
## 97   0.97   1.0000000  0.13333333
## 98   0.98   1.0000000  0.06666667
## 99   0.99   1.0000000  0.06666667</code></pre>
<p>This result contains the sensitivity and specificity for many different cutoff points. These results are most easy to understand by visualizing them:</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="loss-functions.html#cb124-1"></a>res_loss <span class="op">%&gt;%</span></span>
<span id="cb124-2"><a href="loss-functions.html#cb124-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(specificity, sensitivity)) <span class="op">+</span></span>
<span id="cb124-3"><a href="loss-functions.html#cb124-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb124-4"><a href="loss-functions.html#cb124-4"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="04_lossfuns_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>On the <code>X</code> axis we have the true false rate (specificity) and on the <code>Y</code> axis we have the true positive rate (sensitivity). As expected, there is a trade off between the two. That is, cutoffs that improve the specificity does so at the expense of sensitivity. This is evident from the negative correlation in the scatterplot. Instead of visualizing the specificity as the true negative rate, let’s subtract 1 such that as the <code>X</code> axis increases, it means that the error is increasing:</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="loss-functions.html#cb125-1"></a>res_loss <span class="op">%&gt;%</span></span>
<span id="cb125-2"><a href="loss-functions.html#cb125-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>specificity, sensitivity)) <span class="op">+</span></span>
<span id="cb125-3"><a href="loss-functions.html#cb125-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb125-4"><a href="loss-functions.html#cb125-4"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="04_lossfuns_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>The ideal result from this plot is that most points cluster on the top left quadrant. This would mean that the sensitivity is high (the true positive rate) and the specificity is high (because <span class="math inline">\(1 - specificity\)</span> will switch the direction of the accuracy to the lower values of the <code>X</code> axis). Let’s add two lines to the plot. One that joins all the dots, such that it’s clearer where the improvements are and a diagonal line:</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="loss-functions.html#cb126-1"></a>res_loss <span class="op">%&gt;%</span></span>
<span id="cb126-2"><a href="loss-functions.html#cb126-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>specificity, sensitivity)) <span class="op">+</span></span>
<span id="cb126-3"><a href="loss-functions.html#cb126-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb126-4"><a href="loss-functions.html#cb126-4"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb126-5"><a href="loss-functions.html#cb126-5"></a><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></span>
<span id="cb126-6"><a href="loss-functions.html#cb126-6"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">+</span></span>
<span id="cb126-7"><a href="loss-functions.html#cb126-7"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="04_lossfuns_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Most statistical software will calculate this plot for you, but you just calculated it manually! These plots and statistics are sometimes treated as mystified but by writing them down step by step you can truly understand the intuition behind them. Let’s allow R to calculate this for us automatically:</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="loss-functions.html#cb127-1"></a>p1 &lt;-</span>
<span id="cb127-2"><a href="loss-functions.html#cb127-2"></a><span class="st">  </span>res1 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb127-3"><a href="loss-functions.html#cb127-3"></a><span class="st">  </span><span class="kw">roc_curve</span>(dv, .pred_<span class="dv">1</span>) <span class="op">%&gt;%</span></span>
<span id="cb127-4"><a href="loss-functions.html#cb127-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">.threshold =</span> <span class="kw">round</span>(.threshold, <span class="dv">2</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb127-5"><a href="loss-functions.html#cb127-5"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>specificity, sensitivity)) <span class="op">+</span></span>
<span id="cb127-6"><a href="loss-functions.html#cb127-6"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb127-7"><a href="loss-functions.html#cb127-7"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">text =</span> <span class="kw">paste0</span>(<span class="st">&quot;Cutoff: &quot;</span>, .threshold)), <span class="dt">alpha =</span> <span class="dv">0</span>) <span class="op">+</span></span>
<span id="cb127-8"><a href="loss-functions.html#cb127-8"></a><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></span>
<span id="cb127-9"><a href="loss-functions.html#cb127-9"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span>
<span id="cb127-10"><a href="loss-functions.html#cb127-10"></a></span>
<span id="cb127-11"><a href="loss-functions.html#cb127-11"></a>p1</span></code></pre></div>
<p><img src="04_lossfuns_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Although we built this plot from scratch, there is one thing we’re missing: the actual cutoff points! Here we’re visualizing the sensitivity and specificity of each cutoff point but it’s not entirely intuitive because we cannot see these cutoff values. Let’s make that plot interactive:</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="loss-functions.html#cb128-1"></a><span class="kw">ggplotly</span>(p1)</span></code></pre></div>
<div id="htmlwidget-78b7707d0b1b1d9dc45e" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-78b7707d0b1b1d9dc45e">{"x":{"data":[{"x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0.0666666666666667,0.0666666666666667,0.0666666666666667,0.0666666666666667,0.133333333333333,0.133333333333333,0.133333333333333,0.2,0.2,0.2,0.2,0.266666666666667,0.266666666666667,0.333333333333333,0.4,0.4,0.466666666666667,0.466666666666667,0.533333333333333,0.6,0.666666666666667,0.733333333333333,0.8,0.866666666666667,0.933333333333333,1,1],"y":[0,0.0434782608695652,0.0869565217391304,0.130434782608696,0.173913043478261,0.217391304347826,0.260869565217391,0.304347826086957,0.347826086956522,0.391304347826087,0.434782608695652,0.478260869565217,0.521739130434783,0.521739130434783,0.565217391304348,0.608695652173913,0.652173913043478,0.652173913043478,0.695652173913043,0.739130434782609,0.739130434782609,0.782608695652174,0.826086956521739,0.869565217391304,0.869565217391304,0.91304347826087,0.91304347826087,0.91304347826087,0.956521739130435,0.956521739130435,1,1,1,1,1,1,1,1,1,1],"text":["1 - specificity: 0.00000000<br />sensitivity: 0.00000000","1 - specificity: 0.00000000<br />sensitivity: 0.04347826","1 - specificity: 0.00000000<br />sensitivity: 0.08695652","1 - specificity: 0.00000000<br />sensitivity: 0.13043478","1 - specificity: 0.00000000<br />sensitivity: 0.17391304","1 - specificity: 0.00000000<br />sensitivity: 0.21739130","1 - specificity: 0.00000000<br />sensitivity: 0.26086957","1 - specificity: 0.00000000<br />sensitivity: 0.30434783","1 - specificity: 0.00000000<br />sensitivity: 0.34782609","1 - specificity: 0.00000000<br />sensitivity: 0.39130435","1 - specificity: 0.00000000<br />sensitivity: 0.43478261","1 - specificity: 0.00000000<br />sensitivity: 0.47826087","1 - specificity: 0.00000000<br />sensitivity: 0.52173913","1 - specificity: 0.06666667<br />sensitivity: 0.52173913","1 - specificity: 0.06666667<br />sensitivity: 0.56521739","1 - specificity: 0.06666667<br />sensitivity: 0.60869565","1 - specificity: 0.06666667<br />sensitivity: 0.65217391","1 - specificity: 0.13333333<br />sensitivity: 0.65217391","1 - specificity: 0.13333333<br />sensitivity: 0.69565217","1 - specificity: 0.13333333<br />sensitivity: 0.73913043","1 - specificity: 0.20000000<br />sensitivity: 0.73913043","1 - specificity: 0.20000000<br />sensitivity: 0.78260870","1 - specificity: 0.20000000<br />sensitivity: 0.82608696","1 - specificity: 0.20000000<br />sensitivity: 0.86956522","1 - specificity: 0.26666667<br />sensitivity: 0.86956522","1 - specificity: 0.26666667<br />sensitivity: 0.91304348","1 - specificity: 0.33333333<br />sensitivity: 0.91304348","1 - specificity: 0.40000000<br />sensitivity: 0.91304348","1 - specificity: 0.40000000<br />sensitivity: 0.95652174","1 - specificity: 0.46666667<br />sensitivity: 0.95652174","1 - specificity: 0.46666667<br />sensitivity: 1.00000000","1 - specificity: 0.53333333<br />sensitivity: 1.00000000","1 - specificity: 0.60000000<br />sensitivity: 1.00000000","1 - specificity: 0.66666667<br />sensitivity: 1.00000000","1 - specificity: 0.73333333<br />sensitivity: 1.00000000","1 - specificity: 0.80000000<br />sensitivity: 1.00000000","1 - specificity: 0.86666667<br />sensitivity: 1.00000000","1 - specificity: 0.93333333<br />sensitivity: 1.00000000","1 - specificity: 1.00000000<br />sensitivity: 1.00000000","1 - specificity: 1.00000000<br />sensitivity: 1.00000000"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0.0666666666666667,0.0666666666666667,0.0666666666666667,0.0666666666666667,0.133333333333333,0.133333333333333,0.133333333333333,0.2,0.2,0.2,0.2,0.266666666666667,0.266666666666667,0.333333333333333,0.4,0.4,0.466666666666667,0.466666666666667,0.533333333333333,0.6,0.666666666666667,0.733333333333333,0.8,0.866666666666667,0.933333333333333,1,1],"y":[0,0.0434782608695652,0.0869565217391304,0.130434782608696,0.173913043478261,0.217391304347826,0.260869565217391,0.304347826086957,0.347826086956522,0.391304347826087,0.434782608695652,0.478260869565217,0.521739130434783,0.521739130434783,0.565217391304348,0.608695652173913,0.652173913043478,0.652173913043478,0.695652173913043,0.739130434782609,0.739130434782609,0.782608695652174,0.826086956521739,0.869565217391304,0.869565217391304,0.91304347826087,0.91304347826087,0.91304347826087,0.956521739130435,0.956521739130435,1,1,1,1,1,1,1,1,1,1],"text":["Cutoff: -Inf<br />1 - specificity: 0.00000000<br />sensitivity: 0.00000000","Cutoff: 0<br />1 - specificity: 0.00000000<br />sensitivity: 0.04347826","Cutoff: 0<br />1 - specificity: 0.00000000<br />sensitivity: 0.08695652","Cutoff: 0<br />1 - specificity: 0.00000000<br />sensitivity: 0.13043478","Cutoff: 0<br />1 - specificity: 0.00000000<br />sensitivity: 0.17391304","Cutoff: 0<br />1 - specificity: 0.00000000<br />sensitivity: 0.21739130","Cutoff: 0<br />1 - specificity: 0.00000000<br />sensitivity: 0.26086957","Cutoff: 0.02<br />1 - specificity: 0.00000000<br />sensitivity: 0.30434783","Cutoff: 0.02<br />1 - specificity: 0.00000000<br />sensitivity: 0.34782609","Cutoff: 0.03<br />1 - specificity: 0.00000000<br />sensitivity: 0.39130435","Cutoff: 0.09<br />1 - specificity: 0.00000000<br />sensitivity: 0.43478261","Cutoff: 0.09<br />1 - specificity: 0.00000000<br />sensitivity: 0.47826087","Cutoff: 0.1<br />1 - specificity: 0.00000000<br />sensitivity: 0.52173913","Cutoff: 0.1<br />1 - specificity: 0.06666667<br />sensitivity: 0.52173913","Cutoff: 0.11<br />1 - specificity: 0.06666667<br />sensitivity: 0.56521739","Cutoff: 0.13<br />1 - specificity: 0.06666667<br />sensitivity: 0.60869565","Cutoff: 0.18<br />1 - specificity: 0.06666667<br />sensitivity: 0.65217391","Cutoff: 0.19<br />1 - specificity: 0.13333333<br />sensitivity: 0.65217391","Cutoff: 0.23<br />1 - specificity: 0.13333333<br />sensitivity: 0.69565217","Cutoff: 0.24<br />1 - specificity: 0.13333333<br />sensitivity: 0.73913043","Cutoff: 0.36<br />1 - specificity: 0.20000000<br />sensitivity: 0.73913043","Cutoff: 0.4<br />1 - specificity: 0.20000000<br />sensitivity: 0.78260870","Cutoff: 0.43<br />1 - specificity: 0.20000000<br />sensitivity: 0.82608696","Cutoff: 0.44<br />1 - specificity: 0.20000000<br />sensitivity: 0.86956522","Cutoff: 0.51<br />1 - specificity: 0.26666667<br />sensitivity: 0.86956522","Cutoff: 0.54<br />1 - specificity: 0.26666667<br />sensitivity: 0.91304348","Cutoff: 0.63<br />1 - specificity: 0.33333333<br />sensitivity: 0.91304348","Cutoff: 0.64<br />1 - specificity: 0.40000000<br />sensitivity: 0.91304348","Cutoff: 0.64<br />1 - specificity: 0.40000000<br />sensitivity: 0.95652174","Cutoff: 0.7<br />1 - specificity: 0.46666667<br />sensitivity: 0.95652174","Cutoff: 0.71<br />1 - specificity: 0.46666667<br />sensitivity: 1.00000000","Cutoff: 0.86<br />1 - specificity: 0.53333333<br />sensitivity: 1.00000000","Cutoff: 0.88<br />1 - specificity: 0.60000000<br />sensitivity: 1.00000000","Cutoff: 0.93<br />1 - specificity: 0.66666667<br />sensitivity: 1.00000000","Cutoff: 0.93<br />1 - specificity: 0.73333333<br />sensitivity: 1.00000000","Cutoff: 0.95<br />1 - specificity: 0.80000000<br />sensitivity: 1.00000000","Cutoff: 0.95<br />1 - specificity: 0.86666667<br />sensitivity: 1.00000000","Cutoff: 0.97<br />1 - specificity: 0.93333333<br />sensitivity: 1.00000000","Cutoff: 1<br />1 - specificity: 1.00000000<br />sensitivity: 1.00000000","Cutoff: Inf<br />1 - specificity: 1.00000000<br />sensitivity: 1.00000000"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(0,0,0,1)","opacity":0,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"}},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[-0.05,1.05],"y":[-0.05,1.05],"text":"intercept: 0<br />slope: 1","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"dash"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":26.2283105022831,"r":7.30593607305936,"b":40.1826484018265,"l":48.9497716894977},"font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-0.05,1.05],"tickmode":"array","ticktext":["0.00","0.25","0.50","0.75","1.00"],"tickvals":[0,0.25,0.5,0.75,1],"categoryorder":"array","categoryarray":["0.00","0.25","0.50","0.75","1.00"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":3.65296803652968,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"1 - specificity","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-0.05,1.05],"tickmode":"array","ticktext":["0.00","0.25","0.50","0.75","1.00"],"tickvals":[0,0.25,0.5,0.75,1],"categoryorder":"array","categoryarray":["0.00","0.25","0.50","0.75","1.00"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":3.65296803652968,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"sensitivity","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"16105c8a5dd9":{"x":{},"y":{},"type":"scatter"},"1610178106":{"text":{},"x":{},"y":{}},"1610717b0f6":{"intercept":{},"slope":{}}},"cur_data":"16105c8a5dd9","visdat":{"16105c8a5dd9":["function (y) ","x"],"1610178106":["function (y) ","x"],"1610717b0f6":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>You can now <strong>hover</strong> over the line and check the actual cutoff point associated with each sensitivity and specificity. Let’s suppose that for us, it’s more important to have a high sensitivity than a higher specificity. However, we want a specificity rate of at least 80%. The cutoff <code>0.44</code> offers both a sensitivity of 86% and a <span class="math inline">\(1 - specificity\)</span> of 20%, which translates to a specificity of 80%.</p>
<p>Although we’ve chosen this threshold from a very manual approach, there are formulas that already choose the most optimal combination for you. Due to the lack of time in the course, we won’t be covering them but feel free to look them up. One example is the Youden Index.</p>
<p>The last loss function we’ll discuss is a very small extension of the ROC curve. It’s called the <strong>A</strong>rea <strong>U</strong>nder the <strong>C</strong>urve or <span class="math inline">\(AUC\)</span>. As the name describes it, the <span class="math inline">\(AUC\)</span> is the percentage of the plot that is under the curve. For example:</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="loss-functions.html#cb129-1"></a>p1 <span class="op">+</span></span>
<span id="cb129-2"><a href="loss-functions.html#cb129-2"></a><span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="kw">aes</span>(<span class="dt">ymax =</span> sensitivity, <span class="dt">ymin =</span> <span class="dv">0</span>), <span class="dt">fill =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">6</span>)</span></code></pre></div>
<p><img src="04_lossfuns_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>As we described earlier, as the more points are located in the top left quadrant, the higher the overall accuracy of our model. If the points are very close to the top left quadrant, the area under the curve will be higher. As expected, higher <span class="math inline">\(AUC\)</span> values reflect increasing accuracy. We can let R workout the details with our predictions:</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="loss-functions.html#cb130-1"></a>res1 <span class="op">%&gt;%</span></span>
<span id="cb130-2"><a href="loss-functions.html#cb130-2"></a><span class="st">  </span><span class="kw">roc_auc</span>(dv, .pred_<span class="dv">1</span>)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 roc_auc binary         0.904</code></pre>
<p>It seems that 90% of the space of the plot is under the curve.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tree-based-methods.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="unsupervised-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
